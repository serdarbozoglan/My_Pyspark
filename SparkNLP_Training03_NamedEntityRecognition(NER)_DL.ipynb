{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkNLP_Training03_NamedEntityRecognition(NER)_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOazmgsVk1T9eMj0bgTTSVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serdarbozoglan/My_Pyspark/blob/master/SparkNLP_Training03_NamedEntityRecognition(NER)_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F68jb_FSWn_q",
        "colab_type": "text"
      },
      "source": [
        "# Code is from GitHub\n",
        "\n",
        "https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1cSRLSNMOI-",
        "colab_type": "text"
      },
      "source": [
        "Eger daha once indirdigimiz local'de bulunan bir pretrainedModel varsa onu cagirirken **.pretrained()** yerine **.load()** kullanabiliriz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_E0GxcFrATu",
        "colab_type": "code",
        "outputId": "5714794a-378d-45a6-faa3-4cfc591e01c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.4.5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_242\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.242-b08, mixed mode)\n",
            "Collecting pyspark==2.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/21/f05c186f4ddb01d15d0ddc36ef4b7e3cedbeb6412274a41f26b55a650ee5/pyspark-2.4.4.tar.gz (215.7MB)\n",
            "\u001b[K     |████████████████████████████████| 215.7MB 69kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.4-py2.py3-none-any.whl size=216130388 sha256=b681aa5ffb36eecebcdae5b5cbb311ca99ca4ab3c6bc2e8e201b4dd47372cdfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp==2.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/46/5c5a2bda407f693126386da5378f132e5e163fa6dfa46109548270348786/spark_nlp-2.4.5-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 9.2MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYfRI1sCIlOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lmOKqb3wYmi",
        "colab_type": "text"
      },
      "source": [
        "## Start Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBXyJUAyrSXP",
        "colab_type": "code",
        "outputId": "586aabd3-6eb9-4b23-bc6b-585a909baaa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import sparknlp\n",
        "#secret = '2NPvFO6B2a'\n",
        "#spark = start(secret)\n",
        "spark = sparknlp.start(gpu=True)\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "print('Spark NLP Version : ', sparknlp.version())\n",
        "print('Apache Spark Version :', spark.version)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP Version :  2.4.5\n",
            "Apache Spark Version : 2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHv4xyK0wGtG",
        "colab_type": "text"
      },
      "source": [
        "## CoNLL Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5vZ_QPavM9d",
        "colab_type": "code",
        "outputId": "6bb297e3-076c-4a80-82e7-20a846ae0d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train\n",
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-21 03:21:30--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3283420 (3.1M) [text/plain]\n",
            "Saving to: ‘eng.train’\n",
            "\n",
            "eng.train           100%[===================>]   3.13M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-04-21 03:21:31 (74.3 MB/s) - ‘eng.train’ saved [3283420/3283420]\n",
            "\n",
            "--2020-04-21 03:21:34--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827443 (808K) [text/plain]\n",
            "Saving to: ‘eng.testa’\n",
            "\n",
            "eng.testa           100%[===================>] 808.05K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-04-21 03:21:35 (40.8 MB/s) - ‘eng.testa’ saved [827443/827443]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkiZDnF2vM6l",
        "colab_type": "code",
        "outputId": "9df53ac6-c22a-4f62-cb90-ede22d2e3482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "with open(\"eng.train\") as f:\n",
        "    train_txt =f.read()\n",
        "\n",
        "print (train_txt[:500])\n",
        "## CoNLL Data Format always starts with \"-DOCSTART- -X- -X- O\" and should be in the format below"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-DOCSTART- -X- -X- O\n",
            "\n",
            "EU NNP B-NP B-ORG\n",
            "rejects VBZ B-VP O\n",
            "German JJ B-NP B-MISC\n",
            "call NN I-NP O\n",
            "to TO B-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ B-NP B-MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP B-NP B-PER\n",
            "Blackburn NNP I-NP I-PER\n",
            "\n",
            "BRUSSELS NNP B-NP B-LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT B-NP O\n",
            "European NNP I-NP B-ORG\n",
            "Commission NNP I-NP I-ORG\n",
            "said VBD B-VP O\n",
            "on IN B-PP O\n",
            "Thursday NNP B-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD B-VP O\n",
            "with IN B-PP O\n",
            "German JJ B-NP B-MISC\n",
            "advice NN I-NP O\n",
            "to TO B-PP O\n",
            "consumers NNS B-NP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aaHu_vxvM3u",
        "colab_type": "code",
        "outputId": "23b7e3ca-aaf4-443b-8b7a-5c27834dbcbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sparknlp.training import CoNLL\n",
        "\n",
        "training_data = CoNLL().readDataset(spark, './eng.train')\n",
        "training_data.show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|EU rejects German...|[[document, 0, 47...|[[document, 0, 47...|[[token, 0, 1, EU...|[[pos, 0, 1, NNP,...|[[named_entity, 0...|\n",
            "|     Peter Blackburn|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 4, Pe...|[[pos, 0, 4, NNP,...|[[named_entity, 0...|\n",
            "| BRUSSELS 1996-08-22|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 7, BR...|[[pos, 0, 7, NNP,...|[[named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh0H_sVdvM0z",
        "colab_type": "code",
        "outputId": "ce478de9-3e9f-4029-a54e-4a76ca748590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "training_data.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = false)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- sentence: array (nullable = false)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token: array (nullable = false)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- pos: array (nullable = false)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- label: array (nullable = false)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzafUsudvMx9",
        "colab_type": "code",
        "outputId": "9b7b44cf-8186-4a18-e804-57c5cefd8094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_data.count()\n",
        "# 14K sentences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14041"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4ZrnsM-vMvI",
        "colab_type": "code",
        "outputId": "7f7a84b5-ff6d-44c1-d9af-1f01e12f936f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "training_data.select(F.explode(F.arrays_zip('token.result', 'pos.result',  'label.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"pos\"),\n",
        "        F.expr(\"cols['2']\").alias(\"ner_label\")).show(truncate=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+---------+\n",
            "|     token|pos|ner_label|\n",
            "+----------+---+---------+\n",
            "|        EU|NNP|    B-ORG|\n",
            "|   rejects|VBZ|        O|\n",
            "|    German| JJ|   B-MISC|\n",
            "|      call| NN|        O|\n",
            "|        to| TO|        O|\n",
            "|   boycott| VB|        O|\n",
            "|   British| JJ|   B-MISC|\n",
            "|      lamb| NN|        O|\n",
            "|         .|  .|        O|\n",
            "|     Peter|NNP|    B-PER|\n",
            "| Blackburn|NNP|    I-PER|\n",
            "|  BRUSSELS|NNP|    B-LOC|\n",
            "|1996-08-22| CD|        O|\n",
            "|       The| DT|        O|\n",
            "|  European|NNP|    B-ORG|\n",
            "|Commission|NNP|    I-ORG|\n",
            "|      said|VBD|        O|\n",
            "|        on| IN|        O|\n",
            "|  Thursday|NNP|        O|\n",
            "|        it|PRP|        O|\n",
            "+----------+---+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZrfV-bHvMsU",
        "colab_type": "code",
        "outputId": "601eca13-2169-4150-a186-beb89da20817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "training_data.select(F.explode(F.arrays_zip('token.result','label.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"ground_truth\")).groupBy('ground_truth').count().orderBy('count', ascending=False).show(100,truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------+\n",
            "|ground_truth|count |\n",
            "+------------+------+\n",
            "|O           |169578|\n",
            "|B-LOC       |7140  |\n",
            "|B-PER       |6600  |\n",
            "|B-ORG       |6321  |\n",
            "|I-PER       |4528  |\n",
            "|I-ORG       |3704  |\n",
            "|B-MISC      |3438  |\n",
            "|I-LOC       |1157  |\n",
            "|I-MISC      |1155  |\n",
            "+------------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axbmkSkevMph",
        "colab_type": "code",
        "outputId": "3064ca40-d52a-4187-914f-481ac931ab84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# You can use any word embeddings you want (Glove, Elmo, Bert, custom etc.)\n",
        "\n",
        "glove_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
        "          .setInputCols([\"document\", \"token\"])\\\n",
        "          .setOutputCol(\"embeddings\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSHHHo4HvMmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ner_label will be our target column\n",
        "# .setPo = Dropout ratio\n",
        "nerTagger = NerDLApproach()\\\n",
        "  .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setMaxEpochs(6)\\\n",
        "  .setLr(0.001)\\\n",
        "  .setPo(0.005)\\\n",
        "  .setBatchSize(32)\\\n",
        "  .setRandomSeed(0)\\\n",
        "  .setVerbose(1)\\\n",
        "  .setValidationSplit(0.2)\\\n",
        "  .setEvaluationLogExtended(True) \\\n",
        "  .setEnableOutputLogs(True)\\\n",
        "  .setIncludeConfidence(True)\n",
        "\n",
        "ner_pipeline = Pipeline(stages=[\n",
        "          glove_embeddings,\n",
        "          nerTagger\n",
        " ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiUM5w0OzIj-",
        "colab_type": "text"
      },
      "source": [
        "## Fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX8Q7IR3vMj0",
        "colab_type": "code",
        "outputId": "4b5e08aa-6e26-406d-d299-382c7c5ad939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "ner_model = ner_pipeline.fit(training_data)\n",
        "# if you get an error for incompatible TF graph, use 4.1 NerDL-Graph.ipynb notebook to create a graph \n",
        "# from GitHub repository"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 104 ms, sys: 27.5 ms, total: 132 ms\n",
            "Wall time: 9min 31s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOkDkefRvMhB",
        "colab_type": "code",
        "outputId": "f473b1f7-034a-468b-eebb-fc61f3727cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!cd ~/annotator_logs && ls -lt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 876 Apr 21 03:36 NerDLApproach_b84a0e7cb18b.log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0wJY_SuvMeN",
        "colab_type": "code",
        "outputId": "ccf4da68-4a99-4421-f47a-2a3f90873a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "!cat ~/annotator_logs/NerDLApproach_cc78216e548c.log"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of the selected graph: ner-dl/blstm_10_100_128_120.pb\n",
            "Training started, trainExamples: 11233, labels: 9 chars: 84, \n",
            "\n",
            "\n",
            "Epoch: 0 started, learning rate: 0.001, dataset size: 11233\n",
            "Done, 83.310741104 loss: 338.20844, batches: 26\n",
            "Quality on validation dataset (20.0%), valExamples = 2808\n",
            "time to finish evaluation: 8.802185197\n",
            "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
            "B-LOC\t 323\t 113\t 1107\t 0.7408257\t 0.22587413\t 0.34619507\n",
            "I-ORG\t 5\t 8\t 779\t 0.3846154\t 0.0063775512\t 0.0125470515\n",
            "I-MISC\t 0\t 0\t 225\t 0.0\t 0.0\t 0.0\n",
            "I-LOC\t 0\t 0\t 247\t 0.0\t 0.0\t 0.0\n",
            "I-PER\t 229\t 36\t 657\t 0.86415094\t 0.25846502\t 0.3979149\n",
            "B-MISC\t 0\t 0\t 670\t 0.0\t 0.0\t 0.0\n",
            "B-ORG\t 333\t 237\t 947\t 0.5842105\t 0.26015624\t 0.35999998\n",
            "B-PER\t 289\t 52\t 1019\t 0.84750736\t 0.22094801\t 0.35051548\n",
            "tp: 1179 fp: 446 fn: 5651 labels: 8\n",
            "Macro-average\t prec: 0.42766374, rec: 0.12147762, f1: 0.1892102\n",
            "Micro-average\t prec: 0.72553843, rec: 0.17262079, f1: 0.27888826\n",
            "\n",
            "\n",
            "Epoch: 1 started, learning rate: 9.950249E-4, dataset size: 11233\n",
            "Done, 86.65745414 loss: 199.17252, batches: 26\n",
            "Quality on validation dataset (20.0%), valExamples = 2808\n",
            "time to finish evaluation: 8.389202589\n",
            "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
            "B-LOC\t 848\t 317\t 582\t 0.727897\t 0.59300697\t 0.6535645\n",
            "I-ORG\t 19\t 18\t 765\t 0.5135135\t 0.024234693\t 0.04628502\n",
            "I-MISC\t 0\t 0\t 225\t 0.0\t 0.0\t 0.0\n",
            "I-LOC\t 0\t 0\t 247\t 0.0\t 0.0\t 0.0\n",
            "I-PER\t 728\t 190\t 158\t 0.7930283\t 0.8216704\t 0.80709535\n",
            "B-MISC\t 105\t 31\t 565\t 0.77205884\t 0.15671642\t 0.2605459\n",
            "B-ORG\t 617\t 346\t 663\t 0.6407061\t 0.48203126\t 0.550156\n",
            "B-PER\t 848\t 280\t 460\t 0.75177306\t 0.64831805\t 0.6962233\n",
            "tp: 3165 fp: 1182 fn: 3665 labels: 8\n",
            "Macro-average\t prec: 0.52487206, rec: 0.34074724, f1: 0.41322714\n",
            "Micro-average\t prec: 0.7280883, rec: 0.4633968, f1: 0.5663416\n",
            "\n",
            "\n",
            "Epoch: 2 started, learning rate: 9.90099E-4, dataset size: 11233\n",
            "Done, 82.655822332 loss: 151.84325, batches: 26\n",
            "Quality on validation dataset (20.0%), valExamples = 2808\n",
            "time to finish evaluation: 8.203548585\n",
            "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
            "B-LOC\t 1126\t 625\t 304\t 0.6430611\t 0.7874126\t 0.70795345\n",
            "I-ORG\t 58\t 35\t 726\t 0.6236559\t 0.073979594\t 0.1322691\n",
            "I-MISC\t 0\t 0\t 225\t 0.0\t 0.0\t 0.0\n",
            "I-LOC\t 0\t 0\t 247\t 0.0\t 0.0\t 0.0\n",
            "I-PER\t 824\t 285\t 62\t 0.7430117\t 0.9300226\t 0.8260652\n",
            "B-MISC\t 228\t 48\t 442\t 0.82608694\t 0.3402985\t 0.48202962\n",
            "B-ORG\t 639\t 269\t 641\t 0.7037445\t 0.49921876\t 0.58409506\n",
            "B-PER\t 1012\t 256\t 296\t 0.79810727\t 0.7737003\t 0.78571427\n",
            "tp: 3887 fp: 1518 fn: 2943 labels: 8\n",
            "Macro-average\t prec: 0.54220843, rec: 0.425579, f1: 0.47686613\n",
            "Micro-average\t prec: 0.71914893, rec: 0.5691069, f1: 0.6353903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmrGrlEkvMbW",
        "colab_type": "code",
        "outputId": "15805eaf-a4ce-4cd2-c05a-d6bbb13e6950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from sparknlp.training import CoNLL\n",
        "\n",
        "test_data = CoNLL().readDataset(spark, './eng.testa')\n",
        "\n",
        "test_data = glove_embeddings.transform(test_data)\n",
        "\n",
        "test_data.show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|          embeddings|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|CRICKET - LEICEST...|[[document, 0, 64...|[[document, 0, 64...|[[token, 0, 6, CR...|[[pos, 0, 6, NNP,...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|   LONDON 1996-08-30|[[document, 0, 16...|[[document, 0, 16...|[[token, 0, 5, LO...|[[pos, 0, 5, NNP,...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|West Indian all-r...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 3, We...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|[[word_embeddings...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihZ9tCifvMYc",
        "colab_type": "code",
        "outputId": "80239162-23ef-434d-d1f6-500f5e50b67b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# transform equals predict in sklearn\n",
        "predictions = ner_model.transform(test_data)\n",
        "predictions.show(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|          embeddings|                 ner|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|CRICKET - LEICEST...|[[document, 0, 64...|[[document, 0, 64...|[[token, 0, 6, CR...|[[pos, 0, 6, NNP,...|[[named_entity, 0...|[[word_embeddings...|[[named_entity, 0...|\n",
            "|   LONDON 1996-08-30|[[document, 0, 16...|[[document, 0, 16...|[[token, 0, 5, LO...|[[pos, 0, 5, NNP,...|[[named_entity, 0...|[[word_embeddings...|[[named_entity, 0...|\n",
            "|West Indian all-r...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 3, We...|[[pos, 0, 3, NNP,...|[[named_entity, 0...|[[word_embeddings...|[[named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTjHqXh8vMVm",
        "colab_type": "code",
        "outputId": "717576b3-e399-40a5-f5a5-054567f9060c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "predictions.select('token.result', 'label.result', 'ner.result').show(3, truncate=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                            result|                                            result|                                            result|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "|[CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, TO...|             [O, O, B-ORG, O, O, O, O, O, O, O, O]|         [B-LOC, O, B-ORG, O, O, O, O, O, O, O, O]|\n",
            "|                              [LONDON, 1996-08-30]|                                        [B-LOC, O]|                                        [B-LOC, O]|\n",
            "|[West, Indian, all-rounder, Phil, Simmons, took...|[B-MISC, I-MISC, O, B-PER, I-PER, O, O, O, O, O...|[B-LOC, O, O, B-PER, I-PER, O, O, O, O, O, O, O...|\n",
            "+--------------------------------------------------+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86I5xd9F1xfo",
        "colab_type": "text"
      },
      "source": [
        "## Test Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd3DFow7vMS0",
        "colab_type": "code",
        "outputId": "458bfc65-17c0-43f7-e99f-71d65855436a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
        "        F.expr(\"cols['2']\").alias(\"prediction\")).show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+------------+----------+\n",
            "|token         |ground_truth|prediction|\n",
            "+--------------+------------+----------+\n",
            "|CRICKET       |O           |B-LOC     |\n",
            "|-             |O           |O         |\n",
            "|LEICESTERSHIRE|B-ORG       |B-ORG     |\n",
            "|TAKE          |O           |O         |\n",
            "|OVER          |O           |O         |\n",
            "|AT            |O           |O         |\n",
            "|TOP           |O           |O         |\n",
            "|AFTER         |O           |O         |\n",
            "|INNINGS       |O           |O         |\n",
            "|VICTORY       |O           |O         |\n",
            "|.             |O           |O         |\n",
            "|LONDON        |B-LOC       |B-LOC     |\n",
            "|1996-08-30    |O           |O         |\n",
            "|West          |B-MISC      |B-LOC     |\n",
            "|Indian        |I-MISC      |O         |\n",
            "|all-rounder   |O           |O         |\n",
            "|Phil          |B-PER       |B-PER     |\n",
            "|Simmons       |I-PER       |I-PER     |\n",
            "|took          |O           |O         |\n",
            "|four          |O           |O         |\n",
            "+--------------+------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ9sZSw9vBUb",
        "colab_type": "text"
      },
      "source": [
        "## Below Codes Belong To Previous Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDqBOetXnwUt",
        "colab_type": "text"
      },
      "source": [
        "## LemmatizerModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNR8Ao2slh3s",
        "colab_type": "code",
        "outputId": "beaa1442-886b-4797-ac42-ab15a863bd18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "!wget -O news_category_test.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-21 03:01:42--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_test.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.27.14\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.27.14|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1504408 (1.4M) [text/csv]\n",
            "Saving to: ‘news_category_test.csv’\n",
            "\n",
            "news_category_test. 100%[===================>]   1.43M  1.32MB/s    in 1.1s    \n",
            "\n",
            "2020-04-21 03:01:44 (1.32 MB/s) - ‘news_category_test.csv’ saved [1504408/1504408]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfOulIzwljHh",
        "colab_type": "code",
        "outputId": "f5f05861-5330-47ec-d873-a05257d43e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "news_df = spark.read \\\n",
        "        .option(\"header\", 'true') \\\n",
        "        .csv(\"news_category_test.csv\") \\\n",
        "        .withColumnRenamed(\"description\", 'text')\n",
        "\n",
        "news_df.show(truncate=50)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------------------------------------+\n",
            "|category|                                              text|\n",
            "+--------+--------------------------------------------------+\n",
            "|Business|Unions representing workers at Turner   Newall ...|\n",
            "|Sci/Tech| TORONTO, Canada    A second team of rocketeers...|\n",
            "|Sci/Tech| A company founded by a chemistry researcher at...|\n",
            "|Sci/Tech| It's barely dawn when Mike Fitzpatrick starts ...|\n",
            "|Sci/Tech| Southern California's smog fighting agency wen...|\n",
            "|Sci/Tech|\"The British Department for Education and Skill...|\n",
            "|Sci/Tech|\"confessed author of the Netsky and Sasser viru...|\n",
            "|Sci/Tech|\\\\FOAF/LOAF  and bloom filters have a lot of in...|\n",
            "|Sci/Tech|\"Wiltshire Police warns about \"\"phishing\"\" afte...|\n",
            "|Sci/Tech|In its first two years, the UK's dedicated card...|\n",
            "|Sci/Tech| A group of technology companies  including Tex...|\n",
            "|Sci/Tech| Apple Computer Inc.&lt;AAPL.O&gt; on  Tuesday ...|\n",
            "|Sci/Tech| Free Record Shop, a Dutch music  retail chain,...|\n",
            "|Sci/Tech|A giant 100km colony of ants  which has been di...|\n",
            "|Sci/Tech|                      \"Dolphin groups, or \"\"pods\"\"|\n",
            "|Sci/Tech|Tyrannosaurus rex achieved its massive size due...|\n",
            "|Sci/Tech|  Scientists have discovered irregular lumps be...|\n",
            "|Sci/Tech|  ESAs Mars Express has relayed pictures from o...|\n",
            "|Sci/Tech|When did life begin? One evidential clue stems ...|\n",
            "|Sci/Tech|update Earnings per share rise compared with a ...|\n",
            "+--------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI2h5At9ljLf",
        "colab_type": "code",
        "outputId": "f8abec10-0c27-4622-eac8-3abff78ca47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        " # lemma_antbnc is coming from JohnsonLab Github page there is the list of pretrained Models\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained('lemma_antbnc', 'en')\\\n",
        "            .setInputCols([\"token\"])\\\n",
        "            .setOutputCol(\"lemma\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2pAW3I8ljAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "        .setInputCol('text')\\\n",
        "        .setOutputCol('document')\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "        .setInputCols(['document'])\\\n",
        "        .setOutputCol('token')\n",
        "\n",
        "stemmer = Stemmer()\\\n",
        "        .setInputCols(['token'])\\\n",
        "        .setOutputCol('stem')\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "              documentAssembler,\n",
        "              tokenizer,\n",
        "              stemmer,\n",
        "              lemmatizer          \n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D9swOfhli-M",
        "colab_type": "code",
        "outputId": "773bf084-5398-4086-ab93-8710ff99a61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df)\n",
        "result.show(5, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|category|                                                                                                text|                                                                                            document|                                                                                               token|                                                                                                stem|                                                                                               lemma|\n",
            "+--------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|Business|Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stric...|[[document, 0, 126, Unions representing workers at Turner   Newall say they are 'disappointed' af...|[[token, 0, 5, Unions, [sentence -> 0], []], [token, 7, 18, representing, [sentence -> 0], []], [...|[[token, 0, 5, union, [sentence -> 0], []], [token, 7, 18, repres, [sentence -> 0], []], [token, ...|[[token, 0, 5, Unions, [sentence -> 0], []], [token, 7, 18, represent, [sentence -> 0], []], [tok...|\n",
            "|Sci/Tech| TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize,...|[[document, 0, 222,  TORONTO, Canada    A second team of rocketeers competing for the  #36;10 mil...|[[token, 1, 7, TORONTO, [sentence -> 0], []], [token, 8, 8, ,, [sentence -> 0], []], [token, 10, ...|[[token, 1, 7, toronto, [sentence -> 0], []], [token, 8, 8, ,, [sentence -> 0], []], [token, 10, ...|[[token, 1, 7, TORONTO, [sentence -> 0], []], [token, 8, 8, ,, [sentence -> 0], []], [token, 10, ...|\n",
            "|Sci/Tech| A company founded by a chemistry researcher at the University of Louisville won a grant to devel...|[[document, 0, 209,  A company founded by a chemistry researcher at the University of Louisville ...|[[token, 1, 1, A, [sentence -> 0], []], [token, 3, 9, company, [sentence -> 0], []], [token, 11, ...|[[token, 1, 1, a, [sentence -> 0], []], [token, 3, 9, compani, [sentence -> 0], []], [token, 11, ...|[[token, 1, 1, A, [sentence -> 0], []], [token, 3, 9, company, [sentence -> 0], []], [token, 11, ...|\n",
            "|Sci/Tech| It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures an...|[[document, 0, 267,  It's barely dawn when Mike Fitzpatrick starts his shift with a blur of color...|[[token, 1, 4, It's, [sentence -> 0], []], [token, 6, 11, barely, [sentence -> 0], []], [token, 1...|[[token, 1, 4, it', [sentence -> 0], []], [token, 6, 11, bare, [sentence -> 0], []], [token, 13, ...|[[token, 1, 4, It's, [sentence -> 0], []], [token, 6, 11, barely, [sentence -> 0], []], [token, 1...|\n",
            "|Sci/Tech| Southern California's smog fighting agency went after emissions of the bovine variety Friday, ad...|[[document, 0, 174,  Southern California's smog fighting agency went after emissions of the bovin...|[[token, 1, 8, Southern, [sentence -> 0], []], [token, 10, 21, California's, [sentence -> 0], []]...|[[token, 1, 8, southern, [sentence -> 0], []], [token, 10, 21, california', [sentence -> 0], []],...|[[token, 1, 8, Southern, [sentence -> 0], []], [token, 10, 21, California's, [sentence -> 0], []]...|\n",
            "+--------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OaZSdCXxmoJ",
        "colab_type": "text"
      },
      "source": [
        "## PerceptronModel(POS- Part of Speech Tags)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsfAoTr5li77",
        "colab_type": "code",
        "outputId": "64901c58-18df-4e5f-e646-8d0de61e30c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pos = PerceptronModel.pretrained('pos_anc', 'en')\\\n",
        "        .setInputCols(['document', 'token'])\\\n",
        "        .setOutputCol('pos')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUIr3VB6li6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        "              documentAssembler,\n",
        "              tokenizer,\n",
        "              stemmer,\n",
        "              lemmatizer,\n",
        "              pos         \n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Npzox_li3c",
        "colab_type": "code",
        "outputId": "572b3e99-b567-4fba-d157-e4b38d956af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df)\n",
        "result.show(5, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|category|                                                                                                text|                                                                                            document|                                                                                               token|                                                                                                stem|                                                                                               lemma|                                                                                                 pos|\n",
            "+--------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|Business|Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stric...|[[document, 0, 126, Unions representing workers at Turner   Newall say they are 'disappointed' af...|[[token, 0, 5, Unions, [sentence -> 0], []], [token, 7, 18, representing, [sentence -> 0], []], [...|[[token, 0, 5, union, [sentence -> 0], []], [token, 7, 18, repres, [sentence -> 0], []], [token, ...|[[token, 0, 5, Unions, [sentence -> 0], []], [token, 7, 18, represent, [sentence -> 0], []], [tok...|[[pos, 0, 5, NNP, [word -> Unions], []], [pos, 7, 18, VBG, [word -> representing], []], [pos, 20,...|\n",
            "|Sci/Tech| TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize,...|[[document, 0, 222,  TORONTO, Canada    A second team of rocketeers competing for the  #36;10 mil...|[[token, 1, 7, TORONTO, [sentence -> 0], []], [token, 8, 8, ,, [sentence -> 0], []], [token, 10, ...|[[token, 1, 7, toronto, [sentence -> 0], []], [token, 8, 8, ,, [sentence -> 0], []], [token, 10, ...|[[token, 1, 7, TORONTO, [sentence -> 0], []], [token, 8, 8, ,, [sentence -> 0], []], [token, 10, ...|[[pos, 1, 7, NNP, [word -> TORONTO], []], [pos, 8, 8, ,, [word -> ,], []], [pos, 10, 15, NNP, [wo...|\n",
            "|Sci/Tech| A company founded by a chemistry researcher at the University of Louisville won a grant to devel...|[[document, 0, 209,  A company founded by a chemistry researcher at the University of Louisville ...|[[token, 1, 1, A, [sentence -> 0], []], [token, 3, 9, company, [sentence -> 0], []], [token, 11, ...|[[token, 1, 1, a, [sentence -> 0], []], [token, 3, 9, compani, [sentence -> 0], []], [token, 11, ...|[[token, 1, 1, A, [sentence -> 0], []], [token, 3, 9, company, [sentence -> 0], []], [token, 11, ...|[[pos, 1, 1, DT, [word -> A], []], [pos, 3, 9, NN, [word -> company], []], [pos, 11, 17, VBN, [wo...|\n",
            "|Sci/Tech| It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures an...|[[document, 0, 267,  It's barely dawn when Mike Fitzpatrick starts his shift with a blur of color...|[[token, 1, 4, It's, [sentence -> 0], []], [token, 6, 11, barely, [sentence -> 0], []], [token, 1...|[[token, 1, 4, it', [sentence -> 0], []], [token, 6, 11, bare, [sentence -> 0], []], [token, 13, ...|[[token, 1, 4, It's, [sentence -> 0], []], [token, 6, 11, barely, [sentence -> 0], []], [token, 1...|[[pos, 1, 4, NNP, [word -> It's], []], [pos, 6, 11, RB, [word -> barely], []], [pos, 13, 16, NN, ...|\n",
            "|Sci/Tech| Southern California's smog fighting agency went after emissions of the bovine variety Friday, ad...|[[document, 0, 174,  Southern California's smog fighting agency went after emissions of the bovin...|[[token, 1, 8, Southern, [sentence -> 0], []], [token, 10, 21, California's, [sentence -> 0], []]...|[[token, 1, 8, southern, [sentence -> 0], []], [token, 10, 21, california', [sentence -> 0], []],...|[[token, 1, 8, Southern, [sentence -> 0], []], [token, 10, 21, California's, [sentence -> 0], []]...|[[pos, 1, 8, NNP, [word -> Southern], []], [pos, 10, 21, NNP, [word -> California's], []], [pos, ...|\n",
            "+--------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppsafnKwli0S",
        "colab_type": "code",
        "outputId": "8db6cbe7-2178-466a-fcb5-cfa02d4b8dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "result.select('token.result', 'pos.result').show(10, truncate=60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------+------------------------------------------------------------+\n",
            "|                                                      result|                                                      result|\n",
            "+------------------------------------------------------------+------------------------------------------------------------+\n",
            "|[Unions, representing, workers, at, Turner, Newall, say, ...|[NNP, VBG, NNS, IN, NNP, NNP, VBP, PRP, VBP, POS, JJ, POS...|\n",
            "|[TORONTO, ,, Canada, A, second, team, of, rocketeers, com...|[NNP, ,, NNP, DT, JJ, NN, IN, NNS, VBG, IN, DT, NN, CD, N...|\n",
            "|[A, company, founded, by, a, chemistry, researcher, at, t...|[DT, NN, VBN, IN, DT, NN, NN, IN, DT, NNP, IN, NNP, VBD, ...|\n",
            "|[It's, barely, dawn, when, Mike, Fitzpatrick, starts, his...|[NNP, RB, NN, WRB, NNP, NNP, VBZ, PRP$, NN, IN, DT, NN, I...|\n",
            "|[Southern, California's, smog, fighting, agency, went, af...|[NNP, NNP, NN, VBG, NN, VBD, IN, NNS, IN, DT, NN, NN, NNP...|\n",
            "|[\", The, British, Department, for, Education, and, Skills...|['', DT, NNP, NNP, IN, NNP, CC, NNS, (, NNP, ), RB, VBD, ...|\n",
            "|[\", confessed, author, of, the, Netsky, and, Sasser, viru...|['', VBN, NN, IN, DT, NNP, CC, NNP, NNS, ,, VBZ, JJ, IN, ...|\n",
            "|[\\\\FOAF/LOAF, and, bloom, filters, have, a, lot, of, inte...|[NN, CC, NN, NNS, VBP, DT, NN, IN, JJ, NNS, IN, NN, CC, N...|\n",
            "|[\", Wiltshire, Police, warns, about, \"\", phishing, \"\", af...|['', NNP, NNP, VBZ, IN, NN, VBG, NN, IN, PRP$, NN, NN, NN...|\n",
            "|[In, its, first, two, years, ,, the, UK's, dedicated, car...|[IN, PRP$, JJ, CD, NNS, ,, DT, NNP, VBN, NN, NN, NN, ,, V...|\n",
            "+------------------------------------------------------------+------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAA1qU8rlixs",
        "colab_type": "code",
        "outputId": "0f1d9acf-babe-4fcf-9434-0362b88633af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))\n",
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'token.begin', 'token.end', 'stem.result',  'lemma.result', 'pos.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "        F.expr(\"cols['2']\").alias(\"end\"),\n",
        "        F.expr(\"cols['3']\").alias(\"stem\"),\n",
        "        F.expr(\"cols['4']\").alias(\"lemma\"),\n",
        "        F.expr(\"cols['5']\").alias(\"pos\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unions</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>union</td>\n",
              "      <td>Unions</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>representing</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>repres</td>\n",
              "      <td>represent</td>\n",
              "      <td>VBG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>workers</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>worker</td>\n",
              "      <td>worker</td>\n",
              "      <td>NNS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>28</td>\n",
              "      <td>29</td>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Turner</td>\n",
              "      <td>31</td>\n",
              "      <td>36</td>\n",
              "      <td>turner</td>\n",
              "      <td>Turner</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Newall</td>\n",
              "      <td>40</td>\n",
              "      <td>45</td>\n",
              "      <td>newal</td>\n",
              "      <td>Newall</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>say</td>\n",
              "      <td>47</td>\n",
              "      <td>49</td>\n",
              "      <td>sai</td>\n",
              "      <td>say</td>\n",
              "      <td>VBP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>they</td>\n",
              "      <td>51</td>\n",
              "      <td>54</td>\n",
              "      <td>thei</td>\n",
              "      <td>they</td>\n",
              "      <td>PRP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>are</td>\n",
              "      <td>56</td>\n",
              "      <td>58</td>\n",
              "      <td>ar</td>\n",
              "      <td>be</td>\n",
              "      <td>VBP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>'</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>'</td>\n",
              "      <td>'</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          token  begin  end    stem      lemma  pos\n",
              "0        Unions      0    5   union     Unions  NNP\n",
              "1  representing      7   18  repres  represent  VBG\n",
              "2       workers     20   26  worker     worker  NNS\n",
              "3            at     28   29      at         at   IN\n",
              "4        Turner     31   36  turner     Turner  NNP\n",
              "5        Newall     40   45   newal     Newall  NNP\n",
              "6           say     47   49     sai        say  VBP\n",
              "7          they     51   54    thei       they  PRP\n",
              "8           are     56   58      ar         be  VBP\n",
              "9             '     60   60       '          '  POS"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V78zEGSrlivD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# same in LightPipeline\n",
        "\n",
        "light_model = LightPipeline(pipelineModel)\n",
        "\n",
        "light_result = light_model.annotate('Unions representing workers at Turner Newall say there are lots of books there.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9UV40gqlh05",
        "colab_type": "code",
        "outputId": "e660bd00-e784-4567-ae63-dc0db2984c49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "list(zip(light_result['token'], light_result['stem'], light_result['lemma'], light_result['pos']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Unions', 'union', 'Unions', 'NNP'),\n",
              " ('representing', 'repres', 'represent', 'VBG'),\n",
              " ('workers', 'worker', 'worker', 'NNS'),\n",
              " ('at', 'at', 'at', 'IN'),\n",
              " ('Turner', 'turner', 'Turner', 'NNP'),\n",
              " ('Newall', 'newal', 'Newall', 'NNP'),\n",
              " ('say', 'sai', 'say', 'VB'),\n",
              " ('there', 'there', 'there', 'EX'),\n",
              " ('are', 'ar', 'be', 'VBP'),\n",
              " ('lots', 'lot', 'lot', 'NNS'),\n",
              " ('of', 'of', 'of', 'IN'),\n",
              " ('books', 'book', 'book', 'NNS'),\n",
              " ('there', 'there', 'there', 'RB'),\n",
              " ('.', '.', '.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_NW-J6rlhyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# applying POS chunker to find a custom pattern\n",
        "\n",
        "chunker = Chunker()\\\n",
        "    .setInputCols([\"document\", \"pos\"])\\\n",
        "    .setOutputCol(\"chunk\")\\\n",
        "    .setRegexParsers([\"<NNP>+\", \"<DT>?<JJ>*<NN>\"])\n",
        "\n",
        "# NNP: Proper Noun\n",
        "# NN: Common Noun\n",
        "# DT: Determinator (e.g. the)\n",
        "# JJ: Adjective\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "              documentAssembler,\n",
        "              tokenizer,\n",
        "              stemmer,\n",
        "              lemmatizer,\n",
        "              pos,\n",
        "              chunker        \n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tbQZ9gUlhwn",
        "colab_type": "code",
        "outputId": "aff77609-3172-4993-f823-5e10eedba74b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))\n",
        "result.show(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|category|                text|            document|               token|                stem|               lemma|                 pos|               chunk|\n",
            "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Business|Unions representi...|[[document, 0, 12...|[[token, 0, 5, Un...|[[token, 0, 5, un...|[[token, 0, 5, Un...|[[pos, 0, 5, NNP,...|[[chunk, 0, 5, Un...|\n",
            "|Sci/Tech| TORONTO, Canada ...|[[document, 0, 22...|[[token, 1, 7, TO...|[[token, 1, 7, to...|[[token, 1, 7, TO...|[[pos, 1, 7, NNP,...|[[chunk, 1, 7, TO...|\n",
            "|Sci/Tech| A company founde...|[[document, 0, 20...|[[token, 1, 1, A,...|[[token, 1, 1, a,...|[[token, 1, 1, A,...|[[pos, 1, 1, DT, ...|[[chunk, 52, 61, ...|\n",
            "|Sci/Tech| It's barely dawn...|[[document, 0, 26...|[[token, 1, 4, It...|[[token, 1, 4, it...|[[token, 1, 4, It...|[[pos, 1, 4, NNP,...|[[chunk, 1, 4, It...|\n",
            "|Sci/Tech| Southern Califor...|[[document, 0, 17...|[[token, 1, 8, So...|[[token, 1, 8, so...|[[token, 1, 8, So...|[[pos, 1, 8, NNP,...|[[chunk, 1, 21, S...|\n",
            "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZw2nrSnlhu3",
        "colab_type": "code",
        "outputId": "3b00c97b-1bf9-4d3d-fe0b-287e6f380826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "result.select('chunk').show(10,truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|chunk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[[chunk, 0, 5, Unions, [sentence -> 0, chunk -> 0], []], [chunk, 31, 45, Turner   Newall, [sentence -> 0, chunk -> 1], []], [chunk, 113, 125, Federal Mogul, [sentence -> 0, chunk -> 2], []], [chunk, 92, 99, stricken, [sentence -> 0, chunk -> 3], []], [chunk, 101, 106, parent, [sentence -> 0, chunk -> 4], []], [chunk, 108, 111, firm, [sentence -> 0, chunk -> 5], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|[[chunk, 1, 7, TORONTO, [sentence -> 0, chunk -> 0], []], [chunk, 10, 15, Canada, [sentence -> 0, chunk -> 1], []], [chunk, 82, 95, Ansari X Prize, [sentence -> 0, chunk -> 2], []], [chunk, 20, 32, A second team, [sentence -> 0, chunk -> 3], []], [chunk, 62, 72, the  #36;10, [sentence -> 0, chunk -> 4], []], [chunk, 98, 106, a contest, [sentence -> 0, chunk -> 5], []], [chunk, 122, 144, funded suborbital space, [sentence -> 0, chunk -> 6], []], [chunk, 146, 151, flight, [sentence -> 0, chunk -> 7], []], [chunk, 179, 194, the first launch, [sentence -> 0, chunk -> 8], []], [chunk, 196, 199, date, [sentence -> 0, chunk -> 9], []], [chunk, 209, 221, manned rocket, [sentence -> 0, chunk -> 10], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
            "|[[chunk, 52, 61, University, [sentence -> 0, chunk -> 0], []], [chunk, 66, 75, Louisville, [sentence -> 0, chunk -> 1], []], [chunk, 1, 9, A company, [sentence -> 0, chunk -> 2], []], [chunk, 22, 32, a chemistry, [sentence -> 0, chunk -> 3], []], [chunk, 34, 43, researcher, [sentence -> 0, chunk -> 4], []], [chunk, 81, 87, a grant, [sentence -> 0, chunk -> 5], []], [chunk, 100, 107, a method, [sentence -> 0, chunk -> 6], []], [chunk, 178, 189, the building, [sentence -> 0, chunk -> 7], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
            "|[[chunk, 1, 4, It's, [sentence -> 0, chunk -> 0], []], [chunk, 23, 38, Mike Fitzpatrick, [sentence -> 0, chunk -> 1], []], [chunk, 161, 169, Lightning, [sentence -> 0, chunk -> 2], []], [chunk, 205, 209, Winds, [sentence -> 0, chunk -> 3], []], [chunk, 13, 16, dawn, [sentence -> 0, chunk -> 4], []], [chunk, 51, 55, shift, [sentence -> 0, chunk -> 5], []], [chunk, 62, 67, a blur, [sentence -> 0, chunk -> 6], []], [chunk, 141, 147, the day, [sentence -> 0, chunk -> 7], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "|[[chunk, 1, 21, Southern California's, [sentence -> 0, chunk -> 0], []], [chunk, 87, 92, Friday, [sentence -> 0, chunk -> 1], []], [chunk, 23, 26, smog, [sentence -> 0, chunk -> 2], []], [chunk, 37, 42, agency, [sentence -> 0, chunk -> 3], []], [chunk, 68, 77, the bovine, [sentence -> 0, chunk -> 4], []], [chunk, 79, 85, variety, [sentence -> 0, chunk -> 5], []], [chunk, 104, 115, the nation's, [sentence -> 0, chunk -> 6], []], [chunk, 139, 141, air, [sentence -> 0, chunk -> 7], []], [chunk, 143, 151, pollution, [sentence -> 0, chunk -> 8], []], [chunk, 158, 162, dairy, [sentence -> 0, chunk -> 9], []], [chunk, 164, 166, cow, [sentence -> 0, chunk -> 10], []], [chunk, 168, 173, manure, [sentence -> 0, chunk -> 11], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
            "|[[chunk, 5, 22, British Department, [sentence -> 0, chunk -> 0], []], [chunk, 28, 36, Education, [sentence -> 0, chunk -> 1], []], [chunk, 50, 53, DfES, [sentence -> 0, chunk -> 2], []], [chunk, 78, 92, Music Manifesto, [sentence -> 0, chunk -> 3], []], [chunk, 74, 77, a \"\", [sentence -> 0, chunk -> 4], []], [chunk, 93, 94, \"\", [sentence -> 0, chunk -> 5], []], [chunk, 96, 103, campaign, [sentence -> 0, chunk -> 6], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
            "|[[chunk, 25, 30, Netsky, [sentence -> 0, chunk -> 0], []], [chunk, 36, 41, Sasser, [sentence -> 0, chunk -> 1], []], [chunk, 160, 168, Wednesday, [sentence -> 0, chunk -> 2], []], [chunk, 191, 196, Sophos, [sentence -> 0, chunk -> 3], []], [chunk, 220, 226, Jaschan, [sentence -> 0, chunk -> 4], []], [chunk, 254, 260, Germany, [sentence -> 0, chunk -> 5], []], [chunk, 265, 267, May, [sentence -> 0, chunk -> 6], []], [chunk, 325, 330, Netsky, [sentence -> 0, chunk -> 7], []], [chunk, 336, 341, Sasser, [sentence -> 0, chunk -> 8], []], [chunk, 11, 16, author, [sentence -> 0, chunk -> 9], []], [chunk, 74, 80, percent, [sentence -> 0, chunk -> 10], []], [chunk, 85, 89, virus, [sentence -> 0, chunk -> 11], []], [chunk, 130, 134, month, [sentence -> 0, chunk -> 12], []], [chunk, 136, 140, virus, [sentence -> 0, chunk -> 13], []], [chunk, 142, 148, roundup, [sentence -> 0, chunk -> 14], []], [chunk, 173, 181, antivirus, [sentence -> 0, chunk -> 15], []], [chunk, 183, 189, company, [sentence -> 0, chunk -> 16], []], [chunk, 202, 203, \"\", [sentence -> 0, chunk -> 17], []], [chunk, 211, 214, year, [sentence -> 0, chunk -> 18], []], [chunk, 243, 249, custody, [sentence -> 0, chunk -> 19], []], [chunk, 272, 277, police, [sentence -> 0, chunk -> 20], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "|[[chunk, 170, 180, GPG/OpenPGP, [sentence -> 0, chunk -> 0], []], [chunk, 218, 221, FOAF, [sentence -> 0, chunk -> 1], []], [chunk, 293, 296, FOAF, [sentence -> 0, chunk -> 2], []], [chunk, 315, 317, PGP, [sentence -> 0, chunk -> 3], []], [chunk, 394, 396, PGP, [sentence -> 0, chunk -> 4], []], [chunk, 426, 434, FOAF\\file, [sentence -> 0, chunk -> 5], []], [chunk, 498, 500, PGP, [sentence -> 0, chunk -> 6], []], [chunk, 556, 559, FOAF, [sentence -> 0, chunk -> 7], []], [chunk, 0, 10, \\\\FOAF/LOAF, [sentence -> 0, chunk -> 8], []], [chunk, 17, 21, bloom, [sentence -> 0, chunk -> 9], []], [chunk, 36, 40, a lot, [sentence -> 0, chunk -> 10], []], [chunk, 72, 85, social\\network, [sentence -> 0, chunk -> 11], []], [chunk, 91, 99, whitelist, [sentence -> 0, chunk -> 12], []], [chunk, 101, 116, distribution.\\\\I, [sentence -> 0, chunk -> 13], []], [chunk, 138, 142, level, [sentence -> 0, chunk -> 14], []], [chunk, 198, 209, distribution, [sentence -> 0, chunk -> 15], []], [chunk, 223, 226, file, [sentence -> 0, chunk -> 16], []], [chunk, 319, 335, key fingerprint(s, [sentence -> 0, chunk -> 17], []], [chunk, 342, 357, identities?\\This, [sentence -> 0, chunk -> 18], []], [chunk, 370, 374, a lot, [sentence -> 0, chunk -> 19], []], [chunk, 476, 482, a bloom, [sentence -> 0, chunk -> 20], []], [chunk, 484, 489, filter, [sentence -> 0, chunk -> 21], []], [chunk, 527, 542, entire whitelist, [sentence -> 0, chunk -> 22], []], [chunk, 545, 554, the source, [sentence -> 0, chunk -> 23], []], [chunk, 561, 564, file, [sentence -> 0, chunk -> 24], []], [chunk, 575, 580, course, [sentence -> 0, chunk -> 25], []], [chunk, 582, 588, need\\to, [sentence -> 0, chunk -> 26], []], [chunk, 593, 604, encrypted )., [sentence -> 0, chunk -> 27], []], [chunk, 605, 610, \\\\Your, [sentence -> 0, chunk -> 28], []], [chunk, 612, 620, whitelist, [sentence -> 0, chunk -> 29], []], [chunk, 646, 663, the social network, [sentence -> 0, chunk -> 30], []], [chunk, 673, 701, client\\discovered new identit, [sentence -> 0, chunk -> 31], []], [chunk, 706, 707, \\\\, [sentence -> 0, chunk -> 32], []]]|\n",
            "|[[chunk, 1, 16, Wiltshire Police, [sentence -> 0, chunk -> 0], []], [chunk, 30, 31, \"\", [sentence -> 0, chunk -> 1], []], [chunk, 40, 41, \"\", [sentence -> 0, chunk -> 2], []], [chunk, 53, 57, fraud, [sentence -> 0, chunk -> 3], []], [chunk, 59, 63, squad, [sentence -> 0, chunk -> 4], []], [chunk, 65, 69, chief, [sentence -> 0, chunk -> 5], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "|[[chunk, 28, 31, UK's, [sentence -> 0, chunk -> 0], []], [chunk, 43, 46, card, [sentence -> 0, chunk -> 1], []], [chunk, 48, 52, fraud, [sentence -> 0, chunk -> 2], []], [chunk, 54, 57, unit, [sentence -> 0, chunk -> 3], []], [chunk, 81, 86, stolen, [sentence -> 0, chunk -> 4], []]]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQxc8H7IlhsP",
        "colab_type": "code",
        "outputId": "1c7f60c7-86c7-4f2d-91d3-a39ef3a30273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip('chunk.result', 'chunk.begin',  'chunk.end')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "        F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "        F.expr(\"cols['2']\").alias(\"end\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unions</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Turner   Newall</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Federal Mogul</td>\n",
              "      <td>113</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>stricken</td>\n",
              "      <td>92</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>parent</td>\n",
              "      <td>101</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>firm</td>\n",
              "      <td>108</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>TORONTO</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Canada</td>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ansari X Prize</td>\n",
              "      <td>82</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>A second team</td>\n",
              "      <td>20</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             chunk  begin  end\n",
              "0           Unions      0    5\n",
              "1  Turner   Newall     31   45\n",
              "2    Federal Mogul    113  125\n",
              "3         stricken     92   99\n",
              "4           parent    101  106\n",
              "5             firm    108  111\n",
              "6          TORONTO      1    7\n",
              "7           Canada     10   15\n",
              "8   Ansari X Prize     82   95\n",
              "9    A second team     20   32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3TayLw6199e",
        "colab_type": "text"
      },
      "source": [
        "## Dependency Parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qa0P1rBKlhqY",
        "colab_type": "code",
        "outputId": "09189c3e-4bbd-41f9-ddb6-a3a24b057cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "dep_parser = DependencyParserModel.pretrained('dependency_conllu')\\\n",
        "            .setInputCols(['document', 'pos', 'token'])\\\n",
        "            .setOutputCol('dependency')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dependency_conllu download started this may take some time.\n",
            "Approximate size to download 16.6 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfXqknLulhoP",
        "colab_type": "code",
        "outputId": "cbf3b890-c6ae-49fb-be56-bd3eaeffcd71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "typed_dep_parser = TypedDependencyParserModel.pretrained('dependency_typed_conllu')\\\n",
        "        .setInputCols([\"token\", \"pos\", \"dependency\"])\\\n",
        "        .setOutputCol(\"dependency_type\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dependency_typed_conllu download started this may take some time.\n",
            "Approximate size to download 257.4 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcWhditBlhmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stemmer,\n",
        " lemmatizer,\n",
        " pos,\n",
        " dep_parser,\n",
        " typed_dep_parser\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_sB-qIXlhkD",
        "colab_type": "code",
        "outputId": "596d2a00-f993-4fca-e479-7ad9644ae5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "\n",
        "result = pipelineModel.transform(news_df.limit(100))\n",
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'token.begin',  'token.end', 'dependency.result', 'dependency_type.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "        F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "        F.expr(\"cols['2']\").alias(\"end\"),\n",
        "        F.expr(\"cols['3']\").alias(\"dependency\"),\n",
        "        F.expr(\"cols['4']\").alias(\"dependency_type\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "      <th>dependency</th>\n",
              "      <th>dependency_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unions</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>root</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>representing</td>\n",
              "      <td>7</td>\n",
              "      <td>18</td>\n",
              "      <td>workers</td>\n",
              "      <td>amod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>workers</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>Unions</td>\n",
              "      <td>flat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>28</td>\n",
              "      <td>29</td>\n",
              "      <td>Turner</td>\n",
              "      <td>case</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Turner</td>\n",
              "      <td>31</td>\n",
              "      <td>36</td>\n",
              "      <td>workers</td>\n",
              "      <td>flat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Newall</td>\n",
              "      <td>40</td>\n",
              "      <td>45</td>\n",
              "      <td>say</td>\n",
              "      <td>nsubj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>say</td>\n",
              "      <td>47</td>\n",
              "      <td>49</td>\n",
              "      <td>Unions</td>\n",
              "      <td>parataxis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>they</td>\n",
              "      <td>51</td>\n",
              "      <td>54</td>\n",
              "      <td>disappointed</td>\n",
              "      <td>nsubj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>are</td>\n",
              "      <td>56</td>\n",
              "      <td>58</td>\n",
              "      <td>disappointed</td>\n",
              "      <td>nsubj</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>'</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>disappointed</td>\n",
              "      <td>case</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          chunk  begin  end    dependency dependency_type\n",
              "0        Unions      0    5          ROOT            root\n",
              "1  representing      7   18       workers            amod\n",
              "2       workers     20   26        Unions            flat\n",
              "3            at     28   29        Turner            case\n",
              "4        Turner     31   36       workers            flat\n",
              "5        Newall     40   45           say           nsubj\n",
              "6           say     47   49        Unions       parataxis\n",
              "7          they     51   54  disappointed           nsubj\n",
              "8           are     56   58  disappointed           nsubj\n",
              "9             '     60   60  disappointed            case"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBq183q73wtC",
        "colab_type": "text"
      },
      "source": [
        "## SpellChecker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01CLpbfXlhiW",
        "colab_type": "code",
        "outputId": "ffbae5f8-62d0-4f7d-b935-dfb11b0578b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "spell_checker = NorvigSweetingModel.pretrained('spellcheck_norvig')\\\n",
        "        .setInputCols(\"token\")\\\n",
        "        .setOutputCol(\"corrected\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spellcheck_norvig download started this may take some time.\n",
            "Approximate size to download 4.2 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWniY7D050N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spell_checker = SymmetricDeleteModel.pretrained('spellcheck_sd')\\\n",
        "#         .setInputCols(['token'])\\\n",
        "#         .setOutputCol(\"corrected\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kej7Pqx_lhgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stemmer,\n",
        " lemmatizer,\n",
        " pos,\n",
        " spell_checker\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyVwDT-1lheJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZBzVeiHlhcC",
        "colab_type": "code",
        "outputId": "42fdcb72-1b6d-4398-f3e5-8c31f900ce3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'corrected.result', 'stem.result',  'lemma.result', 'pos.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"corrected\"),\n",
        "        F.expr(\"cols['2']\").alias(\"stem\"),\n",
        "        F.expr(\"cols['3']\").alias(\"lemma\"),\n",
        "        F.expr(\"cols['4']\").alias(\"pos\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>corrected</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unions</td>\n",
              "      <td>Unions</td>\n",
              "      <td>union</td>\n",
              "      <td>Unions</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>representing</td>\n",
              "      <td>representing</td>\n",
              "      <td>repres</td>\n",
              "      <td>represent</td>\n",
              "      <td>VBG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>workers</td>\n",
              "      <td>workers</td>\n",
              "      <td>worker</td>\n",
              "      <td>worker</td>\n",
              "      <td>NNS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>at</td>\n",
              "      <td>IN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Turner</td>\n",
              "      <td>Turner</td>\n",
              "      <td>turner</td>\n",
              "      <td>Turner</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Newall</td>\n",
              "      <td>Newell</td>\n",
              "      <td>newal</td>\n",
              "      <td>Newall</td>\n",
              "      <td>NNP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>say</td>\n",
              "      <td>say</td>\n",
              "      <td>sai</td>\n",
              "      <td>say</td>\n",
              "      <td>VBP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>they</td>\n",
              "      <td>they</td>\n",
              "      <td>thei</td>\n",
              "      <td>they</td>\n",
              "      <td>PRP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>are</td>\n",
              "      <td>are</td>\n",
              "      <td>ar</td>\n",
              "      <td>be</td>\n",
              "      <td>VBP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>'</td>\n",
              "      <td>'</td>\n",
              "      <td>'</td>\n",
              "      <td>'</td>\n",
              "      <td>POS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          token     corrected    stem      lemma  pos\n",
              "0        Unions        Unions   union     Unions  NNP\n",
              "1  representing  representing  repres  represent  VBG\n",
              "2       workers       workers  worker     worker  NNS\n",
              "3            at            at      at         at   IN\n",
              "4        Turner        Turner  turner     Turner  NNP\n",
              "5        Newall        Newell   newal     Newall  NNP\n",
              "6           say           say     sai        say  VBP\n",
              "7          they          they    thei       they  PRP\n",
              "8           are           are      ar         be  VBP\n",
              "9             '             '       '          '  POS"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePeslvZk702c",
        "colab_type": "code",
        "outputId": "e3385ebb-2ea0-4ad7-fafd-6f69ad4ac9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "light_model = LightPipeline(pipelineModel)\n",
        "\n",
        "light_result = light_model.annotate('The patint has pain and headace')\n",
        "\n",
        "list(zip(light_result['token'], light_result['corrected']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'The'),\n",
              " ('patint', 'patient'),\n",
              " ('has', 'has'),\n",
              " ('pain', 'pain'),\n",
              " ('and', 'and'),\n",
              " ('headace', 'headache')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QItu_CC470zN",
        "colab_type": "code",
        "outputId": "ec4b3f2a-12f4-4aa3-9aff-e4113610bd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NER_cb957d2304b3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAMIa-S4HdhH",
        "colab_type": "text"
      },
      "source": [
        "## Word Embeddings (GloVe)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkOl0sT570wj",
        "colab_type": "code",
        "outputId": "20453c4d-1934-440e-cbff-951f1a475c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "glove_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
        "                    .setInputCols(['document', 'token'])\\\n",
        "                    .setOutputCol('embeddings')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSScN47P70uS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        "                               documentAssembler,\n",
        "                               tokenizer,\n",
        "                               glove_embeddings\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC6anSak70rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hKQLGwcLsg6",
        "colab_type": "code",
        "outputId": "e29c6dac-2358-4650-fdad-3a8e694a840b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "result.toPandas()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "      <th>document</th>\n",
              "      <th>token</th>\n",
              "      <th>embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>Unions representing workers at Turner   Newall...</td>\n",
              "      <td>[(document, 0, 126, Unions representing worker...</td>\n",
              "      <td>[(token, 0, 5, Unions, {'sentence': '0'}, []),...</td>\n",
              "      <td>[(word_embeddings, 0, 5, Unions, {'sentence': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>TORONTO, Canada    A second team of rocketeer...</td>\n",
              "      <td>[(document, 0, 222,  TORONTO, Canada    A seco...</td>\n",
              "      <td>[(token, 1, 7, TORONTO, {'sentence': '0'}, [])...</td>\n",
              "      <td>[(word_embeddings, 1, 7, TORONTO, {'sentence':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>A company founded by a chemistry researcher a...</td>\n",
              "      <td>[(document, 0, 209,  A company founded by a ch...</td>\n",
              "      <td>[(token, 1, 1, A, {'sentence': '0'}, []), (tok...</td>\n",
              "      <td>[(word_embeddings, 1, 1, A, {'sentence': '0', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>It's barely dawn when Mike Fitzpatrick starts...</td>\n",
              "      <td>[(document, 0, 267,  It's barely dawn when Mik...</td>\n",
              "      <td>[(token, 1, 4, It's, {'sentence': '0'}, []), (...</td>\n",
              "      <td>[(word_embeddings, 1, 4, It's, {'sentence': '0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>Southern California's smog fighting agency we...</td>\n",
              "      <td>[(document, 0, 174,  Southern California's smo...</td>\n",
              "      <td>[(token, 1, 8, Southern, {'sentence': '0'}, []...</td>\n",
              "      <td>[(word_embeddings, 1, 8, Southern, {'sentence'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Sports</td>\n",
              "      <td>England coach Sven Goran Eriksson said Tuesda...</td>\n",
              "      <td>[(document, 0, 171,  England coach Sven Goran ...</td>\n",
              "      <td>[(token, 1, 7, England, {'sentence': '0'}, [])...</td>\n",
              "      <td>[(word_embeddings, 1, 7, England, {'sentence':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Sports</td>\n",
              "      <td>Striker Emile Heskey has pulled out of the En...</td>\n",
              "      <td>[(document, 0, 176,  Striker Emile Heskey has ...</td>\n",
              "      <td>[(token, 1, 7, Striker, {'sentence': '0'}, [])...</td>\n",
              "      <td>[(word_embeddings, 1, 7, Striker, {'sentence':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>Business</td>\n",
              "      <td>\" Staples Inc. &amp;lt;A HREF=\"\"http://www.investo...</td>\n",
              "      <td>[(document, 0, 144, \" Staples Inc. &amp;lt;A HREF=...</td>\n",
              "      <td>[(token, 0, 0, \", {'sentence': '0'}, []), (tok...</td>\n",
              "      <td>[(word_embeddings, 0, 0, \", {'sentence': '0', ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>World</td>\n",
              "      <td>AGHDAD, Iraq, Aug. 17  A delegation of Iraqis ...</td>\n",
              "      <td>[(document, 0, 229, AGHDAD, Iraq, Aug. 17  A d...</td>\n",
              "      <td>[(token, 0, 5, AGHDAD, {'sentence': '0'}, []),...</td>\n",
              "      <td>[(word_embeddings, 0, 5, AGHDAD, {'sentence': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Business</td>\n",
              "      <td>U.S. consumer prices dropped in July  for the...</td>\n",
              "      <td>[(document, 0, 211,  U.S. consumer prices drop...</td>\n",
              "      <td>[(token, 1, 3, U.S, {'sentence': '0'}, []), (t...</td>\n",
              "      <td>[(word_embeddings, 1, 3, U.S, {'sentence': '0'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    category  ...                                         embeddings\n",
              "0   Business  ...  [(word_embeddings, 0, 5, Unions, {'sentence': ...\n",
              "1   Sci/Tech  ...  [(word_embeddings, 1, 7, TORONTO, {'sentence':...\n",
              "2   Sci/Tech  ...  [(word_embeddings, 1, 1, A, {'sentence': '0', ...\n",
              "3   Sci/Tech  ...  [(word_embeddings, 1, 4, It's, {'sentence': '0...\n",
              "4   Sci/Tech  ...  [(word_embeddings, 1, 8, Southern, {'sentence'...\n",
              "..       ...  ...                                                ...\n",
              "95    Sports  ...  [(word_embeddings, 1, 7, England, {'sentence':...\n",
              "96    Sports  ...  [(word_embeddings, 1, 7, Striker, {'sentence':...\n",
              "97  Business  ...  [(word_embeddings, 0, 0, \", {'sentence': '0', ...\n",
              "98     World  ...  [(word_embeddings, 0, 5, AGHDAD, {'sentence': ...\n",
              "99  Business  ...  [(word_embeddings, 1, 3, U.S, {'sentence': '0'...\n",
              "\n",
              "[100 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfvuY9mHM88x",
        "colab_type": "code",
        "outputId": "01162437-1122-48d3-e064-8bff6d7bcc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result.select('token.result').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Unions', 'representing', 'workers', 'at', 'Turner', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqKbY8sO70oS",
        "colab_type": "code",
        "outputId": "2dd60992-3d7d-4d64-8342-fab3fcd9d04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result.select('embeddings.embeddings').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(embeddings=[[0.7186499834060669, 0.8075399994850159, -1.1786999702453613, 0.271450012922287, -0.48833000659942627, -0.1893800050020218, -1.1789000034332275, 0.1783600002527237, -0.21995000541210175, -0.7215999960899353, -0.5354200005531311, -0.7507500052452087, 0.41176000237464905, 0.18951000273227692, -0.5093500018119812, -0.735010027885437, 0.33823999762535095, 0.3451400101184845, -0.16735999286174774, 0.130390003323555, 1.1055999994277954, 0.5508400201797485, -0.01406800001859665, -0.5322800278663635, -0.23598000407218933, -1.0496000051498413, -0.6549199819564819, -0.17997999489307404, 0.10341999679803848, -0.315530002117157, 0.31481000781059265, 0.8438799977302551, 0.305400013923645, -1.158400058746338, 0.047777000814676285, -0.04945499822497368, 0.7908999919891357, -0.22619999945163727, -0.6294800043106079, 0.3116999864578247, -0.7399600148200989, 0.1283400058746338, 0.10723999887704849, 0.3257099986076355, -0.2687300145626068, -0.70169997215271, 0.16669000685214996, 0.3308899998664856, -0.04666899889707565, 0.13434000313282013, -0.3903599977493286, 0.35043999552726746, -0.6380500197410583, 1.0003999471664429, -0.1405400037765503, -0.6695600152015686, 0.29333001375198364, -0.04130199924111366, 1.5181000232696533, 0.9282100200653076, -0.0594559982419014, -0.8433300256729126, -0.21630999445915222, -0.364329993724823, 0.0641229972243309, -0.0647289976477623, -0.32370999455451965, 0.32034000754356384, 0.9140099883079529, -0.8716099858283997, 0.14883999526500702, -0.4030100107192993, -1.2542999982833862, -0.7203999757766724, 0.22737999260425568, -0.27138999104499817, 0.25005999207496643, -1.037500023841858, -1.256500005722046, -0.12337999790906906, -0.09109599888324738, -0.980970025062561, 0.20313000679016113, -0.4651400148868561, -1.8416999578475952, -0.6694599986076355, 0.013848000206053257, 1.0679999589920044, -0.08525200188159943, -0.5746700167655945, 0.5118200182914734, -0.6081799864768982, -1.0676000118255615, 0.2016499936580658, -0.13197000324726105, 0.41545000672340393, -0.17709000408649445, 0.5877199769020081, 1.126099944114685, -0.5484700202941895], [0.2567099928855896, 0.30035001039505005, -0.1800599992275238, 0.4666599929332733, 0.985010027885437, 0.2320999950170517, -0.349590003490448, 0.26996999979019165, -0.9966700077056885, -0.43404000997543335, -0.17270000278949738, -1.267799973487854, 0.6458699703216553, -0.08055999875068665, -0.10688000172376633, -0.6668199896812439, 0.12387000024318695, 0.4363200068473816, -0.42590999603271484, 0.23579999804496765, 0.291949987411499, 0.22596000134944916, 0.5508999824523926, -0.42719000577926636, -0.38655999302864075, -0.827970027923584, 0.5543400049209595, -0.13357999920845032, 0.38161998987197876, -0.3511199951171875, 0.688979983329773, 0.2200700044631958, 0.2505500018596649, -0.6122499704360962, 0.2902100086212158, -0.33941999077796936, 0.4240800142288208, 0.7613300085067749, 0.08363699913024902, 2.965199928439688e-05, -0.482369989156723, -0.5486500263214111, 0.3570300042629242, 0.11528000235557556, -0.02913600020110607, 0.11303000152111053, 0.06495899707078934, -0.2025900036096573, -0.5543599724769592, 0.05228099972009659, 0.2602599859237671, -0.19032999873161316, 0.44394999742507935, 0.4488700032234192, -0.5525699853897095, -0.8007299900054932, -0.23316000401973724, -0.5227100253105164, 1.0738999843597412, 0.7515199780464172, -0.043306998908519745, 0.14406999945640564, 0.01055500004440546, 0.06772799789905548, 0.10349000245332718, -0.10739000141620636, 0.30557000637054443, 0.9226899743080139, 0.3269599974155426, 0.1467200070619583, 0.1668899953365326, 0.416269987821579, -0.7763100266456604, 0.2741599977016449, 0.5012400150299072, -0.05087300017476082, 0.053603000938892365, -0.6662099957466125, -1.0152000188827515, -0.46022000908851624, -0.1068200021982193, 0.11057999730110168, -0.12228000164031982, -0.3601199984550476, -0.5704500079154968, -0.567110002040863, -0.45089998841285706, 0.011590000241994858, 0.7907800078392029, -0.19756999611854553, 0.11934000253677368, -0.5461699962615967, -0.4099999964237213, -0.29580000042915344, -0.4774700105190277, -0.092399001121521, -0.507669985294342, -0.053971000015735626, 1.0384999513626099, -0.6452800035476685], [0.5059199929237366, 0.717170000076294, -0.6723600029945374, -0.3211199939250946, -0.5828499794006348, -0.4797700047492981, -0.5024300217628479, 0.6059399843215942, 0.2570900022983551, 0.039739999920129776, -0.03192700073122978, -0.30160999298095703, 0.9995200037956238, 0.010672000236809254, -0.4207099974155426, -0.8516799807548523, 0.6623200178146362, -0.12134999781847, -0.21413999795913696, -0.11890000104904175, 1.055400013923645, 0.43880000710487366, 0.4526999890804291, -0.027317000553011894, -0.3484100103378296, -0.9506000280380249, -0.5811399817466736, -0.018124999478459358, 0.0834140032529831, -0.44440001249313354, 0.3066999912261963, 0.5232599973678589, -0.39351001381874084, -0.40887999534606934, -0.3237999975681305, -0.3342199921607971, 0.0872030034661293, 0.2745499908924103, 0.26190000772476196, 0.029593000188469887, -0.808489978313446, -0.4887700080871582, 0.05382699891924858, 0.00015449999773409218, 0.4752599895000458, 0.03002000041306019, -0.5453600287437439, 0.10292000323534012, -0.11695999652147293, -0.07692000269889832, -0.5880399942398071, -0.6906399726867676, -1.1171000003814697, 1.704800009727478, -0.5256900191307068, -0.6114699840545654, -0.2827399969100952, -0.8695099949836731, 2.5090999603271484, 0.2172500044107437, 0.0027075998950749636, 0.19708000123500824, -0.025436000898480415, -0.45805999636650085, 0.5940200090408325, 0.0884770005941391, 0.5312299728393555, -0.14328999817371368, 0.7018300294876099, -0.5690799951553345, -0.2890099883079529, 0.0835610032081604, -1.1719000339508057, -0.5269100069999695, 0.9336100220680237, 0.5628799796104431, 0.8940799832344055, 0.004341599997133017, -0.8695799708366394, -0.1148499995470047, 0.7176700234413147, -0.2693899869918823, -0.4161800146102905, -0.29308998584747314, -2.13319993019104, -0.5161100029945374, 0.17951999604701996, 0.5551199913024902, -0.44345998764038086, -0.11869999766349792, 1.0706000328063965, -0.7666900157928467, -0.027691999450325966, 0.008368399925529957, -0.7859399914741516, -0.023787999525666237, -0.038107000291347504, 0.5258499979972839, 1.0542000532150269, -0.49779999256134033], [0.17659999430179596, 0.0938510000705719, 0.24350999295711517, 0.4431299865245819, -0.3903700113296509, 0.1252399981021881, -0.199180006980896, 0.598550021648407, -0.8203499913215637, 0.2800599932670593, 0.5423099994659424, 0.023079000413417816, 0.128370001912117, -0.044488999992609024, 0.38370001316070557, -0.7565900087356567, 0.402539998292923, -0.44620001316070557, -0.8159899711608887, -0.00915130041539669, 0.6521900296211243, -0.04365599900484085, 0.5491899847984314, -0.1669600009918213, 0.7302799820899963, -0.20702999830245972, -0.06986299902200699, -0.31259000301361084, 0.27226001024246216, 0.0849049985408783, -0.6049799919128418, 0.4282599985599518, 0.6013399958610535, 0.5095300078392029, -0.39072999358177185, 0.4460799992084503, -0.3633100092411041, 0.5085800290107727, -0.20307999849319458, -0.4350300133228302, -0.08682700246572495, -0.865809977054596, -1.0151000022888184, -0.35725000500679016, -0.12993000447750092, 0.33239999413490295, 0.3025999963283539, 0.06727699935436249, -0.52947998046875, -0.8122299909591675, 0.3956199884414673, -0.7953699827194214, 0.24331000447273254, 1.250599980354309, -1.0168999433517456, -3.339099884033203, -0.7969099879264832, -0.3387700021266937, 1.3660000562667847, 0.8751299977302551, -0.6370099782943726, 0.6838099956512451, -0.05743199959397316, 0.12541000545024872, -0.8258000016212463, -0.5611699819564819, 0.30807000398635864, 0.15449999272823334, 0.6147300004959106, 0.6740300059318542, -0.6083300113677979, -0.2591100037097931, -0.3561899960041046, -0.7118899822235107, -0.31207001209259033, 0.03523800149559975, 0.22487999498844147, -0.3349199891090393, -1.1585999727249146, -0.1737300008535385, 0.9593700170516968, 0.2447900027036667, -0.4620499908924103, -0.07594099640846252, -1.0844000577926636, 0.09367600083351135, 0.4854600131511688, 0.13007999956607819, 0.23454999923706055, -0.27963998913764954, -0.24481000006198883, -0.01621299982070923, 0.463019996881485, 1.029099941253662, -0.8181700110435486, 0.17521999776363373, 0.06797000020742416, 0.05630499869585037, 1.2311999797821045, 0.4069499969482422], [0.5163400173187256, -0.37185999751091003, -0.21775999665260315, -1.0115000009536743, 0.40139999985694885, -0.48410001397132874, 0.36274001002311707, -0.295199990272522, -0.4225800037384033, -0.6284400224685669, 0.677079975605011, 0.1855199933052063, -0.4560000002384186, -0.27423998713493347, 0.1948699951171875, -0.5341399908065796, 0.2921000123023987, 0.006397299934178591, -0.4053399860858917, 0.5121300220489502, -0.5194399952888489, 0.3393799960613251, 0.20580999553203583, 0.011291000060737133, 0.638159990310669, 0.014355000108480453, 0.38120999932289124, 0.43830999732017517, 0.17278000712394714, 0.4677099883556366, 0.17659999430179596, 1.063599944114685, 0.375789999961853, -0.32771000266075134, 0.07033199816942215, -0.06979800015687943, 0.16213999688625336, 0.47082000970840454, -0.15928000211715698, 0.008898500353097916, 0.29624998569488525, 0.01586499996483326, -0.2184399962425232, 0.47314000129699707, -0.6265900135040283, 0.6628699898719788, -0.28477001190185547, -0.5817899703979492, 0.8282600045204163, -0.5599899888038635, -0.09991899877786636, -0.12115000188350677, 0.6798499822616577, -0.010594000108540058, 0.33733999729156494, -1.8145999908447266, 0.12054000049829483, -0.4724299907684326, 0.23309999704360962, 0.3434399962425232, 0.4185599982738495, 0.49876999855041504, 0.3278700113296509, -0.16469000279903412, 0.6084700226783752, -0.0990080013871193, 0.7826200127601624, 0.9123899936676025, -0.32385000586509705, 0.5744799971580505, 0.0430929996073246, -0.007449700031429529, -0.3242799937725067, 0.13481000065803528, 0.293969988822937, -0.4789600074291229, -0.08049900084733963, 0.0689380019903183, -0.4150199890136719, 0.43048998713493347, 0.18953999876976013, -0.2772499918937683, 0.6576300263404846, -0.29875001311302185, -0.4518899917602539, -0.1478700041770935, -0.04415399953722954, -0.34918999671936035, 0.015381000004708767, 0.06195800006389618, 0.4260999858379364, -0.6062099933624268, 0.13437999784946442, -0.05736999958753586, -0.2874299883842468, 0.018695000559091568, -0.04918399825692177, 0.025245999917387962, 0.11162000149488449, 0.17564000189304352], [-0.38857001066207886, -1.1448999643325806, -0.4173699915409088, -0.3196899890899658, -0.165460005402565, -0.7044000029563904, 0.1287499964237213, -0.2604700028896332, 0.072843998670578, -0.13313999772071838, 0.30695000290870667, 0.30621999502182007, -0.8055999875068665, 0.14076000452041626, 0.5093799829483032, 0.42829999327659607, -0.38648998737335205, -0.12078999727964401, 0.2690100073814392, 0.15445999801158905, -0.9643399715423584, 0.2184399962425232, 0.16495999693870544, -0.40257999300956726, 0.6275100111961365, 0.32618001103401184, 0.18004000186920166, 0.638480007648468, -0.14651000499725342, -0.10473000258207321, 0.36970001459121704, 0.0886709988117218, -0.02137799933552742, 0.08805199712514877, -0.48030000925064087, -0.4234499931335449, 0.10125000029802322, -0.4432399868965149, 0.37178000807762146, 0.3024100065231323, 0.3133299946784973, 0.7542200088500977, -0.7046700119972229, -0.021052999421954155, -0.3361999988555908, 0.5351099967956543, 0.0952330008149147, 0.5565699934959412, 0.48232001066207886, 0.07991600036621094, -0.2377299964427948, -0.11638999730348587, -0.17384999990463257, -0.02286599949002266, -0.10662999749183655, 0.08519899845123291, 0.2601099908351898, -0.231330007314682, -0.9035699963569641, -0.42778998613357544, 0.7268999814987183, -0.3881799876689911, 0.017417000606656075, 0.711329996585846, 0.4788999855518341, 0.16690999269485474, 0.0955440029501915, 0.5983399748802185, 0.28762000799179077, 0.35558000206947327, 0.3403699994087219, -0.4964100122451782, -0.21911999583244324, 0.120619997382164, -0.15283000469207764, 0.5616300106048584, 0.14643999934196472, 0.23893000185489655, 0.6222500205039978, 0.10824000090360641, -0.06342200189828873, -0.16155000030994415, 0.5354899764060974, 0.06302300095558167, 0.5450999736785889, -0.01792999915778637, 0.2535499930381775, -0.5223199725151062, 0.17302000522613525, 0.145579993724823, -0.23666000366210938, -0.799239993095398, 0.013623000122606754, 0.18332000076770782, 0.7623199820518494, 0.020555999130010605, -0.08327099680900574, 0.4963800013065338, 0.32493001222610474, 0.04841899871826172], [-0.09168200194835663, 0.5810499787330627, 0.40476998686790466, -0.41978999972343445, -0.8511099815368652, -0.2871899902820587, -0.4194900095462799, -0.10424000024795532, 0.4531700015068054, -0.09907300025224686, -0.1378600001335144, 0.4999000132083893, 0.5004900097846985, -0.2404100000858307, -0.3510099947452545, -0.22020000219345093, -0.06305500119924545, 0.23212000727653503, -0.6916199922561646, 0.7367100119590759, 0.5555499792098999, 0.20746999979019165, 0.11246000230312347, -0.2594200074672699, -0.6325600147247314, 0.12417999655008316, -0.3407500088214874, -0.8563799858093262, 0.10582000017166138, -0.6459000110626221, 0.5820099711418152, 0.6431400179862976, 0.07336899638175964, 0.2123900055885315, 0.029330000281333923, 0.19413000345230103, 0.003222400089725852, 0.48618000745773315, 0.5226699709892273, -0.2737799882888794, -0.874970018863678, -0.13734999299049377, 0.022004999220371246, -0.42271000146865845, -0.5339999794960022, -0.5925400257110596, 0.2692900002002716, -0.3820599913597107, -0.23627999424934387, -0.9333000183105469, 0.3929100036621094, -0.05321599915623665, -0.2512800097465515, 1.0638999938964844, -0.4897100031375885, -2.078900098800659, -0.09421300143003464, -0.21049000322818756, 1.4549000263214111, 0.6456999778747559, -0.3437800109386444, 0.6449999809265137, -0.04327400028705597, -0.5112000107765198, 0.7958199977874756, 0.2083899974822998, 0.7155500054359436, 0.7110900282859802, -0.2954699993133545, -0.06414400041103363, 0.21279999613761902, -0.4695099890232086, -0.35920000076293945, -0.5095700025558472, -0.33882999420166016, -0.21740999817848206, 0.3600499927997589, -0.13832999765872955, -1.0957000255584717, -0.07588399946689606, 1.087499976158142, -0.48247000575065613, -0.5852199792861938, -0.41196000576019287, -2.060800075531006, -0.42155998945236206, 0.0884459987282753, 0.16319000720977783, -0.5987100005149841, -0.08922100067138672, 0.5861899852752686, -0.41773998737335205, -0.30713000893592834, -0.5583800077438354, -0.6597499847412109, -0.08092600107192993, -0.34134000539779663, -0.3874100148677826, 0.3469499945640564, 0.2290000021457672], [-0.07953999936580658, 0.30171000957489014, 0.07951600104570389, -0.7466199994087219, -0.6787899732589722, 0.3502900004386902, -0.19754000008106232, 0.492900013923645, 0.1416199952363968, -0.23789000511169434, 0.10938999801874161, 0.23465000092983246, 0.7776299715042114, 0.12745000422000885, 0.1087300032377243, -0.6802399754524231, 0.2569600045681, 0.5398100018501282, -0.9229400157928467, 0.08830899745225906, 0.5523999929428101, 0.0733409970998764, 0.6342399716377258, -0.09483399987220764, -0.06898800283670425, -0.1128700003027916, -0.1932000070810318, -0.6123300194740295, 0.16718000173568726, -0.4310699999332428, 0.2935500144958496, 0.4258800148963928, -0.22193999588489532, 0.1478700041770935, 0.5369300246238708, 0.12846000492572784, 0.12732000648975372, 0.5089899897575378, 0.24079999327659607, -0.3513000011444092, -0.5248600244522095, -0.37476998567581177, -0.08438199758529663, -0.3959299921989441, -0.14876000583171844, -0.03095100075006485, 0.48431000113487244, -0.24677999317646027, 0.1234700009226799, -1.103700041770935, -0.094930000603199, -0.03843899816274643, 0.10750000178813934, 1.6517000198364258, -0.10341999679803848, -2.433199882507324, 0.04048600047826767, -0.3916400074958801, 1.5943000316619873, 0.9589099884033203, -0.5249599814414978, 1.1476000547409058, -0.5750200152397156, 0.044766999781131744, 1.097000002861023, -0.15884000062942505, 0.5974299907684326, 0.6671199798583984, -0.09516800194978714, -0.5617300271987915, -0.06752300262451172, -0.7989400029182434, 0.06640499830245972, -0.8259099721908569, -0.5870100259780884, 0.180649995803833, -0.28442999720573425, -0.0916450023651123, -1.0480999946594238, -0.05198799818754196, 0.7593600153923035, -0.039712000638246536, -0.9376000165939331, 0.056432999670505524, -2.162600040435791, -0.6435499787330627, 0.42302000522613525, -0.10571999847888947, -0.7261099815368652, -0.12939999997615814, -0.38102999329566956, -0.7458999752998352, -0.009676399640738964, -0.16204999387264252, -1.0562000274658203, -0.11998999863862991, -0.6090899705886841, -0.16730999946594238, 0.653439998626709, 0.3659999966621399], [-0.5153300166130066, 0.8318600058555603, 0.22457000613212585, -0.7386500239372253, 0.18717999756336212, 0.2602100074291229, -0.4256399869918823, 0.671209990978241, -0.31084001064300537, -0.6127499938011169, 0.08952599763870239, -0.24010999500751495, 1.1878000497817993, 0.6760900020599365, -0.022885000333189964, -0.9253299832344055, 0.0711740031838417, 0.38837000727653503, -0.42923998832702637, 0.37143999338150024, 0.3267099857330322, 0.431410014629364, 0.8749499917030334, 0.3400900065898895, -0.2318899929523468, -0.41144001483917236, 0.4906100034713745, -0.32905998826026917, -0.49108999967575073, -0.18987999856472015, 0.3340800106525421, -0.21244999766349792, -0.3838599920272827, -0.08054699748754501, 1.1160999536514282, 0.2361699938774109, 0.3133299946784973, 0.49285998940467834, 0.10000000149011612, -0.151309996843338, -0.14176000654697418, -0.2802000045776367, -0.23880000412464142, -0.35486000776290894, 0.18282000720500946, -0.19133999943733215, 0.6054400205612183, 0.07457300275564194, -0.2073100060224533, -0.6096500158309937, 0.19908000528812408, -0.5702400207519531, -0.17427000403404236, 1.4419000148773193, -0.25018998980522156, -1.864799976348877, 0.4167099893093109, -0.246069997549057, 1.5010000467300415, 0.8741499781608582, -0.6713500022888184, 1.2762000560760498, -0.2721000015735626, 0.17583000659942627, 1.2242000102996826, 0.28242000937461853, 0.6237499713897705, 0.6395099759101868, 0.36913999915122986, -0.8467699885368347, -0.32269999384880066, -0.6715199947357178, -0.19634999334812164, -0.4078899919986725, -0.20965999364852905, -0.1962299942970276, 0.041884999722242355, 0.5396699905395508, -1.1104999780654907, -0.39515000581741333, 0.6658999919891357, -0.2329999953508377, -1.0820000171661377, 0.04646499827504158, -2.099299907684326, -0.2849299907684326, 0.08002500236034393, -0.12962999939918518, -0.30011001229286194, -0.4676400125026703, -0.818310022354126, -0.04850900173187256, -0.3223299980163574, -0.32012999057769775, -1.1207000017166138, -0.05678800120949745, -0.7300400137901306, -1.2023999691009521, 1.1303999423980713, 0.34790000319480896], [-0.3456200063228607, -0.2499299943447113, 0.5867800116539001, -0.8911899924278259, -1.0953999757766724, -0.4507800042629242, -0.07454899698495865, -0.4477899968624115, -0.38492000102996826, -0.49233999848365784, -0.4565599858760834, -0.13328999280929565, 0.18579000234603882, 0.5662999749183655, -0.20201000571250916, -0.3950600028038025, 0.2179100066423416, 1.0369000434875488, -0.27008000016212463, 0.5163000226020813, 0.9096300005912781, 0.6380000114440918, 0.06831599771976471, 0.3900099992752075, 0.2454500049352646, 0.23652000725269318, -0.45475998520851135, -0.6232500076293945, 0.3883500099182129, -0.33044999837875366, 0.8034899830818176, 0.3418000042438507, -0.4699000120162964, 0.41376999020576477, -0.07501000165939331, -0.36963000893592834, -0.2709900140762329, 1.2762000560760498, -0.21642999351024628, -0.40139999985694885, 0.2618100047111511, -0.04493800178170204, 0.32548001408576965, 0.040403999388217926, 0.6220800280570984, -0.19769999384880066, -1.1014000177383423, -0.20589999854564667, 0.8350899815559387, -0.5019199848175049, -0.6549400091171265, 0.42921000719070435, -0.329259991645813, 1.4270999431610107, -1.4865000247955322, -2.397900104522705, 0.9349300265312195, 0.22123000025749207, 1.382099986076355, 0.03232799842953682, -0.3801000118255615, 0.7457699775695801, -0.28080999851226807, -0.9026899933815002, 0.8893100023269653, 0.16672000288963318, 0.5924000144004822, 0.2685999870300293, -0.7648199796676636, -0.21684999763965607, -0.09033799916505814, 0.45489999651908875, -0.7797799706459045, -1.3977999687194824, -0.20202000439167023, -0.3510900139808655, 0.08754800260066986, 0.08208099752664566, -1.1562999486923218, -0.47655999660491943, 1.0377999544143677, 0.11394999921321869, 0.11529000103473663, -0.7287600040435791, -2.191999912261963, -0.1832599937915802, -0.3607499897480011, -0.5494999885559082, 0.3326599895954132, 0.7166399955749512, -0.3002200126647949, -0.06874699890613556, -0.014708000235259533, 0.5183799862861633, -0.020844999700784683, -0.42555001378059387, -0.1932400017976761, -1.3106000423431396, 1.0293999910354614, -0.05879399925470352], [-0.007581300102174282, 0.14488999545574188, 0.6421599984169006, -0.27101001143455505, -1.2273000478744507, -0.16433000564575195, -0.37713998556137085, 0.2883000075817108, 0.26732999086380005, -0.6085100173950195, 0.1693899929523468, 0.28415000438690186, 0.29412999749183655, -1.3288999795913696, -0.7658600211143494, -0.06884100288152695, -1.2421000003814697, -0.055720001459121704, -0.006532900035381317, 0.7798100113868713, 0.4120599925518036, 0.17375999689102173, 0.020843999460339546, 0.08718200027942657, -0.11784999817609787, -0.001383200054988265, 0.1587900072336197, -0.03622300177812576, 0.7257400155067444, 0.1632699966430664, 0.011711999773979187, 0.5464000105857849, -0.0407009981572628, 0.04538000002503395, 0.216839998960495, 0.19634999334812164, -0.1286800056695938, 0.5867099761962891, -0.09666000306606293, -0.4260900020599365, -0.6967700123786926, -0.2946000099182129, 0.5299999713897705, -0.28022000193595886, -0.11022999882698059, -0.3430800139904022, 0.7914000153541565, 0.5018600225448608, 0.2590999901294708, -1.2170000076293945, 0.5778700113296509, -0.5953599810600281, 0.36410000920295715, 0.0779310017824173, -0.5644199848175049, -1.4220999479293823, -0.006037299986928701, 0.3448199927806854, 0.5694699883460999, -0.08419399708509445, -0.31286999583244324, -0.16097000241279602, -0.6402900218963623, 0.037696000188589096, -0.018492000177502632, -0.6342800259590149, 0.4007599949836731, 1.253499984741211, -0.12849999964237213, -0.3964900076389313, 0.4749700129032135, 0.060458000749349594, -0.31859999895095825, -0.4398300051689148, -0.017193999141454697, 0.5263299942016602, 0.3165999948978424, 0.22609999775886536, -0.42333999276161194, -0.24880999326705933, -0.035027001053094864, -0.1817999929189682, -0.6664599776268005, -0.0346670001745224, -1.5872000455856323, -0.9776899814605713, -0.482479989528656, 0.04678399860858917, -1.0400999784469604, -0.6004999876022339, -0.08064600080251694, -0.4371100068092346, -0.3274399936199188, 0.16690999269485474, -0.42034998536109924, 0.5533400177955627, -0.1695600003004074, 0.6168400049209595, -0.4226300120353699, 0.36438998579978943], [-0.3456200063228607, -0.2499299943447113, 0.5867800116539001, -0.8911899924278259, -1.0953999757766724, -0.4507800042629242, -0.07454899698495865, -0.4477899968624115, -0.38492000102996826, -0.49233999848365784, -0.4565599858760834, -0.13328999280929565, 0.18579000234603882, 0.5662999749183655, -0.20201000571250916, -0.3950600028038025, 0.2179100066423416, 1.0369000434875488, -0.27008000016212463, 0.5163000226020813, 0.9096300005912781, 0.6380000114440918, 0.06831599771976471, 0.3900099992752075, 0.2454500049352646, 0.23652000725269318, -0.45475998520851135, -0.6232500076293945, 0.3883500099182129, -0.33044999837875366, 0.8034899830818176, 0.3418000042438507, -0.4699000120162964, 0.41376999020576477, -0.07501000165939331, -0.36963000893592834, -0.2709900140762329, 1.2762000560760498, -0.21642999351024628, -0.40139999985694885, 0.2618100047111511, -0.04493800178170204, 0.32548001408576965, 0.040403999388217926, 0.6220800280570984, -0.19769999384880066, -1.1014000177383423, -0.20589999854564667, 0.8350899815559387, -0.5019199848175049, -0.6549400091171265, 0.42921000719070435, -0.329259991645813, 1.4270999431610107, -1.4865000247955322, -2.397900104522705, 0.9349300265312195, 0.22123000025749207, 1.382099986076355, 0.03232799842953682, -0.3801000118255615, 0.7457699775695801, -0.28080999851226807, -0.9026899933815002, 0.8893100023269653, 0.16672000288963318, 0.5924000144004822, 0.2685999870300293, -0.7648199796676636, -0.21684999763965607, -0.09033799916505814, 0.45489999651908875, -0.7797799706459045, -1.3977999687194824, -0.20202000439167023, -0.3510900139808655, 0.08754800260066986, 0.08208099752664566, -1.1562999486923218, -0.47655999660491943, 1.0377999544143677, 0.11394999921321869, 0.11529000103473663, -0.7287600040435791, -2.191999912261963, -0.1832599937915802, -0.3607499897480011, -0.5494999885559082, 0.3326599895954132, 0.7166399955749512, -0.3002200126647949, -0.06874699890613556, -0.014708000235259533, 0.5183799862861633, -0.020844999700784683, -0.42555001378059387, -0.1932400017976761, -1.3106000423431396, 1.0293999910354614, -0.05879399925470352], [0.37711000442504883, -0.34470999240875244, 0.13404999673366547, -0.011710000224411488, -0.19426999986171722, 0.41464000940322876, 0.4060800075531006, 0.43062999844551086, -0.05705999955534935, -0.1992100030183792, 0.432669997215271, -0.016269000247120857, 0.21709999442100525, -0.002614900004118681, 0.39423999190330505, -0.42803001403808594, -0.01749500073492527, -0.5665799975395203, -0.44558000564575195, -0.18528999388217926, 0.26732000708580017, -0.1571200042963028, 0.21657000482082367, 0.7971400022506714, 0.6962299942970276, 0.2040500044822693, -0.4990699887275696, -0.4551900029182434, 0.382099986076355, 0.2060299962759018, -0.21605999767780304, 0.10092999786138535, -0.501479983329773, -0.11057999730110168, -0.4345499873161316, -0.26785001158714294, -0.2023400068283081, 0.0038320000749081373, -0.49107998609542847, -0.17642000317573547, -0.889710009098053, -0.27900001406669617, 0.8638700246810913, -0.01735600084066391, 0.31209999322891235, 0.4100399911403656, 0.2319899946451187, -0.6081200242042542, 0.44762998819351196, -0.8957899808883667, -0.03849099949002266, -0.2577199935913086, 0.394679993391037, 1.6186000108718872, -0.5488200187683105, -3.029099941253662, -0.7784500122070312, -0.3246299922466278, 1.7657999992370605, 0.9730299711227417, -0.39342001080513, 0.5481100082397461, 0.013163999654352665, 0.3785000145435333, 0.2453799992799759, 0.031078999862074852, 0.23627999424934387, 0.2890099883079529, 0.02704700082540512, 0.28984999656677246, -0.7452300190925598, 0.011517000384628773, -0.3945600092411041, -0.5770599842071533, -0.6360399723052979, 0.31022000312805176, -0.3831700086593628, -0.07766299694776535, -1.3538999557495117, 0.018008999526500702, 0.8564599752426147, 0.03825899958610535, -0.3943699896335602, 0.4433099925518036, -1.080199956893921, -0.4315899908542633, 0.1439100056886673, 0.11853999644517899, -0.5645899772644043, -0.4796600043773651, 0.22859999537467957, -0.24368999898433685, -0.4282299876213074, 1.0365999937057495, -0.8307099938392639, 0.12460000067949295, 0.20630000531673431, 0.5423200130462646, 0.11424999684095383, -0.6692699790000916], [-0.5788900256156921, -0.40292999148368835, 0.41172000765800476, 1.2513999938964844, 0.35229000449180603, -1.3415000438690186, 0.2820200026035309, 0.1426900029182434, 0.10179000347852707, -0.6770700216293335, -0.2632099986076355, 0.5232099890708923, 1.0801000595092773, 0.6777899861335754, -0.3607900142669678, 0.12962999939918518, -0.8265799880027771, -0.9217000007629395, -0.4011400043964386, -0.6662099957466125, -0.10221000015735626, -0.3951700031757355, 0.19452999532222748, 0.15584999322891235, 0.12239000201225281, -0.12654000520706177, -0.5379199981689453, 0.19035999476909637, 0.11736000329256058, -0.12353000044822693, -1.2188999652862549, 0.1775200068950653, 0.06288599967956543, -0.35837000608444214, 0.02927199937403202, 0.4806100130081177, 0.8622599840164185, -0.3111500144004822, -0.4609200060367584, -0.12143000215291977, -0.9769300222396851, -0.040437001734972, 0.8862800002098083, 0.2273399978876114, -0.4742799997329712, -0.4021899998188019, -0.5615900158882141, 0.02390499971807003, 0.2302599996328354, -0.4801900088787079, 0.22432999312877655, -0.30320000648498535, -0.0944259986281395, 0.990119993686676, -0.05650800094008446, -2.3129000663757324, -0.6858500242233276, -0.15794000029563904, 1.3258999586105347, 0.733299970626831, -0.5356699824333191, 0.19133999943733215, -0.5324900150299072, -1.1269999742507935, 0.13673999905586243, 0.45662999153137207, 0.26684999465942383, 1.2177000045776367, -0.5628600120544434, -0.41683998703956604, 0.42114999890327454, -0.9482200145721436, -0.742389976978302, -1.5937999486923218, -0.2850100100040436, 0.23683999478816986, -0.45396000146865845, -0.30959001183509827, -1.5928000211715698, -0.0904499962925911, 0.6240599751472473, -0.11638999730348587, 0.48917001485824585, -0.11948999762535095, -1.0465999841690063, 0.5319899916648865, 0.03388499841094017, 0.8753499984741211, -0.019832000136375427, -0.8773800134658813, -0.028149999678134918, 0.051107000559568405, -1.166100025177002, 0.3783999979496002, -0.6783499717712402, 0.5486199855804443, 0.36230000853538513, 0.3414599895477295, 1.1259000301361084, 0.6499599814414978], [-0.4360800087451935, 0.39103999733924866, 0.516569972038269, -0.13861000537872314, 0.2029000073671341, 0.5072299838066101, -0.012543999589979649, 0.22947999835014343, -0.631600022315979, 0.21198999881744385, -0.018043000251054764, -0.39364001154899597, 0.7416399717330933, 0.3022100031375885, 0.5179200172424316, -0.251910001039505, 0.25372999906539917, -0.6518399715423584, -0.42963001132011414, 0.009362200275063515, 0.023334000259637833, -0.3924500048160553, 0.349480003118515, 0.21217000484466553, 0.7346000075340271, -0.2196200042963028, -0.02861100062727928, -0.34641000628471375, -0.20934000611305237, -0.27090999484062195, -0.17636999487876892, 0.82396000623703, -0.08233900368213654, -0.0348690003156662, 0.07972200214862823, 0.3484100103378296, 0.6088700294494629, 0.22811000049114227, -0.2963300049304962, 0.18633000552654266, 0.23399999737739563, -0.709659993648529, 0.16312000155448914, -0.20857000350952148, 0.09236899763345718, -0.07543499767780304, -0.13905000686645508, -0.35120999813079834, -0.19971999526023865, -0.41686999797821045, -0.31485000252723694, 0.16122999787330627, 0.038881998509168625, 1.6654000282287598, -0.12400999665260315, -3.341900110244751, 0.1092899963259697, -0.02619899995625019, 1.24399995803833, 0.8437399864196777, -0.15679000318050385, 0.7904099822044373, -0.042433001101017, 0.18884000182151794, 0.06434500217437744, -0.1168299987912178, 1.0467000007629395, 0.7181299924850464, 0.5783399939537048, 0.27013999223709106, -0.5090799927711487, -0.08399499952793121, -0.1437000036239624, -0.7640799880027771, 0.27417999505996704, 0.5681399703025818, -0.39375001192092896, -0.32558000087738037, -0.9285399913787842, -0.1309799998998642, 1.3277000188827515, 0.11851000040769577, -0.15550999343395233, 0.5971999764442444, -1.0839999914169312, -0.05813699960708618, 0.23885999619960785, 0.145579993724823, -0.5930299758911133, -0.4751099944114685, -0.22064000368118286, -0.3759100139141083, -0.7964900135993958, 0.013465000316500664, -0.4459500014781952, -0.34623000025749207, -0.7539799809455872, -0.3517000079154968, 0.9945600032806396, 0.08819600194692612], [-0.11282999813556671, 0.7653300166130066, 0.6446700096130371, 0.1842699944972992, -0.19002999365329742, 0.23714999854564667, -0.05802600085735321, -0.14211000502109528, 0.38468998670578003, 0.6078900098800659, -0.3142000138759613, 0.5435100197792053, 0.030271999537944794, -0.4512900114059448, 0.017520999535918236, -0.5222300291061401, 0.7859500050544739, -0.7816600203514099, -0.24677999317646027, 0.04839500039815903, -0.0034568000119179487, 0.5400500297546387, 0.2959100008010864, -0.057238999754190445, 0.05564900115132332, 0.33103999495506287, -0.8218500018119812, 0.6083899736404419, -0.1834699958562851, 0.44029998779296875, 0.7214400172233582, -0.18605999648571014, 0.7125200033187866, 0.12815000116825104, -0.9749299883842468, 0.03708000108599663, 0.38256001472473145, 0.23534999787807465, -0.28571999073028564, 0.12335000187158585, -0.8108299970626831, -0.1057400032877922, 0.06492999941110611, -0.336650013923645, 1.2943999767303467, 0.5556899905204773, 0.008269700221717358, 0.49107998609542847, 0.2354699969291687, -0.1401199996471405, -0.9315800070762634, 0.43898001313209534, 0.05976000055670738, 0.3922500014305115, 0.09828700125217438, -0.6474000215530396, -0.2717300057411194, -0.280349999666214, 0.8560600280761719, 0.5214599967002869, 0.31942999362945557, 0.9513700008392334, 0.32190999388694763, -0.25523999333381653, 0.06370700150728226, 1.0686999559402466, -0.5587199926376343, 0.21604999899864197, 0.06790199875831604, -0.7274199724197388, -0.31683000922203064, -0.2653299868106842, -0.615119993686676, -0.06509999930858612, 1.0200999975204468, 1.0283000469207764, -0.05609399825334549, 0.7148500084877014, -0.39399999380111694, -0.39678001403808594, 0.5613899827003479, 0.4387100040912628, -0.3796199858188629, 0.22363999485969543, -0.5209900140762329, -0.12427999824285507, -0.021245000883936882, 0.13744999468326569, -1.1241999864578247, -0.35401999950408936, 0.028768999502062798, 0.04729299992322922, 0.49494999647140503, -0.15660999715328217, -0.25148001313209534, 0.9825500249862671, 0.06425400078296661, 0.3996100127696991, -0.5863699913024902, -0.754040002822876], [0.8168200254440308, -0.08447899669408798, 0.10582000017166138, -0.09696800261735916, -0.40623000264167786, 0.245619997382164, -0.26368001103401184, -0.11647000163793564, -0.2964800000190735, -0.19724999368190765, 0.40206998586654663, -0.34876999258995056, 0.7182599902153015, -0.3012300133705139, -0.20430999994277954, -0.7391499876976013, 0.17750999331474304, -0.24796000123023987, 0.45302000641822815, 0.81836998462677, -0.4276700019836426, 0.16498999297618866, -0.26282998919487, 0.7226499915122986, 0.08420900255441666, 0.046858999878168106, 0.3475799858570099, 0.030100999400019646, -0.3721800148487091, 0.64860999584198, 0.769320011138916, 1.3483999967575073, 0.09092999994754791, -0.6919199824333191, -0.16141000390052795, 0.39381998777389526, 0.5805799961090088, 0.18976999819278717, 0.6922399997711182, -0.3671799898147583, 0.4923200011253357, 0.3575400114059448, -0.6515200138092041, -0.026580000296235085, -0.21254000067710876, 0.7483900189399719, -0.18776999413967133, 0.6740000247955322, -0.14350999891757965, -0.669950008392334, -0.22032000124454498, -0.37264999747276306, -0.27452000975608826, 0.09581500291824341, 0.23343999683856964, -1.3949999809265137, -0.09892000257968903, -0.22578999400138855, 1.7908999919891357, 0.7389199733734131, 0.9982799887657166, -0.07462199777364731, 0.33131998777389526, -0.382779985666275, 0.5643200278282166, 0.21172000467777252, -0.10023999959230423, 0.7872200012207031, 0.24145999550819397, 0.2962999939918518, -0.2787199914455414, -0.2325199991464615, -0.17858999967575073, -0.4784199893474579, 0.26076000928878784, 0.4000000059604645, -0.07991400361061096, -1.2007999420166016, -1.0312000513076782, -0.18729999661445618, -0.3384700119495392, 0.11706999689340591, 0.09044100344181061, 0.7994599938392639, -1.1026999950408936, -0.4706900119781494, -0.024952000007033348, -0.11112000048160553, 0.7003700137138367, -0.32471001148223877, -0.27511000633239746, -0.2017499953508377, 0.6383299827575684, -0.21296000480651855, -0.4768199920654297, 0.5130400061607361, 0.21166999638080597, -0.41019999980926514, 0.4440299868583679, -0.46713998913764954], [0.09685300290584564, -0.12647999823093414, -0.7963200211524963, -0.19088000059127808, 0.02132599987089634, -0.9179400205612183, -0.18299999833106995, -0.29989999532699585, -0.4381699860095978, 0.22019000351428986, 0.7611100077629089, -0.41628000140190125, 0.6470000147819519, -0.1420699954032898, -0.7948899865150452, -0.4664599895477295, 0.5280699729919434, -0.47560998797416687, 0.10277000069618225, 0.19182999432086945, -0.1151299998164177, 0.21091000735759735, 0.22474999725818634, -0.14677999913692474, -0.374889999628067, -0.10542000085115433, 0.16820000112056732, 0.028082000091671944, -0.973330020904541, -0.4029799997806549, 0.2537800073623657, 1.1639000177383423, 0.01032899972051382, -0.08742199838161469, -0.3755500018596649, 0.29506999254226685, 0.32027000188827515, -0.24422000348567963, 0.6087200045585632, -0.4664599895477295, 0.5277199745178223, -0.5432000160217285, -0.03254900127649307, -0.367000013589859, -0.4566200077533722, -0.6264700293540955, 0.048037998378276825, 0.2033900022506714, -0.09445399791002274, -0.6043800115585327, 0.31393998861312866, -0.3930400013923645, 0.6108300089836121, 0.4789600074291229, -0.20492999255657196, -2.4286999702453613, -0.4542199969291687, -0.4212000072002411, 1.6721999645233154, 0.3998199999332428, 0.492900013923645, -0.7272899746894836, -0.2064799964427948, 0.3360399901866913, -0.16369999945163727, -0.3383699953556061, 0.1383100003004074, 0.7547600269317627, 0.32701000571250916, 0.3006899952888489, -0.44321998953819275, 0.7955700159072876, -1.2070000171661377, -0.6233100295066833, 0.09158600121736526, -0.6909000277519226, 0.03573700040578842, -0.6994400024414062, -0.9071900248527527, 0.02001499943435192, 1.4825999736785889, 0.5405799746513367, -0.4018799960613251, 0.7540299892425537, -0.45576000213623047, -0.7686499953269958, 0.5347999930381775, 0.04707200080156326, -0.00986700039356947, -0.7246800065040588, -0.26603999733924866, -0.41631001234054565, 0.14382000267505646, -0.4056299924850464, 0.08776199817657471, 0.8262799978256226, -0.2647700011730194, -0.3682900071144104, 0.28839001059532166, 0.5906400084495544], [0.5341299772262573, -0.33316999673843384, 0.5725600123405457, -0.22958999872207642, 0.4509199857711792, 0.03400399908423424, -1.0741000175476074, 0.30035001039505005, -0.47394001483917236, -0.11800000071525574, -0.1035899966955185, 0.2716600000858307, -0.7885500192642212, 0.23797999322414398, -0.29203000664711, -0.01610100083053112, 0.8705400228500366, 0.41280999779701233, -0.4311400055885315, -0.06515700370073318, 0.1333799958229065, 0.007348599843680859, 0.5308200120925903, -0.16487999260425568, -1.0752999782562256, -0.5021600127220154, -0.23265999555587769, -1.2865999937057495, -1.0568000078201294, 0.1265999972820282, 1.0740000009536743, 0.30191999673843384, -0.38124001026153564, -0.2962999939918518, -0.47137001156806946, 0.03184499964118004, 0.06343799829483032, -0.20970000326633453, 0.818589985370636, 0.6564900279045105, -0.24178999662399292, -0.8293300271034241, -0.05778000131249428, 0.2862899899482727, 0.3564800024032593, -0.2803899943828583, 0.15298999845981598, -1.0049999952316284, -0.4476900100708008, -0.605400025844574, 0.39632999897003174, -0.643339991569519, 0.5058299899101257, 0.8031799793243408, -0.2038400024175644, -1.6425000429153442, 0.544979989528656, -0.8424299955368042, 2.668800115585327, 0.6750100255012512, -0.20239000022411346, -0.017822999507188797, 0.34902000427246094, 0.09205000102519989, 0.9571899771690369, -0.35905998945236206, -0.5443999767303467, 0.510420024394989, 0.41343000531196594, 0.47315001487731934, 0.27612999081611633, -0.41464999318122864, -0.24532000720500946, -0.05082099884748459, -0.06980500370264053, -0.1627500057220459, -0.2372400015592575, -0.19682000577449799, -1.4853999614715576, -0.31099000573158264, 0.870959997177124, -0.21449999511241913, -0.36243000626564026, -0.2530199885368347, -1.1050000190734863, -0.07546299695968628, 0.2842400074005127, 0.3709999918937683, -0.07353600114583969, -0.6071400046348572, 0.31883999705314636, -1.0821000337600708, -1.0324000120162964, -0.16986000537872314, 0.006710300222039223, 0.32649001479148865, 0.7165899872779846, 0.21640999615192413, 0.5339199900627136, -0.6038399934768677], [0.5300599932670593, -0.039771001785993576, 0.6609100103378296, -0.39517998695373535, 0.1505099982023239, -0.31801000237464905, -0.3871400058269501, -0.6244000196456909, -0.2819199860095978, 0.19884000718593597, 0.8210099935531616, 0.06665199995040894, -0.021601999178528786, -1.1535999774932861, -0.07554599642753601, -0.34220001101493835, 0.2962700128555298, -0.921239972114563, -0.26883000135421753, 0.4619799852371216, -0.3386000096797943, 0.5033400058746338, -0.4065299928188324, -0.15029999613761902, 0.15489999949932098, 0.07020100206136703, 0.476610004901886, 0.1934099942445755, -0.4388900101184845, -0.7623199820518494, 0.8357399702072144, 0.8544399738311768, 0.06573300063610077, -0.0927790030837059, 0.4584699869155884, 0.5056099891662598, -0.30379000306129456, -0.49884000420570374, -0.33878999948501587, 0.09720899909734726, 0.4219599962234497, 0.4309200048446655, 0.05464800074696541, -0.1897599995136261, -0.18380999565124512, -0.3343600034713745, -0.543429970741272, -0.05595500022172928, 0.709659993648529, 0.058329999446868896, -0.8664100170135498, 0.18694999814033508, 0.6684600114822388, 0.07206200063228607, 0.47819000482559204, -0.9133700132369995, -1.4018000364303589, 0.5366600155830383, 0.5212900042533875, 0.3445500135421753, 0.45173001289367676, 0.18334999680519104, 0.3103100061416626, 0.08829300105571747, 0.2728999853134155, 0.029765000566840172, 1.3869999647140503, 0.7267000079154968, -0.6606400012969971, -0.005873799789696932, -0.5245800018310547, -0.17780999839305878, -0.5523300170898438, 0.3324599862098694, -1.3636000156402588, -1.09089994430542, 0.493120014667511, -0.06805899739265442, 0.3252300024032593, 0.4070900082588196, 0.47154000401496887, -0.2348099946975708, 0.7739400267601013, 0.7327399849891663, 0.2750599980354309, -0.13979999721050262, -0.9348700046539307, -0.26467999815940857, 0.10312999784946442, -0.7406700253486633, 1.2403000593185425, -0.3475100100040436, -0.1599300056695938, -0.02994599938392639, 0.9991400241851807, -0.14665000140666962, 0.29930999875068665, -0.5475299954414368, 0.22777000069618225, -0.6036900281906128], [-0.3397899866104126, 0.20940999686717987, 0.46347999572753906, -0.6479200124740601, -0.3837699890136719, 0.038033999502658844, 0.1712699979543686, 0.15977999567985535, 0.46619001030921936, -0.019169000908732414, 0.41479000449180603, -0.34349000453948975, 0.26872000098228455, 0.04464000090956688, 0.4213100075721741, -0.4103200137615204, 0.15458999574184418, 0.022238999605178833, -0.6465299725532532, 0.25255998969078064, 0.043136000633239746, -0.1944500058889389, 0.4651600122451782, 0.4565100073814392, 0.6858800053596497, 0.09129499644041061, 0.21875, -0.7035099864006042, 0.16785000264644623, -0.35078999400138855, -0.12634000182151794, 0.6638399958610535, -0.2581999897956848, 0.03654199838638306, -0.1360500007867813, 0.4025300145149231, 0.14289000630378723, 0.3813199996948242, -0.12283000349998474, -0.45886000990867615, -0.2528199851512909, -0.30432000756263733, -0.11214999854564667, -0.261819988489151, -0.22482000291347504, -0.44554001092910767, 0.29910001158714294, -0.8561199903488159, -0.14503000676631927, -0.4908599853515625, 0.008297299966216087, -0.17490999400615692, 0.2752400040626526, 1.4400999546051025, -0.2123900055885315, -2.8434998989105225, -0.2795799970626831, -0.4572199881076813, 1.6385999917984009, 0.7880799770355225, -0.552619993686676, 0.6499999761581421, 0.08642599731683731, 0.3901199996471405, 1.0631999969482422, -0.3537899851799011, 0.4832800030708313, 0.34599998593330383, 0.8417400121688843, 0.09870699793100357, -0.24212999641895294, -0.27052998542785645, 0.04528699815273285, -0.40147000551223755, 0.11394999921321869, 0.006222600117325783, 0.03667299821972847, 0.018518000841140747, -1.021299958229065, -0.20805999636650085, 0.640720009803772, -0.06876300275325775, -0.5863500237464905, 0.33476001024246216, -1.1432000398635864, -0.11479999870061874, -0.2509100139141083, -0.45906999707221985, -0.09681899845600128, -0.17946000397205353, -0.06335099786520004, -0.6741200089454651, -0.06889499723911285, 0.5360400080680847, -0.8777300119400024, 0.3180199861526489, -0.3924199938774109, -0.2339400053024292, 0.47297999262809753, -0.02880300022661686]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vC2OnLhn70k7",
        "colab_type": "code",
        "outputId": "31f0241c-9564-4c46-b370-ef2f3f325a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'embeddings.embeddings')).alias('cols'))\\\n",
        ".select(F.expr(\"cols['0']\").alias('token'),\n",
        "        F.expr(\"cols['1']\").alias('embeddings'))\n",
        "\n",
        "result_df.show(10, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "|       token|                                                                                          embeddings|\n",
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "|      Unions|[0.71865, 0.80754, -1.1787, 0.27145, -0.48833, -0.18938, -1.1789, 0.17836, -0.21995, -0.7216, -0....|\n",
            "|representing|[0.25671, 0.30035, -0.18006, 0.46666, 0.98501, 0.2321, -0.34959, 0.26997, -0.99667, -0.43404, -0....|\n",
            "|     workers|[0.50592, 0.71717, -0.67236, -0.32112, -0.58285, -0.47977, -0.50243, 0.60594, 0.25709, 0.03974, -...|\n",
            "|          at|[0.1766, 0.093851, 0.24351, 0.44313, -0.39037, 0.12524, -0.19918, 0.59855, -0.82035, 0.28006, 0.5...|\n",
            "|      Turner|[0.51634, -0.37186, -0.21776, -1.0115, 0.4014, -0.4841, 0.36274, -0.2952, -0.42258, -0.62844, 0.6...|\n",
            "|      Newall|[-0.38857, -1.1449, -0.41737, -0.31969, -0.16546, -0.7044, 0.12875, -0.26047, 0.072844, -0.13314,...|\n",
            "|         say|[-0.091682, 0.58105, 0.40477, -0.41979, -0.85111, -0.28719, -0.41949, -0.10424, 0.45317, -0.09907...|\n",
            "|        they|[-0.07954, 0.30171, 0.079516, -0.74662, -0.67879, 0.35029, -0.19754, 0.4929, 0.14162, -0.23789, 0...|\n",
            "|         are|[-0.51533, 0.83186, 0.22457, -0.73865, 0.18718, 0.26021, -0.42564, 0.67121, -0.31084, -0.61275, 0...|\n",
            "|           '|[-0.34562, -0.24993, 0.58678, -0.89119, -1.0954, -0.45078, -0.074549, -0.44779, -0.38492, -0.4923...|\n",
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4h-VVlhONvj",
        "colab_type": "text"
      },
      "source": [
        "## ELMO Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTcn9V7-lhXJ",
        "colab_type": "code",
        "outputId": "2c90a696-8f99-41ca-c883-8c11d6d02da1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "elmo_embeddings = ElmoEmbeddings.pretrained('elmo')\\\n",
        "          .setInputCols([\"document\", \"token\"])\\\n",
        "          .setOutputCol(\"embeddings\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo download started this may take some time.\n",
            "Approximate size to download 334.1 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajXa0LOqOMV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        "                               documentAssembler,\n",
        "                               tokenizer,\n",
        "                               elmo_embeddings\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMCISt32OMTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaYjL7ICOMQ1",
        "colab_type": "code",
        "outputId": "e4296edd-620f-464d-d56f-2ccad73843fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'embeddings.embeddings')).alias('cols'))\\\n",
        ".select(F.expr(\"cols['0']\").alias('token'),\n",
        "        F.expr(\"cols['1']\").alias('embeddings'))\n",
        "\n",
        "result_df.show(10, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "|       token|                                                                                          embeddings|\n",
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "|      Unions|[0.5389476, 0.4226915, -0.16224553, -0.1128005, -0.12524837, -0.28185326, 0.12341033, -0.22033125...|\n",
            "|representing|[0.1591705, 0.95000935, -0.12492801, 0.9341251, 0.5854504, 0.7605221, -0.30143818, -0.3193327, -0...|\n",
            "|     workers|[0.082813635, -0.17253807, -0.73114246, 0.56554866, 0.36294985, -0.42002633, 0.66982335, -0.71128...|\n",
            "|          at|[-0.11999596, 0.01299414, -0.078183725, 0.037628517, -0.268691, 0.13629477, -0.27565888, 0.343611...|\n",
            "|      Turner|[0.32819325, -0.11992816, 0.15701114, 0.560922, -0.2836563, -0.1923972, 0.04214704, 0.5058229, -0...|\n",
            "|      Newall|[-0.17683865, -0.5143349, -0.45629728, -7.754117E-4, -0.35344997, 0.13644686, -0.7342554, -0.7672...|\n",
            "|         say|[0.2758479, 0.5425131, 0.11938737, -0.97200084, -0.3782459, 0.382091, -0.0076833013, -0.055179976...|\n",
            "|        they|[0.37071806, -0.086831875, -0.17744617, -0.40313995, 0.27108797, -0.30554208, -0.079359025, 0.175...|\n",
            "|         are|[-0.031240113, 0.080358244, -0.28241903, 0.2300406, 0.55738544, 0.040288165, 0.013518998, -0.0653...|\n",
            "|           '|[-0.14411095, 1.057355, -0.08126159, 0.77940845, 0.3371486, 0.4044072, 0.2514691, -0.35001674, -0...|\n",
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35wbh123VkFJ",
        "colab_type": "text"
      },
      "source": [
        "## BERT Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypiQ22iTOMOr",
        "colab_type": "code",
        "outputId": "d980bbf2-711f-4896-9c24-aea28b2568c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "bert_embeddings = BertEmbeddings.pretrained('bert_base_uncased')\\\n",
        "          .setInputCols([\"document\", \"token\"])\\\n",
        "          .setOutputCol(\"embeddings\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_base_uncased download started this may take some time.\n",
            "Approximate size to download 392.5 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DrDNJ7wOMMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        "                               documentAssembler,\n",
        "                               tokenizer,\n",
        "                               bert_embeddings\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXmdfjDzOMKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2__neAdZOMII",
        "colab_type": "code",
        "outputId": "02329ab0-25d6-4750-c40f-297166023711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'embeddings.embeddings')).alias('cols'))\\\n",
        ".select(F.expr(\"cols['0']\").alias('token'),\n",
        "        F.expr(\"cols['1']\").alias('embeddings'))\n",
        "\n",
        "result_df.show(10, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "|       token|                                                                                          embeddings|\n",
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "|      Unions|[0.3423483, -0.7621089, -0.95610934, -0.9080744, 0.03278944, -0.19068101, 0.06841645, 0.8715485, ...|\n",
            "|representing|[-0.6609322, 0.36830586, -0.04385671, -0.062400907, -0.21250877, -0.3625311, -0.13388896, -9.9757...|\n",
            "|     workers|[-0.49138808, -0.0533655, -1.1493174, -0.6050249, -0.7109844, 0.20254612, 0.53452104, -0.6757567,...|\n",
            "|          at|[-0.59642524, 0.13935125, 0.09018701, -0.6268516, -0.75282824, 0.16119936, 0.10998525, 0.6566241,...|\n",
            "|      Turner|[0.87067986, 0.6809796, 0.7334639, 0.03486839, 0.07190508, 0.5168021, 0.18955632, 0.76651955, 0.5...|\n",
            "|      Newall|[0.17950639, -0.34025913, 0.25570604, 0.21317199, -0.33208883, 0.04610029, 0.3697966, -0.25145322...|\n",
            "|         say|[-0.5500088, 0.14300083, 0.28959557, 0.7080928, -0.99415845, 0.84212625, -0.80492914, -0.03846728...|\n",
            "|        they|[-0.7176471, 0.13001046, 0.46331078, -0.2354101, -0.35284284, 0.8512978, -0.34209916, -0.00592160...|\n",
            "|         are|[-0.19961491, 0.113651864, 0.5010431, -0.59946537, -0.9589555, 0.43561456, 0.32378542, -0.9751272...|\n",
            "|           '|[-0.65872747, 0.31395823, 0.1350823, -0.57405597, 0.49498263, -0.6595236, -0.038321733, 0.1784142...|\n",
            "+------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3amnqb7WTtQ",
        "colab_type": "text"
      },
      "source": [
        "## Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCNWtuaZOMFP",
        "colab_type": "code",
        "outputId": "6d09f0d7-2016-4ac4-bf3f-26d3f74b6791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# We will not send \"token\" but only document\n",
        "\n",
        "# no need for token columns \n",
        "use_embeddings = UniversalSentenceEncoder.pretrained('tfhub_use').\\\n",
        "  setInputCols([\"document\"]).\\\n",
        "  setOutputCol(\"sentence_embeddings\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4jPzuuxOMBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        "                               documentAssembler,\n",
        "                               tokenizer,\n",
        "                               use_embeddings\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj_5xYqoOL_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JScBXLHOL8E",
        "colab_type": "code",
        "outputId": "05ad046c-ad5d-4da7-88a9-0aa736f31aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip('document.result', 'sentence_embeddings.embeddings')).alias('cols'))\\\n",
        ".select(F.expr(\"cols['0']\").alias('document'),\n",
        "        F.expr(\"cols['1']\").alias('embeddings'))\n",
        "\n",
        "result_df.show(10, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                            document|                                                                                          embeddings|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stric...|[0.01299754, 0.019844763, -0.02462647, 0.039759073, -0.04424647, 0.01319757, 0.0786744, -0.053629...|\n",
            "| TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize,...|[0.0019999044, 0.051844064, -0.0440291, -5.932768E-4, -0.038505998, -0.02727945, 0.069404684, -0....|\n",
            "| A company founded by a chemistry researcher at the University of Louisville won a grant to devel...|[0.038641855, 0.023220802, -0.0040162755, 0.0719947, 0.02727973, -0.05895184, -0.001953841, 0.002...|\n",
            "| It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures an...|[0.07239491, 0.065592386, -0.015342037, -0.02381427, 0.013229337, -0.046775304, 0.009602194, 0.02...|\n",
            "| Southern California's smog fighting agency went after emissions of the bovine variety Friday, ad...|[-0.034784008, -0.0437179, -8.897013E-4, 0.06168911, 0.033439934, 0.021937974, 0.053599276, -0.05...|\n",
            "|\"The British Department for Education and Skills (DfES) recently launched a \"\"Music Manifesto\"\" c...|[-0.022449756, -0.005116286, -0.011258117, 0.037045613, -0.0015617298, -0.046843972, 0.08016194, ...|\n",
            "|\"confessed author of the Netsky and Sasser viruses, is responsible for 70 percent of virus infect...|[4.522043E-4, -0.044989046, -0.027128313, 0.057110913, 0.04432687, -0.039309442, -0.00496823, -0....|\n",
            "|\\\\FOAF/LOAF  and bloom filters have a lot of interesting properties for social\\network and whitel...|[0.045447677, -0.0673045, 0.047636587, -0.020948866, -0.025080303, 0.03516154, 0.0047589587, 0.03...|\n",
            "|               \"Wiltshire Police warns about \"\"phishing\"\" after its fraud squad chief was targeted.\"|[-0.0051346202, -0.07799961, -0.03284713, -0.010246719, 0.037006848, -0.0535042, 0.009521304, -0....|\n",
            "|In its first two years, the UK's dedicated card fraud unit, has recovered 36,000 stolen cards and...|[0.007892213, -0.070118815, -0.028799068, -0.018356517, 0.02493721, -0.07232398, 0.085445836, 0.0...|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC3TFpadX7Q3",
        "colab_type": "text"
      },
      "source": [
        "## Loading Models from local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAmEOkOGOL5o",
        "colab_type": "code",
        "outputId": "44976b7e-0ef8-4eab-d300-d1830e945556",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!cd ~/cache_pretrained && pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/cache_pretrained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "337jC1BZOL2x",
        "colab_type": "code",
        "outputId": "0e7094cc-14f9-4d05-b3e1-ea4f2b26977d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!cd ~/cache_pretrained && ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 44\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 20:46 bert_base_uncased_en_2.4.0_2.4_1580579889322\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 18:29 dependency_conllu_en_2.0.8_2.4_1561435004077\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 18:29 dependency_typed_conllu_en_2.0.8_2.4_1561473259215\n",
            "drwxr-xr-x 3 root root 4096 Apr 20 20:14 elmo_en_2.4.0_2.4_1580488815299\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 19:46 glove_100d_en_2.4.0_2.4_1579690104032\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 17:57 lemma_antbnc_en_2.0.2_2.4_1556480454569\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 17:59 ner_crf_en_2.4.0_2.4_1580237286004\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 18:09 pos_anc_en_2.0.2_2.4_1556659930154\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 18:35 spellcheck_norvig_en_2.1.0_2.4_1563017660080\n",
            "drwxr-xr-x 4 root root 4096 Apr 20 18:44 spellcheck_sd_en_2.1.0_2.4_1563019290368\n",
            "drwxr-xr-x 3 root root 4096 Apr 20 20:50 tfhub_use_en_2.4.0_2.4_1587136330099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwPLnCy6OLzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_embeddings = WordEmbeddings.load('/root/cache_pretrained/glove_100d_en_2.4.0_2.4_1579690104032')\\\n",
        "                    .setInputCols(['document' 'token'])\\\n",
        "                    .setOutputCol('glove_embeddings')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsnCMDnraSZ9",
        "colab_type": "text"
      },
      "source": [
        "## Using Your Custom Embeddings in Spark NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ob5GFzflhUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_embeddings = WordEmbeddings()\\\n",
        "  .setInputCols([\"sentence\", \"token\"])\\\n",
        "  .setOutputCol(\"my_embeddings\")\\\n",
        "  .setStoragePath('PubMed-shuffle-win-2.bin', \"BINARY\")\\\n",
        "  .setDimension(200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXnzC7NSa3o9",
        "colab_type": "text"
      },
      "source": [
        "## Getting Sentence Embeddings from GloVe, Elmo and BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3w-2TAmakCN",
        "colab_type": "code",
        "outputId": "86babc21-bcd5-4ce5-e09b-d21ca966ee98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "glove_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
        "                    .setInputCols(['document', 'token'])\\\n",
        "                    .setOutputCol('embeddings')\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings()\\\n",
        "                    .setInputCols(['document', 'embeddings'])\\\n",
        "                    .setOutputCol('sentence_embeddings')\\\n",
        "                    .setPoolingStrategy(\"AVERAGE\") # or SUM\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "                               documentAssembler,\n",
        "                               tokenizer,\n",
        "                               glove_embeddings,\n",
        "                               embeddingsSentence # we get average vector of vector representatios of tokens\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "nlpPipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aH-Zgzwakd6",
        "colab_type": "code",
        "outputId": "06b491e6-1eb4-4efb-e3b9-c82513797a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))\n",
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip('document.result', 'sentence_embeddings.embeddings')).alias('cols'))\\\n",
        ".select(F.expr(\"cols['0']\").alias('document'),\n",
        "        F.expr(\"cols['1']\").alias('sentence_embeddings'))\n",
        "\n",
        "result_df.show(10, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                            document|                                                                                 sentence_embeddings|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stric...|[0.01299754, 0.019844763, -0.02462647, 0.039759073, -0.04424647, 0.01319757, 0.0786744, -0.053629...|\n",
            "| TORONTO, Canada    A second team of rocketeers competing for the  #36;10 million Ansari X Prize,...|[0.0019999044, 0.051844064, -0.0440291, -5.932768E-4, -0.038505998, -0.02727945, 0.069404684, -0....|\n",
            "| A company founded by a chemistry researcher at the University of Louisville won a grant to devel...|[0.038641855, 0.023220802, -0.0040162755, 0.0719947, 0.02727973, -0.05895184, -0.001953841, 0.002...|\n",
            "| It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures an...|[0.07239491, 0.065592386, -0.015342037, -0.02381427, 0.013229337, -0.046775304, 0.009602194, 0.02...|\n",
            "| Southern California's smog fighting agency went after emissions of the bovine variety Friday, ad...|[-0.034784008, -0.0437179, -8.897013E-4, 0.06168911, 0.033439934, 0.021937974, 0.053599276, -0.05...|\n",
            "|\"The British Department for Education and Skills (DfES) recently launched a \"\"Music Manifesto\"\" c...|[-0.022449756, -0.005116286, -0.011258117, 0.037045613, -0.0015617298, -0.046843972, 0.08016194, ...|\n",
            "|\"confessed author of the Netsky and Sasser viruses, is responsible for 70 percent of virus infect...|[4.522043E-4, -0.044989046, -0.027128313, 0.057110913, 0.04432687, -0.039309442, -0.00496823, -0....|\n",
            "|\\\\FOAF/LOAF  and bloom filters have a lot of interesting properties for social\\network and whitel...|[0.045447677, -0.0673045, 0.047636587, -0.020948866, -0.025080303, 0.03516154, 0.0047589587, 0.03...|\n",
            "|               \"Wiltshire Police warns about \"\"phishing\"\" after its fraud squad chief was targeted.\"|[-0.0051346202, -0.07799961, -0.03284713, -0.010246719, 0.037006848, -0.0535042, 0.009521304, -0....|\n",
            "|In its first two years, the UK's dedicated card fraud unit, has recovered 36,000 stolen cards and...|[0.007892213, -0.070118815, -0.028799068, -0.018356517, 0.02493721, -0.07232398, 0.085445836, 0.0...|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oKty24bedHb",
        "colab_type": "text"
      },
      "source": [
        "## Cosine Similarity Between 2 embeddings (Sentence Similarity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhdjhA59akaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "\n",
        "v1 = result_df.select('sentence_embeddings').take(2)[0][0]\n",
        "v2 = result_df.select('sentence_embeddings').take(2)[0][0] # ayni cumlenin similarity'sine bkacagiz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP-xL9NfakYA",
        "colab_type": "code",
        "outputId": "a19f6343-6df7-47d8-93ea-b4506035fa10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# similarity = 1 - distance(cosine)\n",
        "1 - distance.cosine(np.array(v1), np.array(v2)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rap6YYfMh0I6",
        "colab_type": "text"
      },
      "source": [
        "## NerDLModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlfSHJFcg-L6",
        "colab_type": "text"
      },
      "source": [
        "### Public NER\n",
        "\n",
        "Available Entities are : PERSON, LOCATION, ORGANIZATION, MISC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv3amf9VakVp",
        "colab_type": "code",
        "outputId": "6a5e4e57-6b89-4bf5-a034-461c24a577bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "public_ner = NerDLModel.pretrained(\"ner_dl\", 'en') \\\n",
        "          .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
        "          .setOutputCol(\"ner\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ner_dl download started this may take some time.\n",
            "Approximate size to download 13.6 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65157624-ca2d-48ed-eddd-0c242621ea1b",
        "id": "xC_yEuhkhNyC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# ner_dl model is trained with glove_100d. So we use the same embeddings in the pipeline\n",
        "glove_embeddings = WordEmbeddingsModel.pretrained('glove_100d').\\\n",
        "  setInputCols([\"document\", 'token']).\\\n",
        "  setOutputCol(\"embeddings\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " glove_embeddings,\n",
        " public_ner\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR1Z9pwtakQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = nlpPipelineModel.transform(news_df.limit(100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wMKPFihakOf",
        "colab_type": "code",
        "outputId": "0be28f5c-e2c3-475a-c8f4-81a3a37d2611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(100))\n",
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'ner.result')).alias('cols'))\\\n",
        ".select(F.expr(\"cols['0']\").alias('token'),\n",
        "        F.expr(\"cols['1']\").alias('ner_label'))\n",
        "\n",
        "result_df.show(30, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+---------+\n",
            "|       token|ner_label|\n",
            "+------------+---------+\n",
            "|      Unions|        O|\n",
            "|representing|        O|\n",
            "|     workers|        O|\n",
            "|          at|        O|\n",
            "|      Turner|    B-ORG|\n",
            "|      Newall|    I-ORG|\n",
            "|         say|        O|\n",
            "|        they|        O|\n",
            "|         are|        O|\n",
            "|           '|        O|\n",
            "|disappointed|        O|\n",
            "|           '|        O|\n",
            "|       after|        O|\n",
            "|       talks|        O|\n",
            "|        with|        O|\n",
            "|    stricken|        O|\n",
            "|      parent|        O|\n",
            "|        firm|        O|\n",
            "|     Federal|    B-ORG|\n",
            "|       Mogul|    I-ORG|\n",
            "|           .|        O|\n",
            "|     TORONTO|    B-LOC|\n",
            "|           ,|        O|\n",
            "|      Canada|    B-LOC|\n",
            "|           A|        O|\n",
            "|      second|        O|\n",
            "|        team|        O|\n",
            "|          of|        O|\n",
            "|  rocketeers|        O|\n",
            "|   competing|        O|\n",
            "+------------+---------+\n",
            "only showing top 30 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tvxmzp5c3ki",
        "colab_type": "text"
      },
      "source": [
        "## NerDL OntoNotes 100D\n",
        "\n",
        "Available Entities: 'CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvqAa7cQakL4",
        "colab_type": "code",
        "outputId": "d9c1e06f-85ad-4c90-f037-9d33ce49f53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "onto_ner = NerDLModel.pretrained(\"onto_100\", 'en') \\\n",
        "          .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
        "          .setOutputCol(\"ner\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " glove_embeddings,\n",
        " onto_ner\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "onto_100 download started this may take some time.\n",
            "Approximate size to download 13.5 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wyAqGAjakJD",
        "colab_type": "code",
        "outputId": "abd9c79f-2e95-4038-ffd5-c17ee0ab0408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(10))\n",
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'ner.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"ner_label\"))\n",
        "\n",
        "result_df.show(100, truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-------------+\n",
            "|       token|    ner_label|\n",
            "+------------+-------------+\n",
            "|      Unions|            O|\n",
            "|representing|            O|\n",
            "|     workers|            O|\n",
            "|          at|            O|\n",
            "|      Turner|        B-ORG|\n",
            "|      Newall|        I-ORG|\n",
            "|         say|            O|\n",
            "|        they|            O|\n",
            "|         are|            O|\n",
            "|           '|            O|\n",
            "|disappointed|            O|\n",
            "|           '|            O|\n",
            "|       after|            O|\n",
            "|       talks|            O|\n",
            "|        with|            O|\n",
            "|    stricken|            O|\n",
            "|      parent|            O|\n",
            "|        firm|            O|\n",
            "|     Federal|        B-ORG|\n",
            "|       Mogul|        I-ORG|\n",
            "|           .|            O|\n",
            "|     TORONTO|        B-GPE|\n",
            "|           ,|            O|\n",
            "|      Canada|        B-GPE|\n",
            "|           A|            O|\n",
            "|      second|    B-ORDINAL|\n",
            "|        team|            O|\n",
            "|          of|            O|\n",
            "|  rocketeers|            O|\n",
            "|   competing|            O|\n",
            "|         for|            O|\n",
            "|         the|            O|\n",
            "|      #36;10|   B-CARDINAL|\n",
            "|     million|   I-CARDINAL|\n",
            "|      Ansari|B-WORK_OF_ART|\n",
            "|           X|I-WORK_OF_ART|\n",
            "|       Prize|I-WORK_OF_ART|\n",
            "|           ,|            O|\n",
            "|           a|            O|\n",
            "|     contest|            O|\n",
            "|         for|            O|\n",
            "|   privately|            O|\n",
            "|      funded|            O|\n",
            "|  suborbital|            O|\n",
            "|       space|            O|\n",
            "|      flight|            O|\n",
            "|           ,|            O|\n",
            "|         has|            O|\n",
            "|  officially|            O|\n",
            "|   announced|            O|\n",
            "|         the|            O|\n",
            "|       first|    B-ORDINAL|\n",
            "|      launch|            O|\n",
            "|        date|            O|\n",
            "|         for|            O|\n",
            "|         its|            O|\n",
            "|      manned|            O|\n",
            "|      rocket|            O|\n",
            "|           .|            O|\n",
            "|           A|            O|\n",
            "|     company|            O|\n",
            "|     founded|            O|\n",
            "|          by|            O|\n",
            "|           a|            O|\n",
            "|   chemistry|            O|\n",
            "|  researcher|            O|\n",
            "|          at|            O|\n",
            "|         the|        B-ORG|\n",
            "|  University|        I-ORG|\n",
            "|          of|        I-ORG|\n",
            "|  Louisville|        I-ORG|\n",
            "|         won|            O|\n",
            "|           a|            O|\n",
            "|       grant|            O|\n",
            "|          to|            O|\n",
            "|     develop|            O|\n",
            "|           a|            O|\n",
            "|      method|            O|\n",
            "|          of|            O|\n",
            "|   producing|            O|\n",
            "|      better|            O|\n",
            "|    peptides|            O|\n",
            "|           ,|            O|\n",
            "|       which|            O|\n",
            "|         are|            O|\n",
            "|       short|            O|\n",
            "|      chains|            O|\n",
            "|          of|            O|\n",
            "|       amino|            O|\n",
            "|       acids|            O|\n",
            "|           ,|            O|\n",
            "|         the|            O|\n",
            "|    building|            O|\n",
            "|      blocks|            O|\n",
            "|          of|            O|\n",
            "|    proteins|            O|\n",
            "|           .|            O|\n",
            "|        It's|            O|\n",
            "|      barely|            O|\n",
            "|        dawn|            O|\n",
            "+------------+-------------+\n",
            "only showing top 100 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2cvfXeKd3Bj",
        "colab_type": "text"
      },
      "source": [
        "## Getting the NER Chunks with NER Converter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ2jDv-Kaj-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ner_converter = NerConverter() \\\n",
        "  .setInputCols([\"document\", \"token\", \"ner\"]) \\\n",
        "  .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " glove_embeddings,\n",
        " onto_ner,\n",
        " ner_converter\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "result = pipelineModel.transform(news_df.limit(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R3SDiREaj6y",
        "colab_type": "code",
        "outputId": "acabb78e-b871-438b-b5c1-19229b9a837b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "result = pipelineModel.transform(news_df.limit(10))\n",
        "\n",
        "\n",
        "result.select(F.explode(F.arrays_zip('ner_chunk.result', 'ner_chunk.metadata')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "        F.expr(\"cols['1']['entity']\").alias(\"ner_label\")).show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------+-----------+\n",
            "|chunk                              |ner_label  |\n",
            "+-----------------------------------+-----------+\n",
            "|Turner   Newall                    |ORG        |\n",
            "|Federal Mogul                      |ORG        |\n",
            "|TORONTO                            |GPE        |\n",
            "|Canada                             |GPE        |\n",
            "|second                             |ORDINAL    |\n",
            "|#36;10 million                     |CARDINAL   |\n",
            "|Ansari X Prize                     |WORK_OF_ART|\n",
            "|first                              |ORDINAL    |\n",
            "|the University of Louisville       |ORG        |\n",
            "|Mike Fitzpatrick                   |PERSON     |\n",
            "|the day                            |DATE       |\n",
            "|Southern California's              |LOC        |\n",
            "|Friday                             |DATE       |\n",
            "|first                              |ORDINAL    |\n",
            "|British                            |NORP       |\n",
            "|Department for Education and Skills|ORG        |\n",
            "|\"\"Music Manifesto\"\"                |WORK_OF_ART|\n",
            "|Netsky                             |PERSON     |\n",
            "|Sasser                             |PERSON     |\n",
            "|70 percent                         |PERCENT    |\n",
            "+-----------------------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXUqj3JweEwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To access to metadata which we will use to get entity, we have to use fullannotate rather tahn annotate\n",
        "\n",
        "light_model = LightPipeline(pipelineModel)\n",
        "\n",
        "light_result = light_model.fullAnnotate('Unions representing workers at Turner Newall say they are disappointed after talks with stricken parent firm Federal Mogul and Mike Fitzpatrick in Canada.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GydCoLvKeEt8",
        "colab_type": "code",
        "outputId": "d81f660a-3697-45d3-9310-bdc86c273ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "chunks = []\n",
        "entities  = []\n",
        "\n",
        "for n in light_result[0]['ner_chunk']:\n",
        "    chunks.append(n.result)\n",
        "    entities.append(n.metadata['entity'])\n",
        "\n",
        "df = pd.DataFrame({\"chunks\":chunks, 'entities':entities})\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunks</th>\n",
              "      <th>entities</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Turner Newall</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Federal Mogul</td>\n",
              "      <td>ORG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mike Fitzpatrick</td>\n",
              "      <td>PERSON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Canada</td>\n",
              "      <td>GPE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             chunks entities\n",
              "0     Turner Newall      ORG\n",
              "1     Federal Mogul      ORG\n",
              "2  Mike Fitzpatrick   PERSON\n",
              "3            Canada      GPE"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEwpOlBDtNwE",
        "colab_type": "text"
      },
      "source": [
        "## Highlight Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxBzqjIytBXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "\n",
        "def get_color():\n",
        "    r = lambda: random.randint(100,255)\n",
        "    return '#%02X%02X%02X' % (r(),r(),r())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOXeRK_-tFxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy import displacy\n",
        "\n",
        "def show_html_spacy(annotated_text, filter_labels=True):\n",
        "\n",
        "    label_list = []\n",
        "    sent_dict_list = []\n",
        "    \n",
        "    for n in annotated_text['ner_chunk']:\n",
        "\n",
        "        ent = {'start': n.begin, 'end':n.end+1, 'label':n.metadata['entity'].upper()}\n",
        "        \n",
        "        label_list.append(n.metadata['entity'].upper())\n",
        "\n",
        "        sent_dict_list.append(ent)\n",
        "   \n",
        "    document_text = [{'text':annotated_text['document'][0].result, 'ents':sent_dict_list,'title':None}]\n",
        "\n",
        "    label_list = list(set(label_list))\n",
        "                \n",
        "    label_color={}\n",
        "    \n",
        "    for l in label_list:\n",
        "        \n",
        "        label_color[l]=get_color()\n",
        "    \n",
        "    colors = {k:label_color[k] for k in label_list}\n",
        "        \n",
        "    \n",
        "    html_text = displacy.render(document_text, style='ent', jupyter=True, manual=True, options= {\"ents\": label_list, 'colors': colors})\n",
        "\n",
        "    return html_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOORS_AJtVpm",
        "colab_type": "code",
        "outputId": "4424d38c-774b-44dd-c953-d84f6c81e605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "ann_text = light_model.fullAnnotate('Unions representing workers at Turner Newall say they are disappointed after talks with stricken parent firm Federal Mogul and Mike Fitzpatrick in Canada.')\n",
        "\n",
        "show_html_spacy (ann_text[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Unions representing workers at \n",
              "<mark class=\"entity\" style=\"background: #AB69D8; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Turner Newall\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " say they are disappointed after talks with stricken parent firm \n",
              "<mark class=\"entity\" style=\"background: #AB69D8; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Federal Mogul\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #7E8075; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mike Fitzpatrick\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #A28BB1; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Canada\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-GENluptYab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_NER_html (annotated_text):\n",
        "    \n",
        "    light_data=annotated_text\n",
        "\n",
        "    html_output=''\n",
        "    \n",
        "    problem_flag = False\n",
        "    new_problem = []\n",
        "    problem_list = []\n",
        "    \n",
        "    label_list = list(set([i.split('-')[1] for i in light_data['ner'] if i!='O']))\n",
        "    \n",
        "        \n",
        "    label_color={}\n",
        "    \n",
        "    for l in label_list:\n",
        "        \n",
        "        label_color[l]=get_color()\n",
        "            \n",
        "    for index, this_token in enumerate(light_data['token']):\n",
        "\n",
        "        try:\n",
        "            ent = light_data['ner'][index].split('-')[1]\n",
        "        except:\n",
        "            ent = light_data['ner'][index]\n",
        "        \n",
        "       \n",
        "        if ent in label_list:\n",
        "            color = label_color[ent]\n",
        "            html_output+='<SPAN style=\"background-color: {}\">'.format(color) + this_token + \" </SPAN>\"\n",
        "        else:\n",
        "            html_output+=this_token + \" \"\n",
        "        \n",
        "\n",
        "    html_output += '</div>'\n",
        "    html_output += '<div>Color codes:'\n",
        "    \n",
        "\n",
        "    for l in label_list:\n",
        "        \n",
        "        html_output += '<SPAN style=\"background-color: {}\">{}</SPAN>, '.format(label_color[l],l)\n",
        "   \n",
        "    \n",
        "    return display(HTML(html_output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYknsNGGtgmH",
        "colab_type": "code",
        "outputId": "f098a709-72dc-4e1b-9032-2f839b38f2a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "ann_text = light_model.annotate('Unions representing workers at Turner Newall say they are disappointed after talks with stricken parent firm Federal Mogul and Mike Fitzpatrick in Canada.')\n",
        "\n",
        "get_NER_html (ann_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Unions representing workers at <SPAN style=\"background-color: #69B1CD\">Turner </SPAN><SPAN style=\"background-color: #69B1CD\">Newall </SPAN>say they are disappointed after talks with stricken parent firm <SPAN style=\"background-color: #69B1CD\">Federal </SPAN><SPAN style=\"background-color: #69B1CD\">Mogul </SPAN>and <SPAN style=\"background-color: #B8E586\">Mike </SPAN><SPAN style=\"background-color: #B8E586\">Fitzpatrick </SPAN>in <SPAN style=\"background-color: #9A9499\">Canada </SPAN>. </div><div>Color codes:<SPAN style=\"background-color: #9A9499\">GPE</SPAN>, <SPAN style=\"background-color: #69B1CD\">ORG</SPAN>, <SPAN style=\"background-color: #B8E586\">PERSON</SPAN>, "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytYox6enlWbm",
        "colab_type": "text"
      },
      "source": [
        "## BELOW CODES IS BELOGS TO Training02 and Training01 Respectively"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHMOiJYptdkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kir2DrS2sr58",
        "colab_type": "code",
        "outputId": "46dc4145-eadf-43a9-b68c-3dfb43171a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://gist.githubusercontent.com/vkocaman/e091605f012ffc1efc0fcda170919602/raw/fae33d25bd026375b2aaf1194b68b9da559c4ac4/annotators.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 13:06:50--  https://gist.githubusercontent.com/vkocaman/e091605f012ffc1efc0fcda170919602/raw/fae33d25bd026375b2aaf1194b68b9da559c4ac4/annotators.csv\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3136 (3.1K) [text/plain]\n",
            "Saving to: ‘annotators.csv’\n",
            "\n",
            "\rannotators.csv        0%[                    ]       0  --.-KB/s               \rannotators.csv      100%[===================>]   3.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-20 13:06:50 (49.3 MB/s) - ‘annotators.csv’ saved [3136/3136]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5QV1YKgs_iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('annotators.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9602dQBtG7I",
        "colab_type": "code",
        "outputId": "d65f7076-321a-4f2c-a169-3bd2e91650ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Annotator</th>\n",
              "      <th>Description</th>\n",
              "      <th>Version</th>\n",
              "      <th>Annotator Approach</th>\n",
              "      <th>Annotator Model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tokenizer*</td>\n",
              "      <td>Identifies tokens with tokenization open stand...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>-</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Normalizer*</td>\n",
              "      <td>Removes all dirty characters from text</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>-</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Stemmer*</td>\n",
              "      <td>Returns hard'-stems out of words with the obje...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lemmatizer*</td>\n",
              "      <td>Retrieves lemmas out of words with the objecti...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>-</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RegexMatcher*</td>\n",
              "      <td>Uses a reference file to match a set of regula...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>TextMatcher*</td>\n",
              "      <td>Annotator to match entire phrases (by token) p...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Chunker*</td>\n",
              "      <td>Matches a pattern of part'-of'-speech tags in ...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>DateMatcher*</td>\n",
              "      <td>Reads from different forms of date and time ex...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>SentenceDetector*</td>\n",
              "      <td>Finds sentence bounds in raw text. Applies rul...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>DeepSentenceDetector*</td>\n",
              "      <td>Finds sentence bounds in raw text. Applies a N...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>POSTagger</td>\n",
              "      <td>Sets a Part'-Of'-Speech tag to each word withi...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ViveknSentimentDetector</td>\n",
              "      <td>Scores a sentence for a sentiment</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>SentimentDetector*</td>\n",
              "      <td>Scores a sentence for a sentiment</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>WordEmbeddings*</td>\n",
              "      <td>Word Embeddings lookup annotator that maps tok...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>BertEmbeddings*</td>\n",
              "      <td>Bert Embeddings that maps tokens to vectors in...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>NerCrf</td>\n",
              "      <td>Named Entity recognition annotator allows for ...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>NerDL</td>\n",
              "      <td>This Named Entity recognition annotator allows...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>NorvigSweeting</td>\n",
              "      <td>This annotator retrieves tokens and makes corr...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>SymmetricDelete</td>\n",
              "      <td>This spell checker is inspired on Symmetric De...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>ContextSpellChecker</td>\n",
              "      <td>Utilizes tensorflow to do context based spell ...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>DependencyParser</td>\n",
              "      <td>Unlabeled parser that finds a grammatical rela...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>TypedDependencyParser</td>\n",
              "      <td>Labeled parser that finds a grammatical relati...</td>\n",
              "      <td>Opensource</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>AssertionLogReg</td>\n",
              "      <td>It will classify each clinically relevant name...</td>\n",
              "      <td>Licensed</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>AssertionDL</td>\n",
              "      <td>It will classify each clinically relevant name...</td>\n",
              "      <td>Licensed</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>EntityResolver</td>\n",
              "      <td>Assigns a ICD10 (International Classification ...</td>\n",
              "      <td>Licensed</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>DeIdentification</td>\n",
              "      <td>Identifies potential pieces of content with pe...</td>\n",
              "      <td>Licensed</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Annotator  ... Annotator Model\n",
              "0                Tokenizer*  ...               +\n",
              "1               Normalizer*  ...               +\n",
              "2                  Stemmer*  ...               -\n",
              "3               Lemmatizer*  ...               +\n",
              "4             RegexMatcher*  ...               +\n",
              "5              TextMatcher*  ...               +\n",
              "6                  Chunker*  ...               -\n",
              "7              DateMatcher*  ...               -\n",
              "8         SentenceDetector*  ...               -\n",
              "9     DeepSentenceDetector*  ...               -\n",
              "10                POSTagger  ...               +\n",
              "11  ViveknSentimentDetector  ...               +\n",
              "12       SentimentDetector*  ...               +\n",
              "13          WordEmbeddings*  ...               +\n",
              "14          BertEmbeddings*  ...               -\n",
              "15                   NerCrf  ...               +\n",
              "16                    NerDL  ...               +\n",
              "17           NorvigSweeting  ...               +\n",
              "18          SymmetricDelete  ...               +\n",
              "19      ContextSpellChecker  ...               +\n",
              "20         DependencyParser  ...               +\n",
              "21    TypedDependencyParser  ...               +\n",
              "22          AssertionLogReg  ...               +\n",
              "23              AssertionDL  ...               +\n",
              "24           EntityResolver  ...               +\n",
              "25         DeIdentification  ...               +\n",
              "\n",
              "[26 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQWTtXmztHAG",
        "colab_type": "code",
        "outputId": "6ca272c1-4496-4ecd-cc5c-a7448fba7310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "text = 'Peter Parker is a nice guy and lives in New York.'\n",
        "\n",
        "# spark is our session varaible we have created above as 'spark' = sparknlp.start() \n",
        "spark_df = spark.createDataFrame(([[text]])).toDF('text')\n",
        "spark_df.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------+\n",
            "|text                                             |\n",
            "+-------------------------------------------------+\n",
            "|Peter Parker is a nice guy and lives in New York.|\n",
            "+-------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fDrCP3MtHGW",
        "colab_type": "code",
        "outputId": "7a3bb89a-6ed4-429e-f2de-edab4bf24f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "spark_df = spark.createDataFrame(df)\n",
        "spark_df.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+------------------+---------------+\n",
            "|Annotator              |Description                                                                                                                                                                                                |Version   |Annotator Approach|Annotator Model|\n",
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+------------------+---------------+\n",
            "|Tokenizer*             |Identifies tokens with tokenization open standards                                                                                                                                                         |Opensource|-                 |+              |\n",
            "|Normalizer*            |Removes all dirty characters from text                                                                                                                                                                     |Opensource|-                 |+              |\n",
            "|Stemmer*               |Returns hard'-stems out of words with the objective of retrieving the meaningful part of the word                                                                                                          |Opensource|+                 |-              |\n",
            "|Lemmatizer*            |Retrieves lemmas out of words with the objective of returning a base dictionary word                                                                                                                       |Opensource|-                 |+              |\n",
            "|RegexMatcher*          |Uses a reference file to match a set of regular expressions and put them inside a provided key.                                                                                                            |Opensource|+                 |+              |\n",
            "|TextMatcher*           |Annotator to match entire phrases (by token) provided in a file against a Document                                                                                                                         |Opensource|+                 |+              |\n",
            "|Chunker*               |Matches a pattern of part'-of'-speech tags in order to return meaningful phrases from document                                                                                                             |Opensource|+                 |-              |\n",
            "|DateMatcher*           |Reads from different forms of date and time expressions and converts them to a provided date format                                                                                                        |Opensource|+                 |-              |\n",
            "|SentenceDetector*      |Finds sentence bounds in raw text. Applies rules from Pragmatic Segmenter                                                                                                                                  |Opensource|+                 |-              |\n",
            "|DeepSentenceDetector*  |Finds sentence bounds in raw text. Applies a Named Entity Recognition DL model                                                                                                                             |Opensource|+                 |-              |\n",
            "|POSTagger              |Sets a Part'-Of'-Speech tag to each word within a sentence.                                                                                                                                                |Opensource|+                 |+              |\n",
            "|ViveknSentimentDetector|Scores a sentence for a sentiment                                                                                                                                                                          |Opensource|+                 |+              |\n",
            "|SentimentDetector*     |Scores a sentence for a sentiment                                                                                                                                                                          |Opensource|+                 |+              |\n",
            "|WordEmbeddings*        |Word Embeddings lookup annotator that maps tokens to vectors                                                                                                                                               |Opensource|+                 |+              |\n",
            "|BertEmbeddings*        |Bert Embeddings that maps tokens to vectors in a bidirectional way                                                                                                                                         |Opensource|+                 |-              |\n",
            "|NerCrf                 |Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm                                                                                  |Opensource|+                 |+              |\n",
            "|NerDL                  |This Named Entity recognition annotator allows to train generic NER model based on Neural Networks by utilizing Char CNNs '- BiLSTM '- CRF architecture that achieves state'-of'-the'-art in most datasets.|Opensource|+                 |+              |\n",
            "|NorvigSweeting         |This annotator retrieves tokens and makes corrections automatically if not found in an English dictionary                                                                                                  |Opensource|+                 |+              |\n",
            "|SymmetricDelete        |This spell checker is inspired on Symmetric Delete algorithm                                                                                                                                               |Opensource|+                 |+              |\n",
            "|ContextSpellChecker    |Utilizes tensorflow to do context based spell checking                                                                                                                                                     |Opensource|+                 |+              |\n",
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTxZIKJftHMc",
        "colab_type": "code",
        "outputId": "fdce99de-c5d9-4933-a1a4-b7bbeb00f24f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/annotation/english/spark-nlp-basics/sample-sentences-en.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 13:21:01--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/annotation/english/spark-nlp-basics/sample-sentences-en.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 284 [text/plain]\n",
            "Saving to: ‘sample-sentences-en.txt’\n",
            "\n",
            "\rsample-sentences-en   0%[                    ]       0  --.-KB/s               \rsample-sentences-en 100%[===================>]     284  --.-KB/s    in 0s      \n",
            "\n",
            "2020-04-20 13:21:01 (17.7 MB/s) - ‘sample-sentences-en.txt’ saved [284/284]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AILw1n3tHQs",
        "colab_type": "code",
        "outputId": "c0a79e6c-09eb-49bb-c177-f4dbbe1d7492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# When we use read.text --> it reads line by line\n",
        "spark_df1 = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
        "spark_df1.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |\n",
            "|My life in Russia is very interesting.                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BfwFDC7tHTh",
        "colab_type": "code",
        "outputId": "9b76b7bf-4e8e-43d1-892f-be664040a5cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Eger sadece 1 column secmek istersek\n",
        "spark_df1.select('text').show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |\n",
            "|My life in Russia is very interesting.                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdW2SqbDtHPL",
        "colab_type": "code",
        "outputId": "a0bcc90d-6783-467d-b342-ac1841492655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# The wholeTextFiles reads file as whole single document not line by line\n",
        "textFiles = spark.sparkContext.wholeTextFiles(\"./*.txt\", 4)\n",
        "spark_df = textFiles.toDF(schema=['path', 'file_content'])\n",
        "spark_df.show(truncate=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------+------------------------------+\n",
            "|                          path|                  file_content|\n",
            "+------------------------------+------------------------------+\n",
            "|file:/content/sample-senten...|Peter is a very good person...|\n",
            "+------------------------------+------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndYs7ofXtHKj",
        "colab_type": "code",
        "outputId": "97eed9c5-65ae-4d7a-8fa3-f533ceb5edb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "spark_df.select('file_content').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(file_content=\"Peter is a very good person.\\nMy life in Russia is very interesting.\\nJohn and Peter are brothers. However they don't support each other that much.\\nLucas Nogal Dunbercker is no longer happy. He has a good car though.\\nEurope is very culture rich. There are huge churches! and big houses!\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbjHDtQ5yXV2",
        "colab_type": "text"
      },
      "source": [
        "## Document Assambler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOlntx0syWfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparknlp.base import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSUR42_DtHJJ",
        "colab_type": "code",
        "outputId": "e380ca6b-87df-4eca-dd43-290d4d4ec27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol('text')\\\n",
        ".setOutputCol('document')\\\n",
        ".setCleanupMode('shrink') # bosluklari, tab'leri vs temizler\n",
        "\n",
        "doc_df = documentAssembler.transform(spark_df1)\n",
        "doc_df.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                 |\n",
            "|My life in Russia is very interesting.                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[[document, 0, 76, John and Peter are brothers. However they don't support each other that much., [sentence -> 0], []]]|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[[document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., [sentence -> 0], []]]         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[[document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, [sentence -> 0], []]]        |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWJ8ARy-tHEk",
        "colab_type": "code",
        "outputId": "8455e6d9-8a10-43a3-e37c-9bf028db8713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "doc_df.printSchema()\n",
        "# as we can see we have the metadata here"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJZ0AHOPtG-Z",
        "colab_type": "code",
        "outputId": "6e521e1d-c6a1-4655-f17f-59705172bf98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "doc_df.select('document.result', 'document.begin', 'document.end').show(truncate=False)\n",
        "# each one returns a list type"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "|result                                                                         |begin|end |\n",
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "|[Peter is a very good person.]                                                 |[0]  |[27]|\n",
            "|[My life in Russia is very interesting.]                                       |[0]  |[37]|\n",
            "|[John and Peter are brothers. However they don't support each other that much.]|[0]  |[76]|\n",
            "|[Lucas Nogal Dunbercker is no longer happy. He has a good car though.]         |[0]  |[67]|\n",
            "|[Europe is very culture rich. There are huge churches! and big houses!]        |[0]  |[68]|\n",
            "+-------------------------------------------------------------------------------+-----+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apLGDxjG1tmn",
        "colab_type": "code",
        "outputId": "9f731763-c410-446d-a3a7-7a06678c0a70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# esas df'te document column uzerinde islem yapiyoruz o yuzden document.result olarak tanimlamaliyiz\n",
        "doc_df.select(\"document.result\").take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person.'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqchKFhI1tr2",
        "colab_type": "code",
        "outputId": "bc7d3aca-7a46-4592-a1a8-99c3421c4b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "### Expoloding teh dataframe (to view in much better nice view)\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "doc_df.withColumn(\n",
        "    \"tmp\",\n",
        "    F.explode('document'))\\\n",
        "    .select('tmp.*')\\\n",
        "    .show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "|annotatorType|begin|end|result                                                                       |metadata       |embeddings|\n",
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "|document     |0    |27 |Peter is a very good person.                                                 |[sentence -> 0]|[]        |\n",
            "|document     |0    |37 |My life in Russia is very interesting.                                       |[sentence -> 0]|[]        |\n",
            "|document     |0    |76 |John and Peter are brothers. However they don't support each other that much.|[sentence -> 0]|[]        |\n",
            "|document     |0    |67 |Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[sentence -> 0]|[]        |\n",
            "|document     |0    |68 |Europe is very culture rich. There are huge churches! and big houses!        |[sentence -> 0]|[]        |\n",
            "+-------------+-----+---+-----------------------------------------------------------------------------+---------------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjHSAXJW3_LS",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Detector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV_aw_Zb1twU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "# we feed the document column coming from Document Assembler\n",
        "\n",
        "sentenceDetector = SentenceDetector().\\\n",
        "setInputCols(['document']).\\\n",
        "setOutputCol('sentences')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tP5_3exK4Dup",
        "colab_type": "code",
        "outputId": "e24086dc-7727-496e-95c0-dbd8346557ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "sent_df = sentenceDetector.transform(doc_df)\n",
        "sent_df.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                         |document                                                                                                               |sentences                                                                                                                                                                                          |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                 |[[document, 0, 27, Peter is a very good person., [sentence -> 0], []]]                                                                                                                             |\n",
            "|My life in Russia is very interesting.                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                       |[[document, 0, 37, My life in Russia is very interesting., [sentence -> 0], []]]                                                                                                                   |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[[document, 0, 76, John and Peter are brothers. However they don't support each other that much., [sentence -> 0], []]]|[[document, 0, 27, John and Peter are brothers., [sentence -> 0], []], [document, 29, 76, However they don't support each other that much., [sentence -> 1], []]]                                  |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[[document, 0, 67, Lucas Nogal Dunbercker is no longer happy. He has a good car though., [sentence -> 0], []]]         |[[document, 0, 41, Lucas Nogal Dunbercker is no longer happy., [sentence -> 0], []], [document, 43, 67, He has a good car though., [sentence -> 1], []]]                                           |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[[document, 0, 68, Europe is very culture rich. There are huge churches! and big houses!, [sentence -> 0], []]]        |[[document, 0, 27, Europe is very culture rich., [sentence -> 0], []], [document, 29, 52, There are huge churches!, [sentence -> 1], []], [document, 54, 68, and big houses!, [sentence -> 2], []]]|\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "244pdka54Dx_",
        "colab_type": "code",
        "outputId": "b3eda7ba-96d7-4d0d-a2e9-7e778a5453d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# take works as head in pandas\n",
        "sent_df.select('sentences').take(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='Peter is a very good person.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(sentences=[Row(annotatorType='document', begin=0, end=37, result='My life in Russia is very interesting.', metadata={'sentence': '0'}, embeddings=[])]),\n",
              " Row(sentences=[Row(annotatorType='document', begin=0, end=27, result='John and Peter are brothers.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='document', begin=29, end=76, result=\"However they don't support each other that much.\", metadata={'sentence': '1'}, embeddings=[])])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_EEb8IG4D1d",
        "colab_type": "code",
        "outputId": "b83440bd-4225-4fb0-9569-cc9a9c7a0bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample_text =\"\"\"The patient was prescribed 1 capsule of Advil for 5 days . He was seen by the endocrinology \n",
        "service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro \n",
        "with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should \n",
        "be discontinued indefinitely fro 3 months .\"\"\"\n",
        "sample_text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The patient was prescribed 1 capsule of Advil for 5 days . He was seen by the endocrinology \\nservice and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro \\nwith meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should \\nbe discontinued indefinitely fro 3 months .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQGNO3Ui1tzd",
        "colab_type": "code",
        "outputId": "1875e662-70c8-46e5-b9e6-72bbff0c1fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "spark_df2 = spark.createDataFrame([[sample_text]]).toDF('text')\n",
        "spark_df2.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|text                                                                                                                                                                                                                                                                                                                                                   |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|The patient was prescribed 1 capsule of Advil for 5 days . He was seen by the endocrinology \n",
            "service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro \n",
            "with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should \n",
            "be discontinued indefinitely fro 3 months .|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjRTCQl21tu2",
        "colab_type": "code",
        "outputId": "ac6b66c5-df09-4541-bdbe-f434b76cc57a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "doc_df1 = documentAssembler.transform(spark_df2)\n",
        "sent_df = sentenceDetector.transform(doc_df1)\n",
        "sent_df.show(truncate=40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|                                    text|                                document|                               sentences|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|The patient was prescribed 1 capsule ...|[[document, 0, 339, The patient was p...|[[document, 0, 57, The patient was pr...|\n",
            "+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prx2lEI91tqD",
        "colab_type": "code",
        "outputId": "ac915241-e643-44cf-c9d1-ebb0aebcbc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sent_df.select('sentences.result').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['The patient was prescribed 1 capsule of Advil for 5 days .', 'He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day .', 'It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mCylQL19BYq",
        "colab_type": "code",
        "outputId": "6cb36b65-c1b5-4a5d-dde8-9acb25703acb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sentenceDetector.setExplodeSentences(True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceDetector_18a10300d4df"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ics4Pyp9BiU",
        "colab_type": "code",
        "outputId": "92bcbff6-4ab8-4fd2-bc18-5d17d47725a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "sent_df = sentenceDetector.transform(doc_df1)\n",
        "sent_df.select('sentences.result').show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                      |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[The patient was prescribed 1 capsule of Advil for 5 days .]                                                                                                                                |\n",
            "|[He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day .]|\n",
            "|[It was determined that all SGLT2 inhibitors should be discontinued indefinitely fro 3 months .]                                                                                            |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlB5szs_95s6",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1RcERSs9BtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #we can also use sentence in InputCols\n",
        "tokenizer = Tokenizer()\\\n",
        "            .setInputCols(['document'])\\\n",
        "            .setOutputCol('token')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_AsrFtS9B5s",
        "colab_type": "code",
        "outputId": "d3572801-8382-4a88-c8b3-2919836f20ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "text2 = \"Peter Parker (Spiderman) is a nice guy and live ins New York but has no e-mail! and he's gone so far\"\n",
        "text2.split()\n",
        "# There are unwanted chars below which we do not want a part fo token such as () and !"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Peter',\n",
              " 'Parker',\n",
              " '(Spiderman)',\n",
              " 'is',\n",
              " 'a',\n",
              " 'nice',\n",
              " 'guy',\n",
              " 'and',\n",
              " 'live',\n",
              " 'ins',\n",
              " 'New',\n",
              " 'York',\n",
              " 'but',\n",
              " 'has',\n",
              " 'no',\n",
              " 'e-mail!',\n",
              " 'and',\n",
              " \"he's\",\n",
              " 'gone',\n",
              " 'so',\n",
              " 'far']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTtPHeQ39CAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark_df = spark.createDataFrame([[text2]]).toDF('text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFx1qvSl9CI-",
        "colab_type": "code",
        "outputId": "b60839e8-0b03-4bbb-b03a-0cae49520d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "doc_df = documentAssembler.transform(spark_df)\n",
        "token_df = tokenizer.fit(doc_df).transform(doc_df)\n",
        "token_df.show(truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                text|                                                                                            document|                                                                                               token|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "|Peter Parker (Spiderman) is a nice guy and live ins New York but has no e-mail! and he's gone so far|[[document, 0, 99, Peter Parker (Spiderman) is a nice guy and live ins New York but has no e-mail...|[[token, 0, 4, Peter, [sentence -> 0], []], [token, 6, 11, Parker, [sentence -> 0], []], [token, ...|\n",
            "+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBPKONDe9CQT",
        "colab_type": "code",
        "outputId": "d78e5948-6bb7-44eb-977b-6a008e78ecdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "token_df.select('token.result').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(', 'Spiderman', ')', 'is', 'a', 'nice', 'guy', 'and', 'live', 'ins', 'New', 'York', 'but', 'has', 'no', 'e-mail', '!', 'and', \"he's\", 'gone', 'so', 'far'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsDoXpTw9CW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Eger tokenizer i custome etmek istersek\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "            .setInputCols(['document'])\\\n",
        "            .setOutputCol('token')\\\n",
        "            .setSplitChars(['-'])\\\n",
        "            .setContextChars(['(', ')', '?', '!'])\\\n",
        "            .addException('New York')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5qvA-Zj9CU8",
        "colab_type": "code",
        "outputId": "5166d5cd-9300-43a2-93f4-577b2baffbf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "token_df = tokenizer.fit(doc_df).transform(token_df)\n",
        "token_df.select('token.result').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(', 'Spiderman', ')', 'is', 'a', 'nice', 'guy', 'and', 'live', 'ins', 'New York', 'but', 'has', 'no', 'e', 'mail', '!', 'and', \"he's\", 'gone', 'so', 'far'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LD3CdWVuEUR4",
        "colab": {}
      },
      "source": [
        "# # Pay Attention to () for Spiderman\n",
        "tokenizer33 = Tokenizer()\\\n",
        "            .setInputCols(['document'])\\\n",
        "            .setOutputCol('token')\\\n",
        "            .setSplitChars(['-'])\\\n",
        "            .setContextChars(['?', '!'])\\\n",
        "            .addException('New York')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xASRwm9e9CNo",
        "colab_type": "code",
        "outputId": "e1202535-ccee-4bae-a49f-122e9dcb8599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "token_df = tokenizer33.fit(doc_df).transform(token_df)\n",
        "token_df.select('token.result').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'live', 'ins', 'New York', 'but', 'has', 'no', 'e', 'mail', '!', 'and', \"he's\", 'gone', 'so', 'far'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw98GR7Q9CHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Let's Combine Everything in a Pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-09E8aP9CFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "                    .setInputCol('text')\\\n",
        "                    .setOutputCol('document')\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "                    .setInputCols(['document'])\\\n",
        "                    .setOutputCol('sentences')\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "            .setInputCols(['sentences'])\\\n",
        "            .setOutputCol('token')\n",
        "        \n",
        "nlPipeline = Pipeline(stages=[\n",
        "                              documentAssembler,\n",
        "                              sentenceDetector,\n",
        "                              tokenizer\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "\n",
        "pipelineModel = nlPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vENbCgDw9CDQ",
        "colab_type": "code",
        "outputId": "80ea4eae-40f1-455e-f43e-b5fb59e4b214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "spark_df = spark.read.text('./sample-sentences-en.txt').toDF('text')\n",
        "spark_df.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |\n",
            "|My life in Russia is very interesting.                                       |\n",
            "|John and Peter are brothers. However they don't support each other that much.|\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ6OD_ES9B-L",
        "colab_type": "code",
        "outputId": "5c8bbe06-b0b2-44c5-91bd-ee2cc6948440",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result = pipelineModel.transform(spark_df)\n",
        "result.show(truncate=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|           sentences|               token|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[document, 0, 27...|[[token, 0, 4, Pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 1, My...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[document, 0, 27...|[[token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[document, 0, 41...|[[token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[document, 0, 27...|[[token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sfhTlmz9B8c",
        "colab_type": "code",
        "outputId": "cf9c5285-5775-4d56-ba18-e844479ecc26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "result.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- sentences: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- token: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyK-qCgR9B3r",
        "colab_type": "code",
        "outputId": "1d89b82b-bfc1-4e2b-e67c-cc51d79d9b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "result.select('sentences.result').take(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter is a very good person.']),\n",
              " Row(result=['My life in Russia is very interesting.']),\n",
              " Row(result=['John and Peter are brothers.', \"However they don't support each other that much.\"])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llHKp9nn9B19",
        "colab_type": "code",
        "outputId": "6dd7e7e0-5535-4d77-d482-29ff2a8578fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "result.select('token').take(3)[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(token=[Row(annotatorType='token', begin=0, end=3, result='John', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=5, end=7, result='and', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=9, end=13, result='Peter', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=15, end=17, result='are', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=19, end=26, result='brothers', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=27, result='.', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=29, end=35, result='However', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=37, end=40, result='they', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=42, end=46, result=\"don't\", metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=48, end=54, result='support', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=56, end=59, result='each', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=61, end=65, result='other', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=67, end=70, result='that', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=72, end=75, result='much', metadata={'sentence': '1'}, embeddings=[]), Row(annotatorType='token', begin=76, end=76, result='.', metadata={'sentence': '1'}, embeddings=[])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giqubqil9Bzr",
        "colab_type": "code",
        "outputId": "21adfc70-f229-4073-ef73-69336eb5f14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "result.select('token.result').take(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'is', 'a', 'very', 'good', 'person', '.']),\n",
              " Row(result=['My', 'life', 'in', 'Russia', 'is', 'very', 'interesting', '.']),\n",
              " Row(result=['John', 'and', 'Peter', 'are', 'brothers', '.', 'However', 'they', \"don't\", 'support', 'each', 'other', 'that', 'much', '.'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cHbVBkTKWwn",
        "colab_type": "code",
        "outputId": "679913cc-49a7-4d1a-9aa4-f5ca51ed1f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "''.join([i for i in \"My name is @# Serdar.\" if i.isalnum() or i==' '])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My name is  Serdar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3iV9xQRIpNM",
        "colab_type": "text"
      },
      "source": [
        "## Normalizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgOLuvao9Bx-",
        "colab_type": "code",
        "outputId": "d621a51a-eba7-4946-bbe6-ce98664c0d36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDIjJNKl9BwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "normalizer = Normalizer()\\\n",
        "            .setInputCols(['token'])\\\n",
        "            .setOutputCol('normalized')\\\n",
        "            .setLowercase(True)\\\n",
        "            .setCleanupPatterns([\"[^\\w\\d\\s]\"]) #remove punctuations and keep alphanumerics only\n",
        "            # if we do not set ClenaupPatterns it till only keep alphabet letters [^A-Za-z]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-la_yWY9Brd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're removing sentence annotator\n",
        "\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlPipeline = Pipeline(stages=[\n",
        "                              documentAssembler,\n",
        "                              tokenizer,\n",
        "                              normalizer\n",
        "])\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "\n",
        "pipelineModel = nlPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eqzmT-u9Bpr",
        "colab_type": "code",
        "outputId": "9a4484a1-0968-4ce4-e03e-5d80119aa954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result = pipelineModel.transform(spark_df)\n",
        "result.show(truncate=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|          normalized|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdQ-B2br9BnS",
        "colab_type": "code",
        "outputId": "6fc9bbb9-690e-4647-e1db-0a3e8a1b5eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "result.select('normalized.result').take(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['peter', 'is', 'a', 'very', 'good', 'person']),\n",
              " Row(result=['my', 'life', 'in', 'russia', 'is', 'very', 'interesting']),\n",
              " Row(result=['john', 'and', 'peter', 'are', 'brothers', 'however', 'they', 'dont', 'support', 'each', 'other', 'that', 'much'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyBpsQd1N9Ap",
        "colab_type": "text"
      },
      "source": [
        "## Stopwrods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqo1_Vmf9BlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "                    .setInputCols('token')\\\n",
        "                    .setOutputCol('cleanTokens')\\\n",
        "                    .setCaseSensitive(False)\n",
        "                    #.setStopWords(['no', 'without']) custom stopwords we can use but it will overwrite existing stopwords and will ONLY use this one\n",
        "                    # if we want to  use normalized ones and just want to use from normalised use \"normalized\" in InputCols\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlPipeline = Pipeline(stages=[\n",
        "                              documentAssembler,\n",
        "                              tokenizer,\n",
        "                              stopwords_cleaner\n",
        "])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
        "\n",
        "pipelineModel = nlPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjcwH_QK9Bf2",
        "colab_type": "code",
        "outputId": "d97a025d-78b2-42da-9e04-f52bee5edbc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result = pipelineModel.transform(spark_df)\n",
        "result.show(truncate=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|         cleanTokens|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 3, 6, li...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyAwIe1-PD3Q",
        "colab_type": "code",
        "outputId": "0a475d0e-43d2-4121-959c-c2259947b96c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "result.select('cleanTokens.result').take(3)\n",
        "# We did not include Normalizer so we might see punctuations and uppercase. If we dont want, se should incluce in pipeline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['Peter', 'good', 'person', '.']),\n",
              " Row(result=['life', 'Russia', 'interesting', '.']),\n",
              " Row(result=['John', 'Peter', 'brothers', '.', 'However', 'support', 'much', '.'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8G6uuiMQq1F",
        "colab_type": "text"
      },
      "source": [
        "## Token Assembler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXO3OS4EPD83",
        "colab_type": "code",
        "outputId": "ea142754-dc90-4828-c6b9-bc1ba813cd91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector().\\\n",
        "    setInputCols(['document']).\\\n",
        "    setOutputCol('sentences')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentences\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\\\n",
        "    .setLowercase(False)\\\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\\\n",
        "\n",
        "tokenassembler = TokenAssembler()\\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"clean_text\")\n",
        "\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "     documentAssembler,\n",
        "    sentenceDetector,\n",
        "     tokenizer,\n",
        "     normalizer,\n",
        "     stopwords_cleaner,\n",
        "     tokenassembler\n",
        " ])\n",
        "\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|           sentences|               token|          normalized|         cleanTokens|          clean_text|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|[[token, 0, 4, Pe...|[[document, 0, 16...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, My...|[[token, 3, 6, li...|[[document, 3, 25...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[document, 0, 27...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|[[token, 0, 3, Jo...|[[document, 0, 18...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[document, 0, 41...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|[[token, 0, 4, Lu...|[[document, 0, 34...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[document, 0, 27...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|[[token, 0, 5, Eu...|[[document, 0, 18...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlhXLMVwPEDg",
        "colab_type": "code",
        "outputId": "1c0e132a-0af4-445c-c4b3-a7856ffedc15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result.select(['text', 'clean_text.result']).show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+------------------------------------------------------+\n",
            "|text                                                                         |result                                                |\n",
            "+-----------------------------------------------------------------------------+------------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[Peter good person]                                   |\n",
            "|My life in Russia is very interesting.                                       |[life Russia interesting]                             |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[John Peter brothers, However dont support much]      |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[Lucas Nogal Dunbercker longer happy, good car though]|\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[Europe culture rich, huge churches, big houses]      |\n",
            "+-----------------------------------------------------------------------------+------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5fzpaHIPEGm",
        "colab_type": "code",
        "outputId": "b505419a-c3ae-40d2-e986-5f6f7660a34a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "result.withColumn(\n",
        "    \"tmp\",\n",
        "    F.explode('clean_text'))\\\n",
        "    .select(\"tmp.*\").select('begin', 'end', 'result', 'metadata.sentence').show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+-----------------------------------+--------+\n",
            "|begin|end|result                             |sentence|\n",
            "+-----+---+-----------------------------------+--------+\n",
            "|0    |16 |Peter good person                  |0       |\n",
            "|3    |25 |life Russia interesting            |0       |\n",
            "|0    |18 |John Peter brothers                |0       |\n",
            "|29   |53 |However dont support much          |1       |\n",
            "|0    |34 |Lucas Nogal Dunbercker longer happy|0       |\n",
            "|52   |66 |good car though                    |1       |\n",
            "|0    |18 |Europe culture rich                |0       |\n",
            "|39   |51 |huge churches                      |1       |\n",
            "|58   |67 |big houses                         |2       |\n",
            "+-----+---+-----------------------------------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2nUOG-JPEL_",
        "colab_type": "code",
        "outputId": "841eeec8-d29a-41fb-89c1-82d66d2c4cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# if we hadn't used Sentence Detector, this would be what we got. (tokenizer gets document instead of sentences column)\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "     documentAssembler,\n",
        "     tokenizer,\n",
        "     normalizer,\n",
        "     stopwords_cleaner,\n",
        "     tokenassembler\n",
        " ])\n",
        "\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.select('text', 'clean_text.result').show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "|text                                                                         |result                                               |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "|Peter is a very good person.                                                 |[Peter good person]                                  |\n",
            "|My life in Russia is very interesting.                                       |[life Russia interesting]                            |\n",
            "|John and Peter are brothers. However they don't support each other that much.|[John Peter brothers However dont support much]      |\n",
            "|Lucas Nogal Dunbercker is no longer happy. He has a good car though.         |[Lucas Nogal Dunbercker longer happy good car though]|\n",
            "|Europe is very culture rich. There are huge churches! and big houses!        |[Europe culture rich huge churches big houses]       |\n",
            "+-----------------------------------------------------------------------------+-----------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WWw0lB_PEP_",
        "colab_type": "code",
        "outputId": "d732f324-864e-4cb2-cb67-efb704b7e654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result.withColumn(\n",
        "    \"tmp\",\n",
        "    F.explode('clean_text'))\\\n",
        "    .select(\"tmp.*\").select('begin', 'end', 'result', 'metadata.sentence').show(truncate=False)\n",
        "\n",
        "# We get all sentences in a single lien and all sentences assigend as 0, there si no other sentence"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+---------------------------------------------------+--------+\n",
            "|begin|end|result                                             |sentence|\n",
            "+-----+---+---------------------------------------------------+--------+\n",
            "|0    |16 |Peter good person                                  |0       |\n",
            "|3    |25 |life Russia interesting                            |0       |\n",
            "|0    |44 |John Peter brothers However dont support much      |0       |\n",
            "|0    |50 |Lucas Nogal Dunbercker longer happy good car though|0       |\n",
            "|0    |43 |Europe culture rich huge churches big houses       |0       |\n",
            "+-----+---+---------------------------------------------------+--------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UZoUgf5TVZ0",
        "colab_type": "text"
      },
      "source": [
        "## Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vlnll0z9PEgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcRo94yYPEj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stemmer\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz-1Ta_bPEr7",
        "colab_type": "code",
        "outputId": "7a322633-162e-466f-8598-7f9ba58a8eea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                stem|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu7pCP4VPExh",
        "colab_type": "code",
        "outputId": "9d2b60f6-1054-4196-f5f9-e996f3c20081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result.select('stem.result').show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------+\n",
            "|result                                                                                     |\n",
            "+-------------------------------------------------------------------------------------------+\n",
            "|[peter, i, a, veri, good, person, .]                                                       |\n",
            "|[my, life, in, russia, i, veri, interest, .]                                               |\n",
            "|[john, and, peter, ar, brother, ., howev, thei, don't, support, each, other, that, much, .]|\n",
            "|[luca, nogal, dunberck, i, no, longer, happi, ., he, ha, a, good, car, though, .]          |\n",
            "|[europ, i, veri, cultur, rich, ., there, ar, huge, church, !, and, big, hous, !]           |\n",
            "+-------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9geXfa9OPEvH",
        "colab_type": "code",
        "outputId": "45453aaa-a5e2-4c76-ea15-95211f926b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'stem.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"stem\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>stem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Peter</td>\n",
              "      <td>peter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>very</td>\n",
              "      <td>veri</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>good</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>person</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>My</td>\n",
              "      <td>my</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>life</td>\n",
              "      <td>life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    token    stem\n",
              "0   Peter   peter\n",
              "1      is       i\n",
              "2       a       a\n",
              "3    very    veri\n",
              "4    good    good\n",
              "5  person  person\n",
              "6       .       .\n",
              "7      My      my\n",
              "8    life    life\n",
              "9      in      in"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89HZ6QVwUhty",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox6qyXnYPEp_",
        "colab_type": "code",
        "outputId": "01791643-8fd9-48f7-df21-92c01d47d6d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 16:01:21--  https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1348552 (1.3M) [text/plain]\n",
            "Saving to: ‘AntBNC_lemmas_ver_001.txt’\n",
            "\n",
            "\r          AntBNC_le   0%[                    ]       0  --.-KB/s               \rAntBNC_lemmas_ver_0 100%[===================>]   1.29M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2020-04-20 16:01:21 (128 MB/s) - ‘AntBNC_lemmas_ver_001.txt’ saved [1348552/1348552]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAZHndJYPEnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = Lemmatizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"lemma\") \\\n",
        "    .setDictionary(\"./AntBNC_lemmas_ver_001.txt\", value_delimiter =\"\\t\", key_delimiter = \"->\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqrLnV-kPEeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " stemmer,\n",
        " lemmatizer\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vofAirf6PEcq",
        "colab_type": "code",
        "outputId": "584aa4f7-3c40-405b-da32-4c02e577073d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|                stem|               lemma|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Peter is a very g...|[[document, 0, 27...|[[token, 0, 4, Pe...|[[token, 0, 4, pe...|[[token, 0, 4, Pe...|\n",
            "|My life in Russia...|[[document, 0, 37...|[[token, 0, 1, My...|[[token, 0, 1, my...|[[token, 0, 1, My...|\n",
            "|John and Peter ar...|[[document, 0, 76...|[[token, 0, 3, Jo...|[[token, 0, 3, jo...|[[token, 0, 3, Jo...|\n",
            "|Lucas Nogal Dunbe...|[[document, 0, 67...|[[token, 0, 4, Lu...|[[token, 0, 4, lu...|[[token, 0, 4, Lu...|\n",
            "|Europe is very cu...|[[document, 0, 68...|[[token, 0, 5, Eu...|[[token, 0, 5, eu...|[[token, 0, 5, Eu...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIBkQZAPEar",
        "colab_type": "code",
        "outputId": "cb03a7cc-984e-4d60-9cb2-930c7ca17b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "result.select('lemma.result').show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------+\n",
            "|result                                                                                       |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "|[Peter, be, a, very, good, person, .]                                                        |\n",
            "|[My, life, in, Russia, be, very, interest, .]                                                |\n",
            "|[John, and, Peter, be, brother, ., However, they, don't, support, each, other, that, much, .]|\n",
            "|[Lucas, Nogal, Dunbercker, be, no, long, happy, ., He, have, a, good, car, though, .]        |\n",
            "|[Europe, be, very, culture, rich, ., There, be, huge, church, !, and, big, house, !]         |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHmtLaMfPEY3",
        "colab_type": "code",
        "outputId": "7e1e15e3-4377-4dc8-da98-d3c5ceb1cfa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "\n",
        "result_df = result.select(F.explode(F.arrays_zip('token.result', 'stem.result',  'lemma.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"stem\"),\n",
        "        F.expr(\"cols['2']\").alias(\"lemma\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>stem</th>\n",
              "      <th>lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Peter</td>\n",
              "      <td>peter</td>\n",
              "      <td>Peter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>i</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>very</td>\n",
              "      <td>veri</td>\n",
              "      <td>very</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>good</td>\n",
              "      <td>good</td>\n",
              "      <td>good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>person</td>\n",
              "      <td>person</td>\n",
              "      <td>person</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>My</td>\n",
              "      <td>my</td>\n",
              "      <td>My</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>life</td>\n",
              "      <td>life</td>\n",
              "      <td>life</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    token    stem   lemma\n",
              "0   Peter   peter   Peter\n",
              "1      is       i      be\n",
              "2       a       a       a\n",
              "3    very    veri    very\n",
              "4    good    good    good\n",
              "5  person  person  person\n",
              "6       .       .       .\n",
              "7      My      my      My\n",
              "8    life    life    life\n",
              "9      in      in      in"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zMjYfl0V_Yk",
        "colab_type": "text"
      },
      "source": [
        "## N-Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRTqUjbTPEW9",
        "colab_type": "code",
        "outputId": "baa90fe7-e3ae-4e79-db74-0da9e1ce4a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "ngrams_cum = NGramGenerator() \\\n",
        "            .setInputCols([\"token\"]) \\\n",
        "            .setOutputCol(\"ngrams\") \\\n",
        "            .setN(3) \\\n",
        "            .setEnableCumulative(True)\\\n",
        "            .setDelimiter(\"_\") # Default is space\n",
        "    \n",
        "# .setN(3) means, take bigrams and trigrams.\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " ngrams_cum\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "result = pipelineModel.transform(spark_df)\n",
        "\n",
        "result.select('ngrams.result').show(truncate=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                                                                                                                  result|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|                                    [Peter, is, a, very, good, person, ., Peter_is, is_a, a_very, very_good, good_person, person_., Peter_is_a, is_a_very, a_very_good, very_good_person, good_person_.]|\n",
            "|[My, life, in, Russia, is, very, interesting, ., My_life, life_in, in_Russia, Russia_is, is_very, very_interesting, interesting_., My_life_in, life_in_Russia, in_Russia_is, Russia_is_very, is_very_...|\n",
            "|[John, and, Peter, are, brothers, ., However, they, don't, support, each, other, that, much, ., John_and, and_Peter, Peter_are, are_brothers, brothers_., ._However, However_they, they_don't, don't_...|\n",
            "|[Lucas, Nogal, Dunbercker, is, no, longer, happy, ., He, has, a, good, car, though, ., Lucas_Nogal, Nogal_Dunbercker, Dunbercker_is, is_no, no_longer, longer_happy, happy_., ._He, He_has, has_a, a_...|\n",
            "|[Europe, is, very, culture, rich, ., There, are, huge, churches, !, and, big, houses, !, Europe_is, is_very, very_culture, culture_rich, rich_., ._There, There_are, are_huge, huge_churches, churche...|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDrAZy-OWiHA",
        "colab_type": "text"
      },
      "source": [
        "## Text Matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzTrzZbRPET3",
        "colab_type": "code",
        "outputId": "1736788f-2ea0-4ac0-a7c3-55a1e7182877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "! wget \thttps://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 16:11:48--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/pubmed/pubmed-sample.csv\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.110.13\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.110.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10484510 (10.0M) [text/csv]\n",
            "Saving to: ‘pubmed-sample.csv’\n",
            "\n",
            "pubmed-sample.csv   100%[===================>]  10.00M  16.8MB/s    in 0.6s    \n",
            "\n",
            "2020-04-20 16:11:49 (16.8 MB/s) - ‘pubmed-sample.csv’ saved [10484510/10484510]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJAKK7DaXfvd",
        "colab_type": "code",
        "outputId": "4e54559b-e5df-4804-f50d-ffb50ec26af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!head pubmed-sample.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AB,TI\n",
            "\"The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family. Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population. The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons separated byapproximately 2.2 and approximately 2.6 kb introns, respectively. We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair (bp) insertion/deletion. Ourexpression studies revealed the presence of the transcript in various humantissues including pancreas, and two major insulin-responsive tissues: fat andskeletal muscle. The characterization of the KCNJ9 gene should facilitate furtherstudies on the function of the KCNJ9 protein and allow evaluation of thepotential role of the locus in Type II diabetes.\",\"Genomic structure and expression of human KCNJ9 (Kir3.3/GIRK3).\"\n",
            "\"BACKGROUND: At present, it is one of the most important issues for the treatment of breast cancer to develop the standard therapy for patients previously treated with anthracyclines and taxanes. With the objective of determining the usefulnessof vinorelbine monotherapy in patients with advanced or recurrent breast cancerafter standard therapy, we evaluated the efficacy and safety of vinorelbine inpatients previously treated with anthracyclines and taxanes. METHODS: Vinorelbinewas administered at a dose level of 25 mg/m(2) intravenously on days 1 and 8 of a3 week cycle. Patients were given three or more cycles in the absence of tumorprogression. A maximum of nine cycles were administered. RESULTS: The responserate in 50 evaluable patients was 20.0% (10 out of 50; 95% confidence interval,10.0-33.7%). Responders plus those who had minor response (MR) or no change (NC) accounted for 58.0% [10 partial responses (PRs) + one MR + 18 NCs out of 50]. TheKaplan-Meier estimate (50% point) of time to progression (TTP) was 115.0 days.The response rate in the visceral organs was 17.3% (nine PRs out of 52). Themajor toxicity was myelosuppression, which was reversible and did not requirediscontinuation of treatment. CONCLUSION: The results of this study show thatvinorelbine monotherapy is useful in patients with advanced or recurrent breastcancer previously exposed to both anthracyclines and taxanes.\",\"Late phase II clinical study of vinorelbine monotherapy in advanced or recurrent breast cancer previously treated with anthracyclines and taxanes.\"\n",
            "\"OBJECTIVE: To investigate the relationship between preoperative atrialfibrillation and early and late clinical outcomes following cardiac surgery.METHODS: A retrospective cohort including all consecutive coronary artery bypass graft and/or valve surgery patients between 1995 and 2005 was identified (n =9796). No patient had a concomitant surgical AF ablation. The association betweenpreoperative atrial fibrillation and in-hospital outcomes was examined. We alsodetermined late death and cardiovascular-related re-hospitalization by linking toadministrative health databases. Median follow-up was 2.9 years (maximum 11years). RESULTS: The prevalence of preoperative atrial fibrillation was 11.3% (n = 1105), ranging from 7.2% in isolated CABG to 30% in valve surgery. In-hospital mortality, stroke, and renal failure were more common in atrial fibrillationpatients (all p < 0.0001), although the association between atrial fibrillationand mortality was not statistically significant in multivariate logisticregression. Longitudinal analyses showed that preoperative atrial fibrillationwas associated with decreased event-free survival (adjusted hazard ratio 1.55,95% confidence interval 1.42-1.70, p < 0.0001). CONCLUSIONS: Preoperative atrial fibrillation is associated with increased late mortality and recurrentcardiovascular events post-cardiac surgery. Effective management strategies foratrial fibrillation need to be explored and may provide an opportunity to improvethe long-term outcomes of cardiac surgical patients.\",\"Preoperative atrial fibrillation decreases event-free survival following cardiac surgery.\"\n",
            "\"Combined EEG/fMRI recording has been used to localize the generators of EEGevents and to identify subject state in cognitive studies and is of increasinginterest. However, the large EEG artifacts induced during fMRI have precludedsimultaneous EEG and fMRI recording, restricting study design. Removing thisartifact is difficult, as it normally exceeds EEG significantly and containscomponents in the EEG frequency range. We have developed a recording system andan artifact reduction method that reduce this artifact effectively. The recordingsystem has large dynamic range to capture both low-amplitude EEG and largeimaging artifact without distortion (resolution 2 microV, range 33.3 mV), 5-kHzsampling, and low-pass filtering prior to the main gain stage. Imaging artifactis reduced by subtracting an averaged artifact waveform, followed by adaptivenoise cancellation to reduce any residual artifact. This method was validated in recordings from five subjects using periodic and continuous fMRI sequences.Spectral analysis revealed differences of only 10 to 18% between EEG recorded in the scanner without fMRI and the corrected EEG. Ninety-nine percent of spikewaves (median 74 microV) added to the recordings were identified in the correctedEEG compared to 12% in the uncorrected EEG. The median noise after artifactreduction was 8 microV. All these measures indicate that most of the artifact wasremoved, with minimal EEG distortion. Using this recording system and artifactreduction method, we have demonstrated that simultaneous EEG/fMRI studies are forthe first time possible, extending the scope of EEG/fMRI studies considerably.\",\"A method for removing imaging artifact from continuous EEG recorded duringfunctional MRI.\"\n",
            "\"Kohlschutter syndrome is a rare neurodegenerative disorder presenting withintractable seizures, developmental regression and characteristic hypoplasticdental enamel indicative of amelogenesis imperfecta. We report a new family with two affected siblings.\",\"Kohlschutter syndrome in siblings.\"\n",
            "\"Statistical analysis of neuroimages is commonly approached with intergroupcomparisons made by repeated application of univariate or multivariate testsperformed on the set of the regions of interest sampled in the acquired images.The use of such large numbers of tests requires application of techniques forcorrection for multiple comparisons. Standard multiple comparison adjustments(such as the Bonferroni) may be overly conservative when data are correlatedand/or not normally distributed. Resampling-based step-down procedures thatsuccessfully account for unknown correlation structures in the data have recentlybeen introduced. We combined resampling step-down procedures with the MinimumVariance Adaptive method, which allows selection of an optimal test statisticfrom a predefined class of statistics for the data under analysis. As shown insimulation studies and analysis of autoradiographic data, the combined technique exhibits a significant increase in statistical power, even for small sample sizes(n = 8, 9, 10).\",\"Selection of an adaptive test statistic for use with multiple comparison analysesof neuroimaging data.\"\n",
            "\"The synthetic DOX-LNA conjugate was characterized by proton nuclear magneticresonance and mass spectrometry. In addition, the purity of the conjugate wasanalyzed by reverse-phase high-performance liquid chromatography. The cellularuptake, intracellular distribution, and cytotoxicity of DOX-LNA were assessed by flow cytometry, fluorescence microscopy, liquid chromatography/electrosprayionization tandem mass spectrometry, and the tetrazolium dye assay using the invitro cell models. The DOX-LNA conjugate showed substantially highertumor-specific cytotoxicity compared with DOX.\",\"Conjugation with alpha-linolenic acid improves cancer cell uptake andcytotoxicity of doxorubicin.\"\n",
            "\"Our objective was to compare three different methods of blood pressuremeasurement through the results of a controlled study aimed at comparing theantihypertensive effects of trandolapril and losartan. Two hundred andtwenty-nine hypertensive patients were randomized in a double-blind parallelgroup study. After a 3-week placebo period, they received either 2 mgtrandolapril or 50 mg losartan once daily for 6 weeks. At the end of both placeboand active treatment periods, three methods of blood pressure measurement wereused: a) office blood pressure (three consecutive measurements); b) home selfblood pressure measurements (SBPM), consisting of three consecutive measurements performed at home in the morning and in the evening for 7 consecutive days; andc) ambulatory blood pressure measurements (ABPM), 24-h BP recordings with threemeasurements per hour. Of the 229 patients, 199 (87%) performed at least 12 validSBPM measurements during both placebo and treatment periods, whereas only 160(70%) performed good quality 24-h ABPM recordings during both periods (P <.0001). One hundred-forty patients performed the three methods of measurementwell. At baseline and with treatment, agreement between office measurements andABPM or SBPM was weak. Conversely, there was a good agreement between ABPM andSBPM. The mean difference (SBP/DBP) between ABPM and SBPM was 4.6 +/- 10.4/3.5+/- 7.1 at baseline and 3.5 +/- 10.0/4.0 +/- 7.0 at the end of the treatmentperiod. The correlation between SBPM and ABPM expressed by the r coefficient and the P values were the following: at baseline 0.79/0.70 (< 0.001/< .0001), withactive treatment 0.74/0.69 (0.0001/.0001). Hourly and 24-h reproducibility ofblood pressure response was quantified by the standard deviation of BP response. Compared with office blood pressure, both global and hourly SBPM responsesexhibited a lower standard deviation. Hourly reproducibility of SBPM response(10.8 mm Hg/6.9 mm Hg) was lower than hourly reproducibility of ABPM response(15.6 mm Hg/11.9 mm Hg). In conclusion, SBPM was easier to perform than ABPM.There was a good agreement between these two methods whereas concordance between SBPM or ABPM and office measurements was weak. As hourly reproducibility of SBPM response is better than reproducibility of both hourly ABPM and office BPresponse, SBPM seems to be the most appropriate method for evaluating residualantihypertensive effect.\",\"Comparison of three blood pressure measurement methods for the evaluation of two antihypertensive drugs: feasibility, agreement, and reproducibility of bloodpressure response.\"\n",
            "\"We conducted a phase II study to assess the efficacy and tolerability ofirinotecan and cisplatin as salvage chemotherapy in patients with advancedgastric adenocarcinoma, progressing after both 5-fluorouracil (5-FU)- andtaxane-containing regimen. Patients with measurable metastatic gastric cancer,progressive after previous chemotherapy that consisted either of a 5-FU-basedregimen followed by second-line chemotherapy containing taxanes or a 5-FU andtaxane combination were treated with irinotecan and cisplatin. Irinotecan 70mg/m(2) was administered on day 1 and day 15; cisplatin 70 mg/m(2) wasadministered on day 1. Treatment was repeated every 4 weeks. For 28 patientsregistered, a total of 94 chemotherapy cycles were administered. The patients'median age was 51 years and 27 (96%) had an ECOG performance status of 1 orbelow. In an intent-to-treat analysis, seven patients (25%) achieved a partialresponse, which maintained for 6.3 months (95% confidence interval 6.2-6.4months). The median progression-free and overall survival were 3.5 and 5.6months, respectively. Major toxic effects included nausea, diarrhea andneurotoxicity. Although there was one possible treatment-related death, toxicity profiles were generally predictable and manageable. We conclude that irinotecanand cisplatin is an active combination for patients with metastatic gastriccancer in whom previous chemotherapy with 5-FU and taxanes has failed.\",\"Salvage chemotherapy with irinotecan and cisplatin in patients with metastaticgastric cancer failing both 5-fluorouracil and taxanes.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7JY4SvwPEKN",
        "colab_type": "code",
        "outputId": "45845ccd-7a0b-4e2a-9146-3f85f9c5205b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "pubMedDF = spark.read\\\n",
        "                .option(\"header\", \"true\")\\\n",
        "                .csv(\"./pubmed-sample.csv\")\\\n",
        "                .filter(\"AB IS NOT null\")\\\n",
        "                .withColumnRenamed(\"AB\", \"text\")\\\n",
        "                .drop(\"TI\")\n",
        "\n",
        "pubMedDF.show(truncate=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                text|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potas...|\n",
            "|BACKGROUND: At present, it is one of the most important issues for the treatment of breast cancer...|\n",
            "|OBJECTIVE: To investigate the relationship between preoperative atrialfibrillation and early and ...|\n",
            "|Combined EEG/fMRI recording has been used to localize the generators of EEGevents and to identify...|\n",
            "|Kohlschutter syndrome is a rare neurodegenerative disorder presenting withintractable seizures, d...|\n",
            "|Statistical analysis of neuroimages is commonly approached with intergroupcomparisons made by rep...|\n",
            "|The synthetic DOX-LNA conjugate was characterized by proton nuclear magneticresonance and mass sp...|\n",
            "|Our objective was to compare three different methods of blood pressuremeasurement through the res...|\n",
            "|We conducted a phase II study to assess the efficacy and tolerability ofirinotecan and cisplatin ...|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoenzyme that catalyzes the oxidative demethylation o...|\n",
            "|We presented the tachinid fly Exorista japonica with moving host models: afreeze-dried larva of t...|\n",
            "|The literature dealing with the water conducting properties of sapwood xylem intrees is inconsist...|\n",
            "|A novel approach to synthesize chitosan-O-isopropyl-5'-O-d4T monophosphateconjugate was developed...|\n",
            "|An HPLC-ESI-MS-MS method has been developed for the quantitative determination offour diterpenoid...|\n",
            "|The localizing and lateralizing values of eye and head ictal deviations duringfrontal lobe seizur...|\n",
            "|OBJECTIVE: To evaluate the effectiveness and acceptability of expectantmanagement of induced and ...|\n",
            "|For the construction of new combinatorial libraries, a lead compound was created by replacing the...|\n",
            "|We report the results of a screen for genetic association with urinary arsenicmetabolite levels i...|\n",
            "|Intraparenchymal pericatheter cyst is rarely reported. Obstruction in theventriculoperitoneal shu...|\n",
            "|It is known that patients with Klinefelter's syndrome are inclined to developconcomitant malignan...|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OheFUk8XPEA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's create a file which will includes the entities we want to find\n",
        "# it will be stored in content directory\n",
        "# It is used for the EXACT WORD/WORDS we're looking for\n",
        "\n",
        "entities = ['KCNJ9', 'GIRK', 'diabates mellitus', 'nucleotide polymorphisms']\n",
        "\n",
        "with open('clinical_entities.txt', 'w') as f:\n",
        "    for i in entities:\n",
        "        f.write(i+'\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IacDcBWvPD6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "entity_extractor = TextMatcher()\\\n",
        "                    .setInputCols(['document', 'token'])\\\n",
        "                    .setOutputCol('matched_entities')\\\n",
        "                    .setEntities('clinical_entities.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x_dkTB2Yp8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " tokenizer,\n",
        " entity_extractor\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXLExr3gZykQ",
        "colab_type": "code",
        "outputId": "64cf811f-7162-476e-d1cc-66859c89eb59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pubMedDF.count()\n",
        "# it has 7.5K rows"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7537"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmx7F2BFYqE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipelineModel.transform(pubMedDF.limit(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8IJVu2vaSNA",
        "colab_type": "code",
        "outputId": "cee016a0-ebdf-4c15-c1e8-e304212b7288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pubMedDF.select('text').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(text='The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family. Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population. The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons separated byapproximately 2.2 and approximately 2.6 kb introns, respectively. We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair (bp) insertion/deletion. Ourexpression studies revealed the presence of the transcript in various humantissues including pancreas, and two major insulin-responsive tissues: fat andskeletal muscle. The characterization of the KCNJ9 gene should facilitate furtherstudies on the function of the KCNJ9 protein and allow evaluation of thepotential role of the locus in Type II diabetes.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn-KdtXOYqJ3",
        "colab_type": "code",
        "outputId": "8c02b6fd-21d5-4276-be4f-7cfd3175c9f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result.select('matched_entities.result').take(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=['KCNJ9', 'GIRK', 'KCNJ9', 'nucleotide polymorphisms', 'KCNJ9', 'KCNJ9'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkRaa11fYqV5",
        "colab_type": "code",
        "outputId": "1ce77ffe-5842-4d85-91a0-3c5d73766576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "result_df = result.select(F.explode(F.arrays_zip('matched_entities.result', 'matched_entities.begin',  'matched_entities.end')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"matched_entities\"),\n",
        "        F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "        F.expr(\"cols['2']\").alias(\"end\")).toPandas()\n",
        "\n",
        "result_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>matched_entities</th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KCNJ9</td>\n",
              "      <td>10</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GIRK</td>\n",
              "      <td>103</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KCNJ9</td>\n",
              "      <td>173</td>\n",
              "      <td>177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>nucleotide polymorphisms</td>\n",
              "      <td>471</td>\n",
              "      <td>494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KCNJ9</td>\n",
              "      <td>801</td>\n",
              "      <td>805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KCNJ9</td>\n",
              "      <td>868</td>\n",
              "      <td>872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           matched_entities  begin  end\n",
              "0                     KCNJ9     10   14\n",
              "1                      GIRK    103  106\n",
              "2                     KCNJ9    173  177\n",
              "3  nucleotide polymorphisms    471  494\n",
              "4                     KCNJ9    801  805\n",
              "5                     KCNJ9    868  872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uem87EYkavpR",
        "colab_type": "text"
      },
      "source": [
        "## RegexMatcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc77Q2wHYqaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rules = '''\n",
        "renal\\s\\w+, followed by 'renal'\n",
        "cardiac\\s\\w+, followed by 'cardiac'\n",
        "'''\n",
        "\n",
        "with open('regex_rules.txt', 'w') as f:\n",
        "    \n",
        "    f.write(rules)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pc_0KeSYqha",
        "colab_type": "code",
        "outputId": "4212d498-ba26-48f5-e89c-df23d777ca2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        ".setInputCol(\"text\")\\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "regex_matcher = RegexMatcher()\\\n",
        "    .setInputCols('document')\\\n",
        "    .setStrategy(\"MATCH_ALL\")\\\n",
        "    .setOutputCol(\"regex_matches\")\\\n",
        "    .setExternalRules(path='./regex_rules.txt', delimiter=',')\n",
        "    \n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " regex_matcher\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "match_df = pipelineModel.transform(pubMedDF)\n",
        "\n",
        "match_df.select('regex_matches.result').take(3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(result=[]),\n",
              " Row(result=[]),\n",
              " Row(result=['renal failure', 'cardiac surgery', 'cardiac surgery', 'cardiac surgical'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amg37JTvYqez",
        "colab_type": "code",
        "outputId": "7ed6259d-7169-434d-a6cd-8484e52fa9a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Sadece regex iceren satirlari gormek istersek\n",
        "\n",
        "match_df.select('text','regex_matches.result')\\\n",
        ".toDF('text','matches').filter(F.size('matches')>1)\\\n",
        ".show(truncate=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                                           matches|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
            "|Aging is a physiological process that causes st...|                     [renal function, renal aging]|\n",
            "|BACKGROUND: Left-sided inferior vena cava (IVC)...|          [renal surgery, renal mass, renal veins]|\n",
            "|A successful kidney transplantation (Tx) offers...|                         [renal failure, renal Tx]|\n",
            "|The transcription factor scleraxis has been imp...|[cardiac valves, cardiac fibroblasts, cardiac c...|\n",
            "|Historically, the only surgical option for remo...|                        [renal mass, renal masses]|\n",
            "|OBJECTIVES: To examine the clinical effectivene...|   [renal transplantation, renal transplantgroups]|\n",
            "|The cardiac natriuretic peptides (NP) atrial na...|[renal mechanisms, renal function, cardiac natr...|\n",
            "|Patients with chronic congestive heart failure ...|[cardiac remodeling, cardiac receptorswith, car...|\n",
            "|OBJECTIVE: To discuss the long-term results of ...|       [cardiac insufficiency, cardiac infarction]|\n",
            "|Bone loss and osteoporotic fractures are common...|     [cardiac transplant, cardiac transplantation]|\n",
            "|The Clinical Guidelines Committee of the Europe...|[cardiac arrest, cardiac surgery, cardiac surge...|\n",
            "|Superoxide anion contributes to the pathogenesi...|[renal hemodynamics, renal hemodynamics, renal ...|\n",
            "|Macrophage-derived factor (MDF) is a lipophilic...|                    [renal glands, renal cortical]|\n",
            "|Quantification of regional myocardial blood flo...|                 [cardiac disease, cardiac events]|\n",
            "|Higher levels of long-chain n-3 polyunsaturated...|[cardiac arrest, cardiac arrest, cardiac arrest...|\n",
            "|Well-known complications related to drug abuse ...|                [renal infarction, cardiac causes]|\n",
            "|The renal glomerulus, the site of plasma ultraf...|              [renal glomerulus, renal epithelial]|\n",
            "|OBJECTIVE: The rise in the number of implantati...|[cardiac pacemakers, cardiac pacemakers, cardia...|\n",
            "|OBJECTIVES: To examine determinants of use of c...|          [cardiac procedures, cardiac procedures]|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2MSXsjbYqUO",
        "colab_type": "code",
        "outputId": "5e8b1143-770d-4f59-d714-8486f4a040f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "match_df.printSchema()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- document: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            " |-- regex_matches: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- annotatorType: string (nullable = true)\n",
            " |    |    |-- begin: integer (nullable = false)\n",
            " |    |    |-- end: integer (nullable = false)\n",
            " |    |    |-- result: string (nullable = true)\n",
            " |    |    |-- metadata: map (nullable = true)\n",
            " |    |    |    |-- key: string\n",
            " |    |    |    |-- value: string (valueContainsNull = true)\n",
            " |    |    |-- embeddings: array (nullable = true)\n",
            " |    |    |    |-- element: float (containsNull = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWjMza7cc4O4",
        "colab_type": "text"
      },
      "source": [
        "## Finisher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEq8-TnNYqSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It removes previous columns which is defined in pipeline and shows only the one which is defined in finisher\n",
        "# Finisher: Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. \n",
        "# The Finisher outputs annotation(s) values into a string."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1V_kiNOYqP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finisher = Finisher()\\\n",
        "        .setInputCols(['regex_matches'])\\\n",
        "        .setIncludeMetadata(False) # set False to remove metadata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_Ehw0wDYqCt",
        "colab_type": "code",
        "outputId": "c322f2c8-bd6b-42f4-f219-bc74065fc99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "nlpPipeline = Pipeline(stages=[\n",
        " documentAssembler, \n",
        " regex_matcher,\n",
        " finisher\n",
        " ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "match_df = pipelineModel.transform(pubMedDF)\n",
        "\n",
        "match_df.show(truncate = 50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                            finished_regex_matches|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|The human KCNJ9 (Kir 3.3, GIRK3) is a member of...|                                                []|\n",
            "|BACKGROUND: At present, it is one of the most i...|                                                []|\n",
            "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
            "|Combined EEG/fMRI recording has been used to lo...|                                                []|\n",
            "|Kohlschutter syndrome is a rare neurodegenerati...|                                                []|\n",
            "|Statistical analysis of neuroimages is commonly...|                                                []|\n",
            "|The synthetic DOX-LNA conjugate was characteriz...|                                                []|\n",
            "|Our objective was to compare three different me...|                                                []|\n",
            "|We conducted a phase II study to assess the eff...|                                                []|\n",
            "|\"Monomeric sarcosine oxidase (MSOX) is a flavoe...|                                                []|\n",
            "|We presented the tachinid fly Exorista japonica...|                                                []|\n",
            "|The literature dealing with the water conductin...|                                                []|\n",
            "|A novel approach to synthesize chitosan-O-isopr...|                                                []|\n",
            "|An HPLC-ESI-MS-MS method has been developed for...|                                                []|\n",
            "|The localizing and lateralizing values of eye a...|                                                []|\n",
            "|OBJECTIVE: To evaluate the effectiveness and ac...|                                                []|\n",
            "|For the construction of new combinatorial libra...|                                                []|\n",
            "|We report the results of a screen for genetic a...|                                                []|\n",
            "|Intraparenchymal pericatheter cyst is rarely re...|                                                []|\n",
            "|It is known that patients with Klinefelter's sy...|                                                []|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME-kQNWwYqAR",
        "colab_type": "code",
        "outputId": "fa100db1-0f8c-4b8a-ef73-a33db0684519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "match_df.printSchema()\n",
        "# it is very simple now"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- finished_regex_matches: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uku0rGBeKyM",
        "colab_type": "code",
        "outputId": "39117836-3c1f-4686-9daf-422fa39e1e4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# If we want to filter out the empty matches\n",
        "\n",
        "match_df.filter(F.size('finished_regex_matches')>=1).show(truncate = 50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|                                              text|                            finished_regex_matches|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "|OBJECTIVE: To investigate the relationship betw...|[renal failure, cardiac surgery, cardiac surger...|\n",
            "|Successful renal transplantation is associated ...|                           [renal transplantation]|\n",
            "|Although inhaled and intranasal corticosteroids...|                               [renal suppression]|\n",
            "|Aging is a physiological process that causes st...|                     [renal function, renal aging]|\n",
            "|Tight junctions form the major paracellular bar...|                                     [renal tight]|\n",
            "|BACKGROUND: Left-sided inferior vena cava (IVC)...|          [renal surgery, renal mass, renal veins]|\n",
            "|A successful kidney transplantation (Tx) offers...|                         [renal failure, renal Tx]|\n",
            "|The transcription factor scleraxis has been imp...|[cardiac valves, cardiac fibroblasts, cardiac c...|\n",
            "|OBJECTIVE: Although present treatment programs ...|                                      [renal cell]|\n",
            "|\"The aim of this study was to evaluate and comp...|                                      [renal cell]|\n",
            "|Historically, the only surgical option for remo...|                        [renal mass, renal masses]|\n",
            "|Patients suffering from sleep apnea syndrome ar...|                                     [cardiac and]|\n",
            "|BACKGROUND: Patients with chronic renal failure...|                                   [renal failure]|\n",
            "|The phosphorylation of glucose, a crucial step ...|                                   [renal tumours]|\n",
            "|The effects of glucose, sorbitol and xylitol in...|                                  [renal filtered]|\n",
            "|INTRODUCTION AND OBJECTIVES: Endothelial dysfun...|                                 [cardiac disease]|\n",
            "|OBJECTIVES: To examine the clinical effectivene...|   [renal transplantation, renal transplantgroups]|\n",
            "|Cardiovascular event rates are markedly increas...|                                  [renal function]|\n",
            "|The kidney (11-HSD2 or 11-HSDK) isozyme of 11be...|                                      [renal cell]|\n",
            "|The cardiac natriuretic peptides (NP) atrial na...|[renal mechanisms, renal function, cardiac natr...|\n",
            "+--------------------------------------------------+--------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ8c5jtBfb93",
        "colab_type": "text"
      },
      "source": [
        "## LightPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e12lGr_7fpPO",
        "colab_type": "text"
      },
      "source": [
        "LightPipelines are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to **deal with smaller amounts of data**. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.\n",
        "\n",
        "Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don't even need to convert the input text to DataFrame in order to feed it into a pipeline that's accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.\n",
        "\n",
        "It is nearly 20x faster than using Spark ML Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3_uTFXZg_hH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note we re-run the pipelineModel which inludes 'lemma', 'stem' etc above so the the very last one we're processing which is coming \n",
        "# from regex_matches. Please pya attention that"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_FpV7i2eK56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "\n",
        "light_model = LightPipeline(pipelineModel)\n",
        "\n",
        "light_result = light_model.annotate(\"John and Peter are brothers. However they don't support each other that much\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wt-O7jOeLB3",
        "colab_type": "code",
        "outputId": "651f55a0-43ef-4264-ce4e-cfd08c0c4f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "light_result.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['document', 'token', 'stem', 'lemma'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46E_2U7ReK2H",
        "colab_type": "code",
        "outputId": "6856d525-8a19-4a5e-ccf4-7bd3781baba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "list(zip(light_result['token'], light_result['stem'], light_result['lemma']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'john', 'John'),\n",
              " ('and', 'and', 'and'),\n",
              " ('Peter', 'peter', 'Peter'),\n",
              " ('are', 'ar', 'be'),\n",
              " ('brothers', 'brother', 'brother'),\n",
              " ('.', '.', '.'),\n",
              " ('However', 'howev', 'However'),\n",
              " ('they', 'thei', 'they'),\n",
              " (\"don't\", \"don't\", \"don't\"),\n",
              " ('support', 'support', 'support'),\n",
              " ('each', 'each', 'each'),\n",
              " ('other', 'other', 'other'),\n",
              " ('that', 'that', 'that'),\n",
              " ('much', 'much', 'much')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MXiLCu3eKvq",
        "colab_type": "code",
        "outputId": "3488326b-c2a7-4a3d-9757-12dad377279a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "light_result = light_model.fullAnnotate(\"John and Peter are brothers. However they don't support each other that much\")\n",
        "light_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'document': [Annotation(document, 0, 75, John and Peter are brothers. However they don't support each other that much, {})],\n",
              "  'lemma': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, be, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'})],\n",
              "  'stem': [Annotation(token, 0, 3, john, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, ar, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brother, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, howev, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, thei, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'})],\n",
              "  'token': [Annotation(token, 0, 3, John, {'sentence': '0'}),\n",
              "   Annotation(token, 5, 7, and, {'sentence': '0'}),\n",
              "   Annotation(token, 9, 13, Peter, {'sentence': '0'}),\n",
              "   Annotation(token, 15, 17, are, {'sentence': '0'}),\n",
              "   Annotation(token, 19, 26, brothers, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 27, ., {'sentence': '0'}),\n",
              "   Annotation(token, 29, 35, However, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 40, they, {'sentence': '0'}),\n",
              "   Annotation(token, 42, 46, don't, {'sentence': '0'}),\n",
              "   Annotation(token, 48, 54, support, {'sentence': '0'}),\n",
              "   Annotation(token, 56, 59, each, {'sentence': '0'}),\n",
              "   Annotation(token, 61, 65, other, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 70, that, {'sentence': '0'}),\n",
              "   Annotation(token, 72, 75, much, {'sentence': '0'})]}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTt5xvL9eKs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### First NoteBook Can Be Seen From Here to Below"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_8WNsDjxibK",
        "colab_type": "text"
      },
      "source": [
        "## Using Pretrained Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKx2DGFPxtbJ",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/JohnSnowLabs/spark-nlp-models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alFZ1XjCxtSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjNHvZUpwxNA",
        "colab_type": "code",
        "outputId": "16f969b8-95d1-4809-d7df-2eb2d6456b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "pipeline = PretrainedPipeline('explain_document_ml', lang='en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0896d6b45bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'explain_document_ml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'PretrainedPipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIKWJt0Fy8gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testDoc = \"\"\"French author who helped pioner the science-fiction genre. \\\n",
        "Verne wrate about space, air, and underwater travel before navigable aircrast and \\\n",
        "practical submarines were invented, and before any means of space travel had been devised.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXvUosLUz3i5",
        "colab_type": "code",
        "outputId": "9d1bf0ea-e340-4a20-df8e-ba52a9ed05cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "testDoc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'French author who helped pioner the science-fiction genre. Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented, and before any means of space travel had been devised.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m5nUlVSz7iU",
        "colab_type": "code",
        "outputId": "9d2428c9-c11b-4ba3-e429-1e1d21c1cfad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "result = pipeline.annotate(testDoc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 34.9 ms, sys: 9.77 ms, total: 44.6 ms\n",
            "Wall time: 2.26 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6gqwXHZ0HuR",
        "colab_type": "code",
        "outputId": "87c31ceb-b74f-4c12-aa4c-dc03116c3243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['document', 'spell', 'pos', 'lemmas', 'token', 'stems', 'sentence'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBtaJpxm0n84",
        "colab_type": "code",
        "outputId": "2a403ac4-58a2-4ee5-c046-edaad9f84a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "result['sentence']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['French author who helped pioner the science-fiction genre.',\n",
              " 'Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented, and before any means of space travel had been devised.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxGKhrNe0yIt",
        "colab_type": "code",
        "outputId": "6fbaae98-daaa-4036-9f22-6bdc853791f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "result['token']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['French',\n",
              " 'author',\n",
              " 'who',\n",
              " 'helped',\n",
              " 'pioner',\n",
              " 'the',\n",
              " 'science-fiction',\n",
              " 'genre',\n",
              " '.',\n",
              " 'Verne',\n",
              " 'wrate',\n",
              " 'about',\n",
              " 'space',\n",
              " ',',\n",
              " 'air',\n",
              " ',',\n",
              " 'and',\n",
              " 'underwater',\n",
              " 'travel',\n",
              " 'before',\n",
              " 'navigable',\n",
              " 'aircrast',\n",
              " 'and',\n",
              " 'practical',\n",
              " 'submarines',\n",
              " 'were',\n",
              " 'invented',\n",
              " ',',\n",
              " 'and',\n",
              " 'before',\n",
              " 'any',\n",
              " 'means',\n",
              " 'of',\n",
              " 'space',\n",
              " 'travel',\n",
              " 'had',\n",
              " 'been',\n",
              " 'devised',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPBB1A1J0z1g",
        "colab_type": "code",
        "outputId": "f69e9ae3-93e3-4a7d-907d-ad2a2d610b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "list(zip(result['token'], result['pos']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('French', 'JJ'),\n",
              " ('author', 'NN'),\n",
              " ('who', 'WP'),\n",
              " ('helped', 'VBD'),\n",
              " ('pioner', 'NN'),\n",
              " ('the', 'DT'),\n",
              " ('science-fiction', 'NN'),\n",
              " ('genre', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Verne', 'NNP'),\n",
              " ('wrate', 'VBD'),\n",
              " ('about', 'IN'),\n",
              " ('space', 'NN'),\n",
              " (',', ','),\n",
              " ('air', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('underwater', 'JJ'),\n",
              " ('travel', 'NN'),\n",
              " ('before', 'IN'),\n",
              " ('navigable', 'JJ'),\n",
              " ('aircrast', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('practical', 'JJ'),\n",
              " ('submarines', 'NNS'),\n",
              " ('were', 'VBD'),\n",
              " ('invented', 'VBN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('before', 'IN'),\n",
              " ('any', 'DT'),\n",
              " ('means', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('space', 'NN'),\n",
              " ('travel', 'NN'),\n",
              " ('had', 'VBD'),\n",
              " ('been', 'VBN'),\n",
              " ('devised', 'VBN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fL_GxQ71Rat",
        "colab_type": "code",
        "outputId": "9c3a6097-fec6-4fc2-ae3f-4724d8692401",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "list(zip(result['token'], result['lemmas'], result['stems'], result['spell']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('French', 'French', 'french', 'French'),\n",
              " ('author', 'author', 'author', 'author'),\n",
              " ('who', 'who', 'who', 'who'),\n",
              " ('helped', 'help', 'help', 'helped'),\n",
              " ('pioner', 'pioneer', 'pioneer', 'pioneer'),\n",
              " ('the', 'the', 'the', 'the'),\n",
              " ('science-fiction', 'sciencefiction', 'sciencefict', 'sciencefiction'),\n",
              " ('genre', 'genre', 'genr', 'genre'),\n",
              " ('.', '.', '.', '.'),\n",
              " ('Verne', 'Verne', 'vern', 'Verne'),\n",
              " ('wrate', 'write', 'wrote', 'wrote'),\n",
              " ('about', 'about', 'about', 'about'),\n",
              " ('space', 'space', 'space', 'space'),\n",
              " (',', ',', ',', ','),\n",
              " ('air', 'air', 'air', 'air'),\n",
              " (',', ',', ',', ','),\n",
              " ('and', 'and', 'and', 'and'),\n",
              " ('underwater', 'underwater', 'underwat', 'underwater'),\n",
              " ('travel', 'travel', 'travel', 'travel'),\n",
              " ('before', 'before', 'befor', 'before'),\n",
              " ('navigable', 'navigable', 'navig', 'navigable'),\n",
              " ('aircrast', 'aircraft', 'aircraft', 'aircraft'),\n",
              " ('and', 'and', 'and', 'and'),\n",
              " ('practical', 'practical', 'practic', 'practical'),\n",
              " ('submarines', 'submarine', 'submarin', 'submarines'),\n",
              " ('were', 'be', 'were', 'were'),\n",
              " ('invented', 'invent', 'invent', 'invented'),\n",
              " (',', ',', ',', ','),\n",
              " ('and', 'and', 'and', 'and'),\n",
              " ('before', 'before', 'befor', 'before'),\n",
              " ('any', 'any', 'ani', 'any'),\n",
              " ('means', 'mean', 'mean', 'means'),\n",
              " ('of', 'of', 'of', 'of'),\n",
              " ('space', 'space', 'space', 'space'),\n",
              " ('travel', 'travel', 'travel', 'travel'),\n",
              " ('had', 'have', 'had', 'had'),\n",
              " ('been', 'be', 'been', 'been'),\n",
              " ('devised', 'devise', 'devis', 'devised'),\n",
              " ('.', '.', '.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LUM2iOp1jiR",
        "colab_type": "code",
        "outputId": "929af8dc-b595-4803-bfcc-95d843fc39bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.DataFrame({\"token\":result['token'],\n",
        "                   \"corrected\":result['spell'],\n",
        "                   'POS':result['pos'],\n",
        "                   'lemmas':result['lemmas'],\n",
        "                   'stems':result['stems']}) \n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>corrected</th>\n",
              "      <th>POS</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>stems</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>French</td>\n",
              "      <td>French</td>\n",
              "      <td>JJ</td>\n",
              "      <td>French</td>\n",
              "      <td>french</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>author</td>\n",
              "      <td>author</td>\n",
              "      <td>NN</td>\n",
              "      <td>author</td>\n",
              "      <td>author</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>who</td>\n",
              "      <td>who</td>\n",
              "      <td>WP</td>\n",
              "      <td>who</td>\n",
              "      <td>who</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>helped</td>\n",
              "      <td>helped</td>\n",
              "      <td>VBD</td>\n",
              "      <td>help</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pioner</td>\n",
              "      <td>pioneer</td>\n",
              "      <td>NN</td>\n",
              "      <td>pioneer</td>\n",
              "      <td>pioneer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>science-fiction</td>\n",
              "      <td>sciencefiction</td>\n",
              "      <td>NN</td>\n",
              "      <td>sciencefiction</td>\n",
              "      <td>sciencefict</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>genre</td>\n",
              "      <td>genre</td>\n",
              "      <td>NN</td>\n",
              "      <td>genre</td>\n",
              "      <td>genr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Verne</td>\n",
              "      <td>Verne</td>\n",
              "      <td>NNP</td>\n",
              "      <td>Verne</td>\n",
              "      <td>vern</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>wrate</td>\n",
              "      <td>wrote</td>\n",
              "      <td>VBD</td>\n",
              "      <td>write</td>\n",
              "      <td>wrote</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>about</td>\n",
              "      <td>about</td>\n",
              "      <td>IN</td>\n",
              "      <td>about</td>\n",
              "      <td>about</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>space</td>\n",
              "      <td>space</td>\n",
              "      <td>NN</td>\n",
              "      <td>space</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>air</td>\n",
              "      <td>air</td>\n",
              "      <td>NN</td>\n",
              "      <td>air</td>\n",
              "      <td>air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>underwater</td>\n",
              "      <td>underwater</td>\n",
              "      <td>JJ</td>\n",
              "      <td>underwater</td>\n",
              "      <td>underwat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "      <td>NN</td>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>before</td>\n",
              "      <td>before</td>\n",
              "      <td>IN</td>\n",
              "      <td>before</td>\n",
              "      <td>befor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>navigable</td>\n",
              "      <td>navigable</td>\n",
              "      <td>JJ</td>\n",
              "      <td>navigable</td>\n",
              "      <td>navig</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>aircrast</td>\n",
              "      <td>aircraft</td>\n",
              "      <td>NN</td>\n",
              "      <td>aircraft</td>\n",
              "      <td>aircraft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>practical</td>\n",
              "      <td>practical</td>\n",
              "      <td>JJ</td>\n",
              "      <td>practical</td>\n",
              "      <td>practic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>submarines</td>\n",
              "      <td>submarines</td>\n",
              "      <td>NNS</td>\n",
              "      <td>submarine</td>\n",
              "      <td>submarin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>were</td>\n",
              "      <td>were</td>\n",
              "      <td>VBD</td>\n",
              "      <td>be</td>\n",
              "      <td>were</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>invented</td>\n",
              "      <td>invented</td>\n",
              "      <td>VBN</td>\n",
              "      <td>invent</td>\n",
              "      <td>invent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>before</td>\n",
              "      <td>before</td>\n",
              "      <td>IN</td>\n",
              "      <td>before</td>\n",
              "      <td>befor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>any</td>\n",
              "      <td>any</td>\n",
              "      <td>DT</td>\n",
              "      <td>any</td>\n",
              "      <td>ani</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>means</td>\n",
              "      <td>means</td>\n",
              "      <td>NNS</td>\n",
              "      <td>mean</td>\n",
              "      <td>mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>space</td>\n",
              "      <td>space</td>\n",
              "      <td>NN</td>\n",
              "      <td>space</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "      <td>NN</td>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>had</td>\n",
              "      <td>had</td>\n",
              "      <td>VBD</td>\n",
              "      <td>have</td>\n",
              "      <td>had</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>been</td>\n",
              "      <td>been</td>\n",
              "      <td>VBN</td>\n",
              "      <td>be</td>\n",
              "      <td>been</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>devised</td>\n",
              "      <td>devised</td>\n",
              "      <td>VBN</td>\n",
              "      <td>devise</td>\n",
              "      <td>devis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              token       corrected  POS          lemmas        stems\n",
              "0            French          French   JJ          French       french\n",
              "1            author          author   NN          author       author\n",
              "2               who             who   WP             who          who\n",
              "3            helped          helped  VBD            help         help\n",
              "4            pioner         pioneer   NN         pioneer      pioneer\n",
              "5               the             the   DT             the          the\n",
              "6   science-fiction  sciencefiction   NN  sciencefiction  sciencefict\n",
              "7             genre           genre   NN           genre         genr\n",
              "8                 .               .    .               .            .\n",
              "9             Verne           Verne  NNP           Verne         vern\n",
              "10            wrate           wrote  VBD           write        wrote\n",
              "11            about           about   IN           about        about\n",
              "12            space           space   NN           space        space\n",
              "13                ,               ,    ,               ,            ,\n",
              "14              air             air   NN             air          air\n",
              "15                ,               ,    ,               ,            ,\n",
              "16              and             and   CC             and          and\n",
              "17       underwater      underwater   JJ      underwater     underwat\n",
              "18           travel          travel   NN          travel       travel\n",
              "19           before          before   IN          before        befor\n",
              "20        navigable       navigable   JJ       navigable        navig\n",
              "21         aircrast        aircraft   NN        aircraft     aircraft\n",
              "22              and             and   CC             and          and\n",
              "23        practical       practical   JJ       practical      practic\n",
              "24       submarines      submarines  NNS       submarine     submarin\n",
              "25             were            were  VBD              be         were\n",
              "26         invented        invented  VBN          invent       invent\n",
              "27                ,               ,    ,               ,            ,\n",
              "28              and             and   CC             and          and\n",
              "29           before          before   IN          before        befor\n",
              "30              any             any   DT             any          ani\n",
              "31            means           means  NNS            mean         mean\n",
              "32               of              of   IN              of           of\n",
              "33            space           space   NN           space        space\n",
              "34           travel          travel   NN          travel       travel\n",
              "35              had             had  VBD            have          had\n",
              "36             been            been  VBN              be         been\n",
              "37          devised         devised  VBN          devise        devis\n",
              "38                .               .    .               .            ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zBKH09W3D7u",
        "colab_type": "text"
      },
      "source": [
        "## Explain Document DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMbgcFPr2ss7",
        "colab_type": "code",
        "outputId": "3463e592-2319-44cf-8abd-428f97977343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pipeline_dl = PretrainedPipeline('explain_document_dl', 'en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "explain_document_dl download started this may take some time.\n",
            "Approx size to download 168.4 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HN9R4jS3PyU",
        "colab_type": "code",
        "outputId": "c2969da0-c1a4-43e8-df5c-b03ffae45299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "result = pipeline_dl.annotate(testDoc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 33.1 ms, sys: 12.7 ms, total: 45.8 ms\n",
            "Wall time: 724 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqRgbLR1G_TR",
        "colab_type": "code",
        "outputId": "9cf0b559-d6d3-4ff5-fe76-48bb1bd36ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['entities', 'stem', 'checked', 'lemma', 'document', 'pos', 'token', 'ner', 'embeddings', 'sentence'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_qxIJRrIJLH",
        "colab_type": "code",
        "outputId": "208e1136-a725-4382-9faa-a4cab1b7048e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result['entities']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['French', 'Verne']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv88V9rTIcq8",
        "colab_type": "code",
        "outputId": "374ada7e-5781-4ceb-c0f9-8cf5f856aeb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df = pd.DataFrame({\"token\":result['token'],\n",
        "                   \"ner_label\":result['ner'],\n",
        "                   \"spell_corrected\":result['checked'],\n",
        "                   'POS':result['pos'],\n",
        "                   'lemma':result['lemma'],\n",
        "                   'stem':result['stem']}) \n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>ner_label</th>\n",
              "      <th>spell_corrected</th>\n",
              "      <th>POS</th>\n",
              "      <th>lemma</th>\n",
              "      <th>stem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>French</td>\n",
              "      <td>B-MISC</td>\n",
              "      <td>French</td>\n",
              "      <td>JJ</td>\n",
              "      <td>French</td>\n",
              "      <td>french</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>author</td>\n",
              "      <td>O</td>\n",
              "      <td>author</td>\n",
              "      <td>NN</td>\n",
              "      <td>author</td>\n",
              "      <td>author</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>who</td>\n",
              "      <td>O</td>\n",
              "      <td>who</td>\n",
              "      <td>WP</td>\n",
              "      <td>who</td>\n",
              "      <td>who</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>helped</td>\n",
              "      <td>O</td>\n",
              "      <td>helped</td>\n",
              "      <td>VBD</td>\n",
              "      <td>help</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>pioner</td>\n",
              "      <td>O</td>\n",
              "      <td>pioneer</td>\n",
              "      <td>NN</td>\n",
              "      <td>pioneer</td>\n",
              "      <td>pioneer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>the</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>science-fiction</td>\n",
              "      <td>O</td>\n",
              "      <td>sciencefiction</td>\n",
              "      <td>NN</td>\n",
              "      <td>sciencefiction</td>\n",
              "      <td>sciencefict</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>genre</td>\n",
              "      <td>O</td>\n",
              "      <td>genre</td>\n",
              "      <td>NN</td>\n",
              "      <td>genre</td>\n",
              "      <td>genr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Verne</td>\n",
              "      <td>B-PER</td>\n",
              "      <td>Verne</td>\n",
              "      <td>NNP</td>\n",
              "      <td>Verne</td>\n",
              "      <td>vern</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>wrate</td>\n",
              "      <td>O</td>\n",
              "      <td>wrote</td>\n",
              "      <td>VBD</td>\n",
              "      <td>write</td>\n",
              "      <td>wrote</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>about</td>\n",
              "      <td>O</td>\n",
              "      <td>about</td>\n",
              "      <td>IN</td>\n",
              "      <td>about</td>\n",
              "      <td>about</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>space</td>\n",
              "      <td>O</td>\n",
              "      <td>space</td>\n",
              "      <td>NN</td>\n",
              "      <td>space</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>air</td>\n",
              "      <td>O</td>\n",
              "      <td>air</td>\n",
              "      <td>NN</td>\n",
              "      <td>air</td>\n",
              "      <td>air</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>underwater</td>\n",
              "      <td>O</td>\n",
              "      <td>underwater</td>\n",
              "      <td>JJ</td>\n",
              "      <td>underwater</td>\n",
              "      <td>underwat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>travel</td>\n",
              "      <td>O</td>\n",
              "      <td>travel</td>\n",
              "      <td>NN</td>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>before</td>\n",
              "      <td>O</td>\n",
              "      <td>before</td>\n",
              "      <td>IN</td>\n",
              "      <td>before</td>\n",
              "      <td>befor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>navigable</td>\n",
              "      <td>O</td>\n",
              "      <td>navigable</td>\n",
              "      <td>JJ</td>\n",
              "      <td>navigable</td>\n",
              "      <td>navig</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>aircrast</td>\n",
              "      <td>O</td>\n",
              "      <td>aircraft</td>\n",
              "      <td>NN</td>\n",
              "      <td>aircraft</td>\n",
              "      <td>aircraft</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>practical</td>\n",
              "      <td>O</td>\n",
              "      <td>practical</td>\n",
              "      <td>JJ</td>\n",
              "      <td>practical</td>\n",
              "      <td>practic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>submarines</td>\n",
              "      <td>O</td>\n",
              "      <td>submarines</td>\n",
              "      <td>NNS</td>\n",
              "      <td>submarine</td>\n",
              "      <td>submarin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>were</td>\n",
              "      <td>O</td>\n",
              "      <td>were</td>\n",
              "      <td>VBD</td>\n",
              "      <td>be</td>\n",
              "      <td>were</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>invented</td>\n",
              "      <td>O</td>\n",
              "      <td>invented</td>\n",
              "      <td>VBN</td>\n",
              "      <td>invent</td>\n",
              "      <td>invent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "      <td>,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>and</td>\n",
              "      <td>O</td>\n",
              "      <td>and</td>\n",
              "      <td>CC</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>before</td>\n",
              "      <td>O</td>\n",
              "      <td>before</td>\n",
              "      <td>IN</td>\n",
              "      <td>before</td>\n",
              "      <td>befor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>any</td>\n",
              "      <td>O</td>\n",
              "      <td>any</td>\n",
              "      <td>DT</td>\n",
              "      <td>any</td>\n",
              "      <td>ani</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>means</td>\n",
              "      <td>O</td>\n",
              "      <td>means</td>\n",
              "      <td>NNS</td>\n",
              "      <td>mean</td>\n",
              "      <td>mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>of</td>\n",
              "      <td>O</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>of</td>\n",
              "      <td>of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>space</td>\n",
              "      <td>O</td>\n",
              "      <td>space</td>\n",
              "      <td>NN</td>\n",
              "      <td>space</td>\n",
              "      <td>space</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>travel</td>\n",
              "      <td>O</td>\n",
              "      <td>travel</td>\n",
              "      <td>NN</td>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>had</td>\n",
              "      <td>O</td>\n",
              "      <td>had</td>\n",
              "      <td>VBD</td>\n",
              "      <td>have</td>\n",
              "      <td>had</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>been</td>\n",
              "      <td>O</td>\n",
              "      <td>been</td>\n",
              "      <td>VBN</td>\n",
              "      <td>be</td>\n",
              "      <td>been</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>devised</td>\n",
              "      <td>O</td>\n",
              "      <td>devised</td>\n",
              "      <td>VBN</td>\n",
              "      <td>devise</td>\n",
              "      <td>devis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "      <td>.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              token ner_label spell_corrected  POS           lemma         stem\n",
              "0            French    B-MISC          French   JJ          French       french\n",
              "1            author         O          author   NN          author       author\n",
              "2               who         O             who   WP             who          who\n",
              "3            helped         O          helped  VBD            help         help\n",
              "4            pioner         O         pioneer   NN         pioneer      pioneer\n",
              "5               the         O             the   DT             the          the\n",
              "6   science-fiction         O  sciencefiction   NN  sciencefiction  sciencefict\n",
              "7             genre         O           genre   NN           genre         genr\n",
              "8                 .         O               .    .               .            .\n",
              "9             Verne     B-PER           Verne  NNP           Verne         vern\n",
              "10            wrate         O           wrote  VBD           write        wrote\n",
              "11            about         O           about   IN           about        about\n",
              "12            space         O           space   NN           space        space\n",
              "13                ,         O               ,    ,               ,            ,\n",
              "14              air         O             air   NN             air          air\n",
              "15                ,         O               ,    ,               ,            ,\n",
              "16              and         O             and   CC             and          and\n",
              "17       underwater         O      underwater   JJ      underwater     underwat\n",
              "18           travel         O          travel   NN          travel       travel\n",
              "19           before         O          before   IN          before        befor\n",
              "20        navigable         O       navigable   JJ       navigable        navig\n",
              "21         aircrast         O        aircraft   NN        aircraft     aircraft\n",
              "22              and         O             and   CC             and          and\n",
              "23        practical         O       practical   JJ       practical      practic\n",
              "24       submarines         O      submarines  NNS       submarine     submarin\n",
              "25             were         O            were  VBD              be         were\n",
              "26         invented         O        invented  VBN          invent       invent\n",
              "27                ,         O               ,    ,               ,            ,\n",
              "28              and         O             and   CC             and          and\n",
              "29           before         O          before   IN          before        befor\n",
              "30              any         O             any   DT             any          ani\n",
              "31            means         O           means  NNS            mean         mean\n",
              "32               of         O              of   IN              of           of\n",
              "33            space         O           space   NN           space        space\n",
              "34           travel         O          travel   NN          travel       travel\n",
              "35              had         O             had  VBD            have          had\n",
              "36             been         O            been  VBN              be         been\n",
              "37          devised         O         devised  VBN          devise        devis\n",
              "38                .         O               .    .               .            ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7wmMZPXJhoJ",
        "colab_type": "text"
      },
      "source": [
        "## Spell Checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ69y26MIw09",
        "colab_type": "code",
        "outputId": "39382813-0997-41f3-b9ab-d447a62f79ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "spell_checker = PretrainedPipeline('check_spelling', 'en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "check_spelling download started this may take some time.\n",
            "Approx size to download 892.6 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO9RziOtJzEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = spell_checker.annotate(testDoc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK-ElyC1J72H",
        "colab_type": "code",
        "outputId": "ca0ddc50-6d3b-40bd-b59c-b5350c88819b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['document', 'sentence', 'token', 'checked'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCQwRbO_J-Cx",
        "colab_type": "code",
        "outputId": "0d751ad2-c082-4223-e899-4ef722e45ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "list(zip(result['token'], result['checked']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('French', 'French'),\n",
              " ('author', 'author'),\n",
              " ('who', 'who'),\n",
              " ('helped', 'helped'),\n",
              " ('pioner', 'pioneer'),\n",
              " ('the', 'the'),\n",
              " ('science-fiction', 'science-fiction'),\n",
              " ('genre', 'genre'),\n",
              " ('.', '.'),\n",
              " ('Verne', 'Vern'),\n",
              " ('wrate', 'wrote'),\n",
              " ('about', 'about'),\n",
              " ('space', 'space'),\n",
              " (',', ','),\n",
              " ('air', 'air'),\n",
              " (',', ','),\n",
              " ('and', 'and'),\n",
              " ('underwater', 'underwater'),\n",
              " ('travel', 'travel'),\n",
              " ('before', 'before'),\n",
              " ('navigable', 'navigable'),\n",
              " ('aircrast', 'aircraft'),\n",
              " ('and', 'and'),\n",
              " ('practical', 'practical'),\n",
              " ('submarines', 'submarines'),\n",
              " ('were', 'were'),\n",
              " ('invented', 'invented'),\n",
              " (',', ','),\n",
              " ('and', 'and'),\n",
              " ('before', 'before'),\n",
              " ('any', 'any'),\n",
              " ('means', 'means'),\n",
              " ('of', 'of'),\n",
              " ('space', 'space'),\n",
              " ('travel', 'travel'),\n",
              " ('had', 'had'),\n",
              " ('been', 'been'),\n",
              " ('devised', 'devised'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dSzsA1NLxuA",
        "colab_type": "text"
      },
      "source": [
        "## Parsing a list of texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llB04f6sKJgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testDoc_list = [ 'French author who helped pioner the science-fiction genre.',\n",
        "'Verne wrate about space, air, and underwater travel before navigable aircrast',\n",
        "'Practical submarines were invented, and before any means of space travel had been devised.']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_elc3a2IMBUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_list = pipeline.annotate(testDoc_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOOf0cpCMH2O",
        "colab_type": "code",
        "outputId": "7862939d-876d-40f2-d80a-e2a89c267bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(result_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIzqwkmNMJgY",
        "colab_type": "code",
        "outputId": "61cac004-8dcc-4298-b1ec-f21c24c2f45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "result_list[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'document': ['French author who helped pioner the science-fiction genre.'],\n",
              " 'lemmas': ['French',\n",
              "  'author',\n",
              "  'who',\n",
              "  'help',\n",
              "  'pioneer',\n",
              "  'the',\n",
              "  'sciencefiction',\n",
              "  'genre',\n",
              "  '.'],\n",
              " 'pos': ['JJ', 'NN', 'WP', 'VBD', 'NN', 'DT', 'NN', 'NN', '.'],\n",
              " 'sentence': ['French author who helped pioner the science-fiction genre.'],\n",
              " 'spell': ['French',\n",
              "  'author',\n",
              "  'who',\n",
              "  'helped',\n",
              "  'pioneer',\n",
              "  'the',\n",
              "  'sciencefiction',\n",
              "  'genre',\n",
              "  '.'],\n",
              " 'stems': ['french',\n",
              "  'author',\n",
              "  'who',\n",
              "  'help',\n",
              "  'pioneer',\n",
              "  'the',\n",
              "  'sciencefict',\n",
              "  'genr',\n",
              "  '.'],\n",
              " 'token': ['French',\n",
              "  'author',\n",
              "  'who',\n",
              "  'helped',\n",
              "  'pioner',\n",
              "  'the',\n",
              "  'science-fiction',\n",
              "  'genre',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebIt_w6FNJ7N",
        "colab_type": "text"
      },
      "source": [
        "## Using fullAnnotate to get more details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhx4qjKxMoH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'Peter Parker is a nice guy and lives in New York'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoC3XG-0NX0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detailed_result = pipeline_dl.fullAnnotate(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcYv6-3wNdwK",
        "colab_type": "code",
        "outputId": "9e03986d-33c2-487f-f95e-0868f642d976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "detailed_result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'checked': [Annotation(token, 0, 4, Peter, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 6, 11, Parker, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 13, 14, is, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 16, 16, a, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 18, 21, nice, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 23, 25, guy, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 27, 29, and, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 31, 35, lives, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 37, 38, in, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 40, 42, New, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 44, 47, York, {'confidence': '1.0', 'sentence': '0'})],\n",
              "  'document': [Annotation(document, 0, 47, Peter Parker is a nice guy and lives in New York, {})],\n",
              "  'embeddings': [Annotation(word_embeddings, 0, 4, Peter, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'Peter', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 6, 11, Parker, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'Parker', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 13, 14, is, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'is', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 16, 16, a, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'a', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 18, 21, nice, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'nice', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 23, 25, guy, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'guy', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 27, 29, and, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'and', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 31, 35, lives, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'lives', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 37, 38, in, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'in', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 40, 42, New, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'New', 'sentence': '0'}),\n",
              "   Annotation(word_embeddings, 44, 47, York, {'isOOV': 'false', 'pieceId': '-1', 'isWordStart': 'true', 'token': 'York', 'sentence': '0'})],\n",
              "  'entities': [Annotation(chunk, 0, 11, Peter Parker, {'entity': 'PER', 'sentence': '0', 'chunk': '0'}),\n",
              "   Annotation(chunk, 40, 47, New York, {'entity': 'LOC', 'sentence': '0', 'chunk': '1'})],\n",
              "  'lemma': [Annotation(token, 0, 4, Peter, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 6, 11, Parker, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 13, 14, be, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 16, 16, a, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 18, 21, nice, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 23, 25, guy, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 27, 29, and, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 31, 35, life, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 37, 38, in, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 40, 42, New, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 44, 47, York, {'confidence': '1.0', 'sentence': '0'})],\n",
              "  'ner': [Annotation(named_entity, 0, 4, B-PER, {'word': 'Peter'}),\n",
              "   Annotation(named_entity, 6, 11, I-PER, {'word': 'Parker'}),\n",
              "   Annotation(named_entity, 13, 14, O, {'word': 'is'}),\n",
              "   Annotation(named_entity, 16, 16, O, {'word': 'a'}),\n",
              "   Annotation(named_entity, 18, 21, O, {'word': 'nice'}),\n",
              "   Annotation(named_entity, 23, 25, O, {'word': 'guy'}),\n",
              "   Annotation(named_entity, 27, 29, O, {'word': 'and'}),\n",
              "   Annotation(named_entity, 31, 35, O, {'word': 'lives'}),\n",
              "   Annotation(named_entity, 37, 38, O, {'word': 'in'}),\n",
              "   Annotation(named_entity, 40, 42, B-LOC, {'word': 'New'}),\n",
              "   Annotation(named_entity, 44, 47, I-LOC, {'word': 'York'})],\n",
              "  'pos': [Annotation(pos, 0, 4, NNP, {'word': 'Peter'}),\n",
              "   Annotation(pos, 6, 11, NNP, {'word': 'Parker'}),\n",
              "   Annotation(pos, 13, 14, VBZ, {'word': 'is'}),\n",
              "   Annotation(pos, 16, 16, DT, {'word': 'a'}),\n",
              "   Annotation(pos, 18, 21, JJ, {'word': 'nice'}),\n",
              "   Annotation(pos, 23, 25, NN, {'word': 'guy'}),\n",
              "   Annotation(pos, 27, 29, CC, {'word': 'and'}),\n",
              "   Annotation(pos, 31, 35, NNS, {'word': 'lives'}),\n",
              "   Annotation(pos, 37, 38, IN, {'word': 'in'}),\n",
              "   Annotation(pos, 40, 42, NNP, {'word': 'New'}),\n",
              "   Annotation(pos, 44, 47, NNP, {'word': 'York'})],\n",
              "  'sentence': [Annotation(document, 0, 47, Peter Parker is a nice guy and lives in New York, {'sentence': '0'})],\n",
              "  'stem': [Annotation(token, 0, 4, peter, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 6, 11, parker, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 13, 14, i, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 16, 16, a, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 18, 21, nice, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 23, 25, gui, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 27, 29, and, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 31, 35, live, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 37, 38, in, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 40, 42, new, {'confidence': '1.0', 'sentence': '0'}),\n",
              "   Annotation(token, 44, 47, york, {'confidence': '1.0', 'sentence': '0'})],\n",
              "  'token': [Annotation(token, 0, 4, Peter, {'sentence': '0'}),\n",
              "   Annotation(token, 6, 11, Parker, {'sentence': '0'}),\n",
              "   Annotation(token, 13, 14, is, {'sentence': '0'}),\n",
              "   Annotation(token, 16, 16, a, {'sentence': '0'}),\n",
              "   Annotation(token, 18, 21, nice, {'sentence': '0'}),\n",
              "   Annotation(token, 23, 25, guy, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 29, and, {'sentence': '0'}),\n",
              "   Annotation(token, 31, 35, lives, {'sentence': '0'}),\n",
              "   Annotation(token, 37, 38, in, {'sentence': '0'}),\n",
              "   Annotation(token, 40, 42, New, {'sentence': '0'}),\n",
              "   Annotation(token, 44, 47, York, {'sentence': '0'})]}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMFP_3y4NfJS",
        "colab_type": "code",
        "outputId": "d544b1d4-0677-4da3-a65d-4b6c414c3528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "detailed_result[0]['entities']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Annotation(chunk, 0, 11, Peter Parker, {'entity': 'PER', 'sentence': '0', 'chunk': '0'}),\n",
              " Annotation(chunk, 40, 47, New York, {'entity': 'LOC', 'sentence': '0', 'chunk': '1'})]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQZu_E43VRPS",
        "colab_type": "code",
        "outputId": "40b5b843-651e-48f9-d5ed-5e6e7b949e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "detailed_result[0]['entities'][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Annotation(chunk, 0, 11, Peter Parker, {'entity': 'PER', 'sentence': '0', 'chunk': '0'})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvtsAdOdVS-u",
        "colab_type": "code",
        "outputId": "8d073426-0078-4120-e362-6e5029d8a2cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To reach the value we use .result method\n",
        "detailed_result[0]['entities'][0].result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Peter Parker'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4K2R0WHVVOc",
        "colab_type": "code",
        "outputId": "6720dbdb-30cb-42f0-828a-8312c8e04e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "detailed_result[0]['entities'][0].metadata['entity']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PER'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx5X3YybN9UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunks = []\n",
        "entities = []\n",
        "\n",
        "for n in detailed_result[0]['entities']:\n",
        "    chunks.append(n.result)\n",
        "    entities.append(n.metadata['entity'])\n",
        "\n",
        "df = pd.DataFrame({'chunks':chunks, 'entities':entities})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jPvNZkpRnph",
        "colab_type": "code",
        "outputId": "ecf06e49-9239-4da4-a02b-01b4b05f001f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_id</th>\n",
              "      <th>token</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>pos</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Peter</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Parker</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>NNP</td>\n",
              "      <td>I-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>is</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>nice</td>\n",
              "      <td>18</td>\n",
              "      <td>21</td>\n",
              "      <td>JJ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>guy</td>\n",
              "      <td>23</td>\n",
              "      <td>25</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>and</td>\n",
              "      <td>27</td>\n",
              "      <td>29</td>\n",
              "      <td>CC</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>lives</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>New</td>\n",
              "      <td>40</td>\n",
              "      <td>42</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-LOC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>York</td>\n",
              "      <td>44</td>\n",
              "      <td>47</td>\n",
              "      <td>NNP</td>\n",
              "      <td>I-LOC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sent_id   token  start  end  pos    ner\n",
              "0         0   Peter      0    4  NNP  B-PER\n",
              "1         0  Parker      6   11  NNP  I-PER\n",
              "2         0      is     13   14  VBZ      O\n",
              "3         0       a     16   16   DT      O\n",
              "4         0    nice     18   21   JJ      O\n",
              "5         0     guy     23   25   NN      O\n",
              "6         0     and     27   29   CC      O\n",
              "7         0   lives     31   35  NNS      O\n",
              "8         0      in     37   38   IN      O\n",
              "9         0     New     40   42  NNP  B-LOC\n",
              "10        0    York     44   47  NNP  I-LOC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG6IEJr6Ulno",
        "colab_type": "code",
        "outputId": "05b43dcc-5246-43b5-896c-1e3cdb4aeb8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "tuples = []\n",
        "\n",
        "for x, y, z in zip(detailed_result[0][\"token\"], detailed_result[0]['pos'], detailed_result[0]['ner']):\n",
        "    tuples.append((int(x.metadata['sentence']), x.result, x.begin, x.end, y.result, z.result))\n",
        "\n",
        "df = pd.DataFrame(tuples, columns=['sent_id', 'token', 'start', 'end', 'pos', 'ner'])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent_id</th>\n",
              "      <th>token</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>pos</th>\n",
              "      <th>ner</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Peter</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Parker</td>\n",
              "      <td>6</td>\n",
              "      <td>11</td>\n",
              "      <td>NNP</td>\n",
              "      <td>I-PER</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>is</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>VBZ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>nice</td>\n",
              "      <td>18</td>\n",
              "      <td>21</td>\n",
              "      <td>JJ</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>guy</td>\n",
              "      <td>23</td>\n",
              "      <td>25</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>and</td>\n",
              "      <td>27</td>\n",
              "      <td>29</td>\n",
              "      <td>CC</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>lives</td>\n",
              "      <td>31</td>\n",
              "      <td>35</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>in</td>\n",
              "      <td>37</td>\n",
              "      <td>38</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>New</td>\n",
              "      <td>40</td>\n",
              "      <td>42</td>\n",
              "      <td>NNP</td>\n",
              "      <td>B-LOC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>York</td>\n",
              "      <td>44</td>\n",
              "      <td>47</td>\n",
              "      <td>NNP</td>\n",
              "      <td>I-LOC</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sent_id   token  start  end  pos    ner\n",
              "0         0   Peter      0    4  NNP  B-PER\n",
              "1         0  Parker      6   11  NNP  I-PER\n",
              "2         0      is     13   14  VBZ      O\n",
              "3         0       a     16   16   DT      O\n",
              "4         0    nice     18   21   JJ      O\n",
              "5         0     guy     23   25   NN      O\n",
              "6         0     and     27   29   CC      O\n",
              "7         0   lives     31   35  NNS      O\n",
              "8         0      in     37   38   IN      O\n",
              "9         0     New     40   42  NNP  B-LOC\n",
              "10        0    York     44   47  NNP  I-LOC"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc-Bo9r5XGLt",
        "colab_type": "text"
      },
      "source": [
        "## Use Pretrained **match_chunk** Pipeline for individual Noun Phrase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Szw_bvlX-5-",
        "colab_type": "text"
      },
      "source": [
        "pipeline uses the regex pattern of `<DT>?<JJ>*<NN>+` (yani determinat (DT) etiher the or a might occur, one or more adjective (JJ) and noun (NN) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGlYgC-cV0jq",
        "colab_type": "code",
        "outputId": "03677f0b-c744-4c99-bb54-7dd1ec29661f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pipeline = PretrainedPipeline('match_chunks', 'en')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "match_chunks download started this may take some time.\n",
            "Approx size to download 4.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppFrKptDYder",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipeline.annotate('The book has many chapters. The red car seems awesome')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0JhdIe4Ypy4",
        "colab_type": "code",
        "outputId": "61e1b374-2fa9-4258-c78c-9f4ba920759c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chunk': ['The book', 'The red car'],\n",
              " 'document': ['The book has many chapters. The red car seems awesome'],\n",
              " 'pos': ['DT', 'NN', 'VBZ', 'JJ', 'NNS', '.', 'DT', 'JJ', 'NN', 'VBZ', 'JJ'],\n",
              " 'sentence': ['The book has many chapters.', 'The red car seems awesome'],\n",
              " 'token': ['The',\n",
              "  'book',\n",
              "  'has',\n",
              "  'many',\n",
              "  'chapters',\n",
              "  '.',\n",
              "  'The',\n",
              "  'red',\n",
              "  'car',\n",
              "  'seems',\n",
              "  'awesome']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh2dsVQCYrLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipeline.annotate('the little brown pitty bird was trying to fly and the green dog was barking at it')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4DtJiEkZHI0",
        "colab_type": "code",
        "outputId": "b216a7cb-2e53-481a-dc51-d2df86d3ad3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chunk': ['the little brown pitty bird', 'the green dog'],\n",
              " 'document': ['the little brown pitty bird was trying to fly and the green dog was barking at it'],\n",
              " 'pos': ['DT',\n",
              "  'JJ',\n",
              "  'JJ',\n",
              "  'NN',\n",
              "  'NN',\n",
              "  'VBD',\n",
              "  'VBG',\n",
              "  'TO',\n",
              "  'VB',\n",
              "  'CC',\n",
              "  'DT',\n",
              "  'JJ',\n",
              "  'NN',\n",
              "  'VBD',\n",
              "  'VBG',\n",
              "  'IN',\n",
              "  'PRP'],\n",
              " 'sentence': ['the little brown pitty bird was trying to fly and the green dog was barking at it'],\n",
              " 'token': ['the',\n",
              "  'little',\n",
              "  'brown',\n",
              "  'pitty',\n",
              "  'bird',\n",
              "  'was',\n",
              "  'trying',\n",
              "  'to',\n",
              "  'fly',\n",
              "  'and',\n",
              "  'the',\n",
              "  'green',\n",
              "  'dog',\n",
              "  'was',\n",
              "  'barking',\n",
              "  'at',\n",
              "  'it']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fig-Ge0IZIC0",
        "colab_type": "code",
        "outputId": "d3e0c31d-02f0-4b7d-da2c-45dcd1a4e672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result['chunk']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the little brown pitty bird']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR7t6vtxllzV",
        "colab_type": "text"
      },
      "source": [
        "## Extract Exact Dates afrom Referential Date Phrahes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiYerG35ZPkk",
        "colab_type": "code",
        "outputId": "614b28e2-61c2-4b8b-a077-9c1ef0a23278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pipeline = PretrainedPipeline('match_datetime', 'en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "match_datetime download started this may take some time.\n",
            "Approx size to download 12.9 KB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjxHYCjAmBID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipeline.annotate('I saw him yesterday and he told me that he would visit us next week.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oToHwIikmNeX",
        "colab_type": "code",
        "outputId": "f7f587c3-3f7d-4f23-81fe-6071be479679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'date': ['2020/04/27', '2020/04/19'],\n",
              " 'document': ['I saw him yesterday and he told me that he would visit us next week.'],\n",
              " 'sentence': ['I saw him yesterday and he told me that he would visit us next week.'],\n",
              " 'token': ['I',\n",
              "  'saw',\n",
              "  'him',\n",
              "  'yesterday',\n",
              "  'and',\n",
              "  'he',\n",
              "  'told',\n",
              "  'me',\n",
              "  'that',\n",
              "  'he',\n",
              "  'would',\n",
              "  'visit',\n",
              "  'us',\n",
              "  'next',\n",
              "  'week',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A89g6O8imRiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipeline.fullAnnotate('I saw him yesterday and he told me that he would visit us next week.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv5Tqx-SnyLv",
        "colab_type": "code",
        "outputId": "5a446a3e-d50d-420d-dbcb-50e1f286c80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'date': [Annotation(date, 58, 66, 2020/04/27, {'sentence': '0'}),\n",
              "   Annotation(date, 10, 18, 2020/04/19, {'sentence': '0'})],\n",
              "  'document': [Annotation(document, 0, 67, I saw him yesterday and he told me that he would visit us next week., {})],\n",
              "  'sentence': [Annotation(document, 0, 67, I saw him yesterday and he told me that he would visit us next week., {'sentence': '0'})],\n",
              "  'token': [Annotation(token, 0, 0, I, {'sentence': '0'}),\n",
              "   Annotation(token, 2, 4, saw, {'sentence': '0'}),\n",
              "   Annotation(token, 6, 8, him, {'sentence': '0'}),\n",
              "   Annotation(token, 10, 18, yesterday, {'sentence': '0'}),\n",
              "   Annotation(token, 20, 22, and, {'sentence': '0'}),\n",
              "   Annotation(token, 24, 25, he, {'sentence': '0'}),\n",
              "   Annotation(token, 27, 30, told, {'sentence': '0'}),\n",
              "   Annotation(token, 32, 33, me, {'sentence': '0'}),\n",
              "   Annotation(token, 35, 38, that, {'sentence': '0'}),\n",
              "   Annotation(token, 40, 41, he, {'sentence': '0'}),\n",
              "   Annotation(token, 43, 47, would, {'sentence': '0'}),\n",
              "   Annotation(token, 49, 53, visit, {'sentence': '0'}),\n",
              "   Annotation(token, 55, 56, us, {'sentence': '0'}),\n",
              "   Annotation(token, 58, 61, next, {'sentence': '0'}),\n",
              "   Annotation(token, 63, 66, week, {'sentence': '0'}),\n",
              "   Annotation(token, 67, 67, ., {'sentence': '0'})]}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vymPiVyoFG2",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mqnp33bnzFQ",
        "colab_type": "code",
        "outputId": "8a3bed44-22a6-4050-94b4-38a7fe7f9eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "pipeline =PretrainedPipeline('analyze_sentiment', 'en')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analyze_sentiment download started this may take some time.\n",
            "Approx size to download 4.9 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Vqv7v2ofjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pipeline.annotate('The movie I watched was not a good one')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMSOT4jXpeKC",
        "colab_type": "code",
        "outputId": "81702475-95bd-401d-c438-b116405d9cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'checked': ['The', 'movie', 'I', 'watched', 'was', 'not', 'a', 'good', 'one'],\n",
              " 'document': ['The movie I watched was not a good one'],\n",
              " 'sentence': ['The movie I watched was not a good one'],\n",
              " 'sentiment': ['negative'],\n",
              " 'token': ['The', 'movie', 'I', 'watched', 'was', 'not', 'a', 'good', 'one']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uV-mK5zpe8G",
        "colab_type": "code",
        "outputId": "d5e2d93f-7e6f-4f98-cf24-70a6068c22bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "result['sentiment']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nb7cDSGprhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}